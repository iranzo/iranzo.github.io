<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>OpenShift on Pablo Iranzo Gómez blog</title>
    <link>https://iranzo.io/tags/openshift/</link>
    <description>Recent content in OpenShift on Pablo Iranzo Gómez blog</description>
    <image>
      <title>Pablo Iranzo Gómez blog</title>
      <url>https://iranzo.io/mugshot.png</url>
      <link>https://iranzo.io/mugshot.png</link>
    </image>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Wed, 12 Feb 2025 09:33:05 +0000</lastBuildDate>
    <atom:link href="https://iranzo.io/tags/openshift/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>OpenShift Layered Images for patching</title>
      <link>https://iranzo.io/blog/2023/11/08/openshift-layered-images-for-patching/</link>
      <pubDate>Wed, 08 Nov 2023 00:00:00 +0200</pubDate>
      <guid>https://iranzo.io/blog/2023/11/08/openshift-layered-images-for-patching/</guid>
      <description>&lt;p&gt;With recent releases of OpenShift like 4.13 you can use &lt;a href=&#34;https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html/post-installation_configuration/coreos-layering&#34;&gt;CoreOS Layering&lt;/a&gt; to apply custom images to the nodes.&lt;/p&gt;
&lt;p&gt;The feature allows to build, via a &lt;code&gt;Dockerfile&lt;/code&gt; a custom image that can later be applied to our nodes.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s review the steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First we need to find the base image being used in our environment with &lt;code&gt;oc adm release info quay.io/openshift-release-dev/ocp-release:4.13.5-aarch64 --image-for=rhel-coreos&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Then we use the returned value in the &lt;code&gt;FROM&lt;/code&gt; line in our Dockerfile&lt;/li&gt;
&lt;li&gt;If we want to add custom packages, we should have a server which is reachable and run &lt;code&gt;createrepo&lt;/code&gt; on the folder containing the rpm&amp;rsquo;s so that &lt;code&gt;rpm-ostree&lt;/code&gt; can download them for installation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example dockerfile:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel Module Management testing</title>
      <link>https://iranzo.io/blog/2023/01/19/kernel-module-management-testing/</link>
      <pubDate>Thu, 19 Jan 2023 08:55:11 +0000</pubDate>
      <guid>https://iranzo.io/blog/2023/01/19/kernel-module-management-testing/</guid>
      <description>&lt;p&gt;Following on the &lt;a href=&#34;../../blog/2022/12/23/using-kcli-to-prepare-for-open-cluster-management-testing/&#34;&gt;Using Kcli to prepare for OCM testing&lt;/a&gt;, we&amp;rsquo;re going to prepare KMM testing in Hub-Spoke approach.&lt;/p&gt;
&lt;p&gt;First we need to prepare our &lt;code&gt;.docker/config.json&lt;/code&gt; with the contents of our OpenShift pull secret used with &lt;code&gt;Kcli&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir -p ~/.docker/
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cp openshift_pull.json ~/.docker/config.json
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;warning-advisories&#34;&gt;Warning advisories&lt;/h2&gt;
&lt;div class=&#34;admonition note&#34;&gt;
    &lt;p class=&#34;admonition-title&#34;&gt;Note&lt;/p&gt;
    &lt;p class=&#34;admonition&#34;&gt;Semi-scripted version available at &lt;a href=&#34;automate.sh&#34;&gt;automate.sh&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&#34;admonition warning&#34;&gt;
    &lt;p class=&#34;admonition-title&#34;&gt;Warning&lt;/p&gt;
    &lt;p class=&#34;admonition&#34;&gt;We&amp;rsquo;re using pre-release bits of the software, that&amp;rsquo;s why we need to define a custom catalog for both the Hub and the Spokes. Once KMM is released it will be available from the official one and just the Policy will be needed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Kcli to prepare for Open Cluster Management testing</title>
      <link>https://iranzo.io/blog/2022/12/23/using-kcli-to-prepare-for-open-cluster-management-testing/</link>
      <pubDate>Fri, 23 Dec 2022 14:04:45 +0000</pubDate>
      <guid>https://iranzo.io/blog/2022/12/23/using-kcli-to-prepare-for-open-cluster-management-testing/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/karmab/Kcli&#34;&gt;Kcli&lt;/a&gt; allows to quickly interact with different virtualization platforms to build machines with some specific configurations, and via the use of &lt;code&gt;plans&lt;/code&gt; it allows to automate most of the setup required to have an environment ready.&lt;/p&gt;
&lt;p&gt;In our case, let&amp;rsquo;s setup an environment to practice with &lt;a href=&#34;https://open-cluster-management.io/getting-started/quick-start/&#34;&gt;Open Cluster Management&lt;/a&gt; but instead of using kind clusters, let&amp;rsquo;s use VM&amp;rsquo;s.&lt;/p&gt;
&lt;div class=&#34;admonition note&#34;&gt;
    &lt;p class=&#34;admonition-title&#34;&gt;Note&lt;/p&gt;
    &lt;p class=&#34;admonition&#34;&gt;We&amp;rsquo;ll require to setup an &lt;code&gt;openshift_pull.json&lt;/code&gt; file for Kcli to consume when accessing the required resources for this to work. That file, contains the credentials for accessing several container registries used for the deployment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenShift&#39;s oc debug and parallel execution</title>
      <link>https://iranzo.io/blog/2022/11/03/openshifts-oc-debug-and-parallel-execution/</link>
      <pubDate>Thu, 03 Nov 2022 14:13:32 +0000</pubDate>
      <guid>https://iranzo.io/blog/2022/11/03/openshifts-oc-debug-and-parallel-execution/</guid>
      <description>&lt;p&gt;A colleague reported some issues in the OpenShift troubleshooting and diagnosis scripts at &lt;a href=&#34;https://github.com/RHsyseng/openshift-checks/&#34;&gt;OpenShift-checks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some time ago I did contribute some changes to use functions and allow using the &lt;a href=&#34;https://github.com/risuorg/risu&#34;&gt;RISU&lt;/a&gt; wrapper to the scripts, helping consuming the results via RISU&amp;rsquo;s HTML interface.&lt;/p&gt;
&lt;p&gt;As my colleague reported, for some plugins, the output of the command was not shown in the HTML Interface.&lt;/p&gt;
&lt;p&gt;After some investigation, it was found that parallel execution for the plugins was causing no output to be shown, but when filtering to individual ones via &lt;code&gt;risu -i XXXXXXX/plugin -l&lt;/code&gt; it was working fine&amp;hellip; the problem was not the check itself, as both of them worked fine when executed individually but failed when executing them together.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Setup a Quay mirror for offline installations with mirror-registry</title>
      <link>https://iranzo.io/blog/2022/08/19/setup-a-quay-mirror-for-offline-installations-with-mirror-registry/</link>
      <pubDate>Fri, 19 Aug 2022 10:00:35 +0000</pubDate>
      <guid>https://iranzo.io/blog/2022/08/19/setup-a-quay-mirror-for-offline-installations-with-mirror-registry/</guid>
      <description>Learn on how to use mirror-registry to create a local copy that can be used to install OpenShift without external Internet connectivity.</description>
    </item>
    <item>
      <title>Check Agent status per state</title>
      <link>https://iranzo.io/tips/acm-host-status/</link>
      <pubDate>Wed, 10 Aug 2022 14:01:32 +0000</pubDate>
      <guid>https://iranzo.io/tips/acm-host-status/</guid>
      <description>&lt;p&gt;Check agent status per state&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;watch -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;oc get agent -A -o jsonpath=&amp;#39;{range .items[*]}{@.status.debugInfo.state}{\&amp;#34;\n\&amp;#34;}{end}&amp;#39; |sort | uniq --count&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Zero Touch Provisioning OpenShift for Edge computing</title>
      <link>https://iranzo.io/blog/2022/03/12/zero-touch-provisioning-openshift-for-edge-computing/</link>
      <pubDate>Sat, 12 Mar 2022 09:34:34 +0200</pubDate>
      <guid>https://iranzo.io/blog/2022/03/12/zero-touch-provisioning-openshift-for-edge-computing/</guid>
      <description>This article describes how to use python to bind against an LDAP server and  perform queries</description>
    </item>
    <item>
      <title>Configuring OpenShift with self-contained NTP</title>
      <link>https://iranzo.io/blog/2020/12/07/configuring-openshift-with-self-contained-ntp/</link>
      <pubDate>Mon, 07 Dec 2020 14:10:34 +0200</pubDate>
      <guid>https://iranzo.io/blog/2020/12/07/configuring-openshift-with-self-contained-ntp/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In a regular OpenShift environment, NTP server is more less like this:


  


&lt;figure&gt;
  &lt;img
    src=&#39;https://g.gravizo.com/svg?%0a%0a%20%20%20%20%20%20digraph%20connected%20%7b%0a%20%20%20%20%20%20%20%20%20%20%2f%2f%20title%0a%20%20%20%20%20%20%20%20%20%20labelloc%3d%22t%22%3b%0a%20%20%20%20%20%20%20%20%20%20label%3d%22Connected%20Cluster%22%3b%0a%20%20%20%20%20%20%20%20%20%20node%20%5bshape%20%3d%20circle%5d%3b%0a%20%20%20%20%20%20%20%20%20%20%7b%20rank%20%3d%20same%3b%20%22External%20NTP%20Server%22%3b%7d%0a%20%20%20%20%20%20%20%20%20%20%7b%20rank%20%3d%20same%3b%20%22Master%201%22%3b%20%22Master%202%22%3b%20%22Master%203%22%7d%0a%20%20%20%20%20%20%20%20%20%20%7b%20rank%20%3d%20same%3b%20%22Worker%201%22%3b%20%22Worker%202%22%3b%20%22Worker%203%22%7d%0a%20%20%20%20%20%20%20%20%20%20%22Master%201%22%20-%3e%20%22External%20NTP%20Server%22%20%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Master%202%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Master%203%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Worker%201%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Worker%202%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Worker%203%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%7d%0a%0a&#39;
    alt=&#39;Diagram&#39;
    /&gt;
    &lt;figcaption&gt;Diagram&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In a self-contained cluster with no connection to external networks NTP server is not reachable, but a reachable NTP server is required for proper cluster synchronization.
Cluster does use SSL certificates that require validation and might fail if the dates between the systems are not in sync or at least pretty close in time.&lt;/p&gt;


  


&lt;figure&gt;
  &lt;img
    src=&#39;https://g.gravizo.com/svg?%0adigraph%20disconnected%20%7b%0a%2f%2f%20title%0alabelloc%3d%22t%22%3b%0alabel%3d%22Disconnected%20Cluster%22%3b%0anode%20%5bshape%20%3d%20circle%5d%3b%0a%7b%20rank%20%3d%20same%3b%20%22Master%201%22%3b%20%22Master%202%22%3b%20%22Master%203%22%7d%0a%7b%20rank%20%3d%20same%3b%20%22Worker%201%22%3b%20%22Worker%202%22%3b%20%22Worker%203%22%7d%0a%22Master%201%22%20-%3e%20%22Master%202%22%20%5bcolor%3d%22red%22%5d%0a%22Master%201%22%20-%3e%20%22Master%203%22%20%5bcolor%3d%22green%22%5d%0a%22Master%202%22%20-%3e%20%22Master%201%22%20%5bcolor%3d%22purple%22%5d%0a%22Master%202%22%20-%3e%20%22Master%203%22%5bcolor%3d%22green%22%5d%0a%22Master%203%22%20-%3e%20%22Master%201%22%5bcolor%3d%22purple%22%5d%0a%22Master%203%22%20-%3e%20%22Master%202%22%5bcolor%3d%22red%22%5d%0a%22Worker%201%22%20-%3e%20%22Master%201%22%20%5bcolor%3d%22purple%22%5d%0a%22Worker%201%22%20-%3e%20%22Master%202%22%20%5bcolor%3d%22red%22%5d%0a%22Worker%201%22%20-%3e%20%22Master%203%22%20%5bcolor%3d%22green%22%5d%0a%22Worker%202%22%20-%3e%20%22Master%201%22%20%5bcolor%3d%22purple%22%5d%0a%22Worker%202%22%20-%3e%20%22Master%202%22%20%5bcolor%3d%22red%22%5d%0a%22Worker%202%22%20-%3e%20%22Master%203%22%20%5bcolor%3d%22green%22%5d%0a%22Worker%203%22%20-%3e%20%22Master%201%22%20%5bcolor%3d%22purple%22%5d%0a%22Worker%203%22%20-%3e%20%22Master%202%22%20%5bcolor%3d%22red%22%5d%0a%22Worker%203%22%20-%3e%20%22Master%203%22%20%5bcolor%3d%22green%22%5d%0a%7d%0a&#39;
    alt=&#39;Diagram&#39;
    /&gt;
    &lt;figcaption&gt;Diagram&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We&amp;rsquo;ve several components already available in our OpenShift cluster that are very useful:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Baremetal Operator</title>
      <link>https://iranzo.io/eko/2019-09-11-baremetal-operator/</link>
      <pubDate>Wed, 11 Sep 2019 11:00:00 +0000</pubDate>
      <guid>https://iranzo.io/eko/2019-09-11-baremetal-operator/</guid>
      <description>&lt;p&gt;This article was published originally at &lt;a href=&#34;https://metal3.io/blog/2019/09/11/Baremetal-operator.html&#34;&gt;https://metal3.io/blog/2019/09/11/Baremetal-operator.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/metal3-io/baremetal-operator/&#34;&gt;baremetal operator&lt;/a&gt;, documented at &lt;a href=&#34;https://github.com/metal3-io/baremetal-operator/blob/master/docs/api.md&#34;&gt;https://github.com/metal3-io/baremetal-operator/blob/master/docs/api.md&lt;/a&gt;, it&amp;rsquo;s the Operator in charge of definitions of physical hosts, containing information about how to reach the Out of Band management controller, URL with the desired image to provision, plus other properties related with hosts being used for provisioning instances.&lt;/p&gt;
&lt;p&gt;Quoting from the project:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Bare Metal Operator implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available hosts as instances of the BareMetalHost Custom Resource Definition. The Bare Metal Operator knows how to:
Inspect the host&amp;rsquo;s hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more.
Provision hosts with a desired image
Clean a host&amp;rsquo;s disk contents before or after provisioning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Postgres repl SSL replication</title>
      <link>https://iranzo.io/blog/2019/01/08/postgres-repl-ssl-replication/</link>
      <pubDate>Tue, 08 Jan 2019 17:30:36 +0100</pubDate>
      <guid>https://iranzo.io/blog/2019/01/08/postgres-repl-ssl-replication/</guid>
      <description>&lt;h1 id=&#34;postgres-across-clusters&#34;&gt;Postgres across clusters&lt;/h1&gt;
&lt;p&gt;For Postgres to work across clusters we do need to have the data being synchronized.&lt;/p&gt;
&lt;p&gt;With some other databases we do have some master-master approach, but
usually have very strict requirements on latency, bandwidth, etc that we
cannot solve with On-Premise + external cloud providers.&lt;/p&gt;
&lt;p&gt;If the replication is based on the storage level instead, then you face that database servers don’t deal well if the data changes underneath it, so it leads to data corruption, on top of the storage-level issues/requirements as well on bandwidth, latency, etc.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quay for Federation</title>
      <link>https://iranzo.io/blog/2019/01/08/quay-for-federation/</link>
      <pubDate>Tue, 08 Jan 2019 17:30:36 +0100</pubDate>
      <guid>https://iranzo.io/blog/2019/01/08/quay-for-federation/</guid>
      <description>&lt;h2 id=&#34;why-this-article&#34;&gt;Why this article?&lt;/h2&gt;
&lt;p&gt;For Federation of OpenShift/Kubernetes clusters we want not only to demo some applications, but build a solution that covers all that will be needed in a real-world deployment.&lt;/p&gt;
&lt;p&gt;Colleagues in the Solutions Engineering team have been working on
demonstrating an application running on different clusters over a Federated Control Plane, allowing it to &amp;lsquo;roam&amp;rsquo; between clusters with shared data access as a way to demonstrate how an application can &amp;lsquo;scale&amp;rsquo; from on-premise deployment to external clouds in order to satisfy business requirements for peak demands.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
