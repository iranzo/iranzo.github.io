[{"content":"As we introduced in RHEL Bootable Containers, we can use Image-Mode in RHEL to build system images that we can write to disk.\nFirst step towards having a minimal system, of course, is minimization and it can touch different approaches and methods, depending on how aggressive space reclaim we want to achieve.\nWarning\nPlease note that many of the listed methods below are making changes to a system that could make it harder to debug, be aware that removing files installed from packages will lead to package integrity verification, and, although it might not affect the system, will have to be accounted in case of integrity verification.\nUsual suspects On a regular RHEL system we usually target several paths:\nInstallation customization Custom partitioning to efficiently use the space Reduced package set Remove extra dependencies Post installation customization Adjust number of kernels to keep installed Remove leftover files Configure system logging to rotate and compress logs Remove unneeded locales Remove unused documentation (/usr/share/doc, /usr/share/man/, etc.) Enforcement of above change: Custom RPM\u0026rsquo;s with %post scripts that for each upgrade in other packages, reapply the cleanup performed on docs, etc Ansible playbook execution This of course it\u0026rsquo;s only targeting on the disk requirements, but if we also target for low RAM use we need to play with additional items:\nReduce running services Stop unused daemons Tune requirements: we might want a Graphical User Interface, but it doesn\u0026rsquo;t need to be GNOME and can use something lighter or even something locked down to just allow running some applications. Disable autostart of daemons on hotplug of devices Use static networking to disable NetworkManager, etc. Where we do leverage Anaconda\u0026rsquo;s \u0026lsquo;Minimal install\u0026rsquo; as starting point:\nYou can learn about this on this article published on Red Hat Developer: Minimizing RHEL for Edge and IOT deployment.\nContainers How is this achieved when we\u0026rsquo;re using containers?\nFor containers there are also other concepts involved, as containers are made up with different filesystem layers that sit on top of each other, and for example, one layer might add a file that is removed at a later point, using disk space which, in the end, will be freed (but the container will have it referenced inside).\nFor containers the recommendation is to reduce the number of layers to avoid this, but on the other side, having atomic layers allows us to reuse already existing ones, so when we\u0026rsquo;re downloading an updated container, all the layers that we had downloaded, are reused and only new ones are added.\nSome tools allow to compact the container layers, reclaiming the space that was used and later freed, but this is against the reutilization of layers mentioned above, so they might require additional download for the modified layers\nWe\u0026rsquo;ll speak about this and leveraging FAB tool for assembling bootable containers in the next article.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2024/11/15/rhel-slim-down/","summary":"Learn how to slim down RHEL and its images for creating reduced images for disks","title":"RHEL Slim Down"},{"content":"Containers have entered in the IT panorama since some time ago, and have helped streamline the process for many applications and its lifecycle management.\nContainers require several pieces to run, like having the base OS running, the runtimes, etc so it\u0026rsquo;s not something we can do on baremetal directly, but with the appearance of inmutable systems like CoreOS and later Fedora Silverblue, many users and products started to build on top of systems that were not touched directly but via extra layers applied with a tighter control over changes.\nAll of this starts with a ContainerFile which inherits from an image and performs actions for adding/removing and or configuring based on our needs, for example:\n# Define the base image for the second stage FROM registry.redhat.io/rhel9/rhel-bootc:latest # Define packages to add ARG EXTRA_RPM_PACKAGES=\u0026#39;pciutils tmux joe bootc ostree openssh-server firefox gnome-kiosk gnome-kiosk-search-appliance gdm xorg-x11-drivers xorg-x11-fonts-75dpi xorg-x11-server-Xorg xorg-x11-xauth xorg-x11-xinit xorg-x11-xinit-session\u0026#39; # cloud-init # Define packages to remove ARG REMOVE_RPM_PACKAGES=\u0026#39;adcli avahi-libs flashrom flatpak* fwupd gdisk libsmbclient libudisks2 nano ncurses netavark nfs-utils pipewire rpcbind samba-client-libs samba-common-libs sg3_utils skopeo sos sssd-ad sssd-common sssd-common-pac sssd-ipa sssd-krb5 sssd-krb5-common sssd-ldap sssd-nfs-idmap toolbox tracker tracker-miners udisks2 WALinuxAgent-udev wireplumber zstd\u0026#39; RUN dnf remove -y ${REMOVE_RPM_PACKAGES} RUN dnf autoremove -y # Enable EPEL RUN dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm RUN mv /etc/selinux /etc/selinux.tmp \u0026amp;\u0026amp; \\ dnf install -y \\ ${EXTRA_RPM_PACKAGES} \\ \u0026amp;\u0026amp; dnf -y upgrade \\ \u0026amp;\u0026amp; dnf clean all \\ \u0026amp;\u0026amp; mv /etc/selinux.tmp /etc/selinux \\ \u0026amp;\u0026amp; ln -s ../cloud-init.target /usr/lib/systemd/system/default.target.wants # Enable cloud-init # Remove documentation, man pages and locales to free up space RUN rm -Rfv /usr/share/info/ /usr/share/man/ /usr/share/doc/ /usr/share/locale/ RUN systemctl disable rhsmcertd # Copy GDM autologin configuration ADD config/gdm-custom.conf /etc/gdm/custom.conf # DMRC was used in prior GNOME versions, use new path ADD config/dmrc /var/lib/AccountsService/users/root # Setup /usr/lib/containers/storage as an additional store for images. # Remove once the base images have this set by default. RUN grep -q /usr/lib/containers/storage /etc/containers/storage.conf || \\ sed -i -e \u0026#39;/additionalimage.*/a \u0026#34;/usr/lib/containers/storage\u0026#34;,\u0026#39; \\ /etc/containers/storage.conf # Added for running as an OCI Container to prevent Overlay on Overlay issues. VOLUME /var/lib/containers In this case, we\u0026rsquo;ll be configuring autologin for the graphics session with the user we\u0026rsquo;ll adding later\nThis is done via /etc/gdm/custom.conf file:\n[daemon] AutomaticLoginEnable=True AutomaticLogin=root And via Gnome configuration for user root:\n[User] Session=org.gnome.Kiosk.SearchApp.Session SystemAccount=true For assembling our container we use the following command:\npodman build -f ContainerFile --squash Once it has finished, we can assign a tag and a name for it, passing as MYID the value of the hash obtained in previous command:\nOURCONTAINERTAG=\u0026#34;quay.io/myuser/mybootccontainer\u0026#34; podman tag $MYID ${OURCONTAINERTAG} Last step is to start the container conversion into a RAW image, an ISO, a QCOW2, etc\u0026hellip; but first adapt the configuration for creating our user:\nContents of config.tmpl.json:\n{ \u0026#34;blueprint\u0026#34;: { \u0026#34;customizations\u0026#34;: { \u0026#34;user\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;linux123\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;$MYKEY\u0026#34;, \u0026#34;home\u0026#34;: \u0026#34;/var/roothome\u0026#34; } ] } } } So\u0026hellip; let\u0026rsquo;s build it!:\n#!/bin/bash mkdir -p output/ export MYKEY=$(cat ~/.ssh/id_rsa.pub) envsubst \u0026lt;config.tmpl.json \u0026gt;config.json # CentOS builder IMAGE=\u0026#34;quay.io/centos-bootc/bootc-image-builder:latest\u0026#34; TYPE=\u0026#34;--rootfs ext4 --type raw\u0026#34; # For generating an ISO that starts anaconda and auto installs to disk # TYPE=\u0026#34;--type anaconda-iso\u0026#34; podman run \\ --rm \\ -it \\ --privileged \\ --pull=always \\ --security-opt label=type:unconfined_t \\ -v $(pwd)/output:/output \\ -v /var/lib/containers/storage:/var/lib/containers/storage \\ -v $(pwd)/config.json:/config.json \\ ${IMAGE} \\ ${TYPE} \\ --local \\ ${OURCONTAINERTAG} Once the process is finished in the output folder the raw image will be available for us to test with virt-manager or for using dd to write it to a USB-drive for baremetal testing.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2024/08/09/rhel-bootable-containers-bootc/","summary":"Learn how to use a container for being the baseline for your computer, making it easier to operate the operating system installation and maintenance.","title":"RHEL bootable containers (BOOTC)"},{"content":" Attention\nThis post was originally published on Red Hat Developer, the community to learn, code, and share faster. To read the original post, click here.\nCheck the video version for Red Hat TV\nIn this article we cover the required steps to configure Kernel Module Management Operator (KMM) and use it to deploy an out-of-tree (OOT) kernel module, as well as leveraging other related technologies to build a toolset for hardware enablement. To illustrate that process, we\u0026rsquo;ll leverage the Intel Data Center GPU Flex 140.\nWhat is the Kernel Module Management Operator? The Kernel Module Management Operator manages, builds, signs, and deploys out-of-tree (OOT) kernel modules and device plug-ins on Red Hat OpenShift Container Platform clusters.\nBefore KMM, cluster admins had to manually install drivers to multiple nodes. Upgrades were painful and prone to errors from incompatible drivers. Furthermore, workloads might get scheduled to a node with broken drivers causing scheduling issues or missing hardware. KMM solves all of these problems, as we\u0026rsquo;ll see.\nKMM is designed to accommodate multiple kernel versions at once for any kernel module, allowing for seamless node upgrades and reduced application downtime. For more information, refer to the Kernel Module Management Operator product documentation.\nKMM is also a community project, which you can test on upstream Kubernetes, and there is a Slack community channel.\nPrerequisites For this scenario, we\u0026rsquo;ll require an already working OpenShift environment as we will use it to deploy the different tools on top. Check the documentation for instructions.\nKMM will require a registry to push images to. If you\u0026rsquo;ve installed on bare metal, ensure the internal registry is enabled and configured (refer to Installing a user-provisioned cluster on bare metal).\nAdditionally, this tutorial references data available from Intel at the following locations:\nIntel Data Center GPU Driver for OpenShift Intel Technology Enabling for OpenShift Set up Node Feature Discovery Operator Node Feature Discovery (NFD) detects hardware features available on nodes and advertises those features using nodes labels, so that they can later be used as selector for scheduling decisions.\nThe NFD Operator automatically adds labels to the nodes that present some characteristics, including if the node has a GPU and which GPU it has.\nIt\u0026rsquo;s an ideal way to identify which nodes require a kernel module to be enabled for the specific node(s) and later use it to instruct KMM to build it only for those.\nWe can install it via the following YAML:\n--- apiVersion: v1 kind: Namespace metadata: name: openshift-nfd --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-nfd namespace: openshift-nfd spec: targetNamespaces: - openshift-nfd --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \u0026#34;stable\u0026#34; installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace Once installed, we can create a CRD as described in the NFD Operator documentation to start.\nCreate a file named nfdcr.yaml with the following contents:\napiVersion: nfd.openshift.io/v1 kind: NodeFeatureDiscovery metadata: name: nfd-instance namespace: openshift-nfd spec: operand: image: quay.io/openshift/origin-node-feature-discovery:4.14 imagePullPolicy: Always servicePort: 12000 workerConfig: configData: | Then apply it with:\noc apply -f nfdcr.yaml Additionally, create a nodefeaturerule.yaml:\napiVersion: nfd.openshift.io/v1alpha1 kind: NodeFeatureRule metadata: name: intel-dp-devices spec: rules: - name: \u0026#34;intel.gpu\u0026#34; labels: \u0026#34;intel.feature.node.kubernetes.io/gpu\u0026#34;: \u0026#34;true\u0026#34; matchFeatures: - feature: pci.device matchExpressions: vendor: { op: In, value: [\u0026#34;8086\u0026#34;] } class: { op: In, value: [\u0026#34;0300\u0026#34;, \u0026#34;0380\u0026#34;] } And once it has been applied, oc describe node displays the labels applied. Below is a partial output of the labels that NFD applies:\nLabels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux feature.node.kubernetes.io/cpu-cpuid.ADX=true feature.node.kubernetes.io/cpu-cpuid.AESNI=true feature.node.kubernetes.io/cpu-cpuid.AMXINT8=true feature.node.kubernetes.io/cpu-cpuid.AMXTILE=true feature.node.kubernetes.io/cpu-cpuid.AVX=true feature.node.kubernetes.io/cpu-cpuid.AVX2=true … As we can see, it starts showing some of the CPU flags among other values as labels in our system. We can then grep for Intel-specific GPU labels with:\n$ oc describe node rhvp-intel-01|grep intel.feature intel.feature.node.kubernetes.io/gpu=true \u0026hellip;where we can see that NFD has detected the GPU.\nAt this point, NFD will take care of our node\u0026rsquo;s HW detection and labeling, which can later be used as node selectors for KMM to deploy the required modules.\nAdvanced node labeling For this use case, the Intel Data Center GPU Driver for OpenShift (i915) driver is only available and tested for some kernel versions.\nUsing NFD labels, we can target specific custom kernel versions for our module deployment and enablement so that only hosts with the required kernel and the required hardware are enabled for driver activation. This ensures that only compatible drivers are installed on nodes with a supported kernel, which is what makes KMM so valuable.\nIn this case, we\u0026rsquo;ll be using nodes that contain the intel.feature.node.kubernetes.io/gpu tag.\nLet\u0026rsquo;s move on to the next steps in our journey, installing KMM and building the kernel module with Driver Toolkit (DTK). DTK is a container image used as a base image where drivers can be built, as it includes kernel packages, some tools, etc.\nSet up Kernel Module Management Operator Install KMM using OperatorHub in our OpenShift Console or the following kmm.yaml:\n--- apiVersion: v1 kind: Namespace metadata: name: openshift-kmm --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: kernel-module-management namespace: openshift-kmm --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: kernel-module-management namespace: openshift-kmm spec: channel: \u0026#34;stable\u0026#34; installPlanApproval: Automatic name: kernel-module-management source: redhat-operators sourceNamespace: openshift-marketplace With:\noc apply -f kmm.yaml Once it\u0026rsquo;s done, we can switch to the openshift-kmm project:\noc project openshift-kmm Create a kernel module As mentioned previously, KMM can perform the compilation and installation of kernel module drivers for our hardware.\nA kernel module can:\nHave dependencies Replace an existing driver Let\u0026rsquo;s explore the use cases in the next sections of this article.\nIntegration with NFD via CRDs KMM uses a kmod (kernel module) image to define which kernel modules to load, which is an OCI image that contains the .ko files.\nIn .spec.selector, we can define which nodes should be selected and as we showcased earlier, we can target one specific label added by NFD, so that only those nodes are targeted for loading the module for the hardware installed.\nKMM dependencies Adding a module might have additional module dependencies, that is, extra modules that need to be already loaded in the kernel.\nWe can use the Custom Resource Definition (CRD) field .spec.moduleLoader.container.modprobe.modulesLoadingOrder to identify the order for module loading starting with upmost module, then the module it depends on, and so on.\nReplace an in-tree module with an out-of-tree module Similar to dependencies, sometimes the module being loaded conflicts with an already loaded kernel module.\nIn this case, we need to have KMM first remove the conflicting module via the .spec.moduleLoader.container.inTreeModuleToRemove field of the CRD. KMM will then proceed and load the newer OOT module.\nFor the Intel Data Center GPU Flex series, the intel_vsec and i915 drivers will have to be removed which will be discussed later in this article.\nConfigure the Driver Toolkit for image building The Driver Toolkit) provides a base image with required kernel development packages that are used to build specific drivers for our platform, which match the kernel version used on each node where the accelerators exist.\nBy using a specially crafted Containerfile containing a reference to DTK_AUTO as shown below:\nARG DTK_AUTO FROM ${DTK_AUTO} RUN gcc \\... | The KMM Operator will replace the required variables as well as pull and build the image for the driver.\nAnd that\u0026rsquo;s all. Easy, right?\nManage heterogeneous nodes in the cluster As we\u0026rsquo;re using labels for selecting specific nodes in our cluster, we can keep a mix of nodes with or without the hardware in our cluster. KMM will take care of loading the required modules on the matching nodes, leaving the other ones without the specific accelerator.\nIn our case we\u0026rsquo;re using intel.feature.node.kubernetes.io/gpu=true as the label to match our intended nodes, leaving other nodes without the GPU affected.\nEnable the Intel Data Center GPU Flex 140 We\u0026rsquo;re going to explore the step-by-step the process for detecting, configuring, and enabling an Intel GPU in our OpenShift environment.\nOne of our workers has the accelerator installed, as reported by lspci | egrep 'Graphic|Display:\n02:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 52) a0:00.0 Display controller: Intel Corporation Data Center GPU Flex 140 (rev 05) Let\u0026rsquo;s create a MachineConfigPool (MCP) to apply the configuration in our environment:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: intel-dgpu spec: machineConfigSelector: matchExpressions: - { key: machineconfiguration.openshift.io/role, operator: In, values: [worker, intel-dgpu, master], } nodeSelector: matchLabels: intel.feature.node.kubernetes.io/gpu: \u0026#34;true\u0026#34; Note\nIf you\u0026rsquo;re using single node OpenShift for this test, remember that the YAMLs must be adapted so that the configuration via MCO applies to the primary MCP; that is, using the selector machineconfiguration.openshift.io/role: master.\nUsing a machine configuration, we can define new parameters, like for example disable the built-in drivers with this YAML:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: intel-dgpu name: 100-intel-dgpu-machine-config-disable-i915 spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:,blacklist%20i915 mode: 0644 overwrite: true path: /etc/modprobe.d/blacklist-i915.conf - contents: source: data:,blacklist%20intel_vsec mode: 0644 overwrite: true path: /etc/modprobe.d/blacklist-intel-vsec.conf After this YAML is applied, we can check that there are no modules applied via oc debug node/rhvp-intel-01 (NOTE: if you see intel_vsec or i915 in the output, verify that the MachineConfig defined was correctly applied):\n$ lsmod|egrep \u0026#39;i915|vsec\u0026#39; Now, we need to define the path to find the firmware for the module, and for this, there are two approaches; the first one is an MCO that patches the kernel command line (and will cause a reboot of the node) as configured with the following YAML:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: master name: 100-alternative-fw-path-for-master-nodes spec: config: ignition: version: 3.2.0 kernelArguments: - firmware_class.path=/var/lib/firmware And for validating, we can check on the hosts via oc debug node/rhvp-intel-01 that the new parameter has been enabled:\n$ cat /proc/cmdline BOOT_IMAGE=(hd5,gpt3)/ostree/rhcos-085fdd39288474060c9d5bd7a88fabe8d218fcc960186712834c5e4ab319cb1d/vmlinuz-5.14.0-284.52.1.el9_2.x86_64 ignition.platform.id=metal ostree=/ostree/boot.0/rhcos/085fdd39288474060c9d5bd7a88fabe8d218fcc960186712834c5e4ab319cb1d/0 ip=ens801f0:dhcp,dhcp6 root=UUID=bf0c9edf-4aab-48a8-9549-5005fff7890e rw rootflags=prjquota boot=UUID=282ee60b-3053-4c2e-8f92-612af621e245 firmware_class.path=/var/lib/firmware systemd.unified_cgroup_hierarchy=1 cgroup_no_v1=all psi=1 The other approach is to patch the configuration for the KMM operator.\nAlternatively, we can modify the configmap by setting the path for the firmware which doesn\u0026rsquo;t cause a reboot, which is useful in single node OpenShift installations:\n$ oc patch configmap kmm-operator-manager-config -n openshift-kmm --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/data/controller_config.yaml\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;healthProbeBindAddress: :8081\\nmetricsBindAddress: 127.0.0.1:8080\\nleaderElection:\\n enabled: true\\n resourceID: kmm.sigs.x-k8s.io\\nwebhook:\\n disableHTTP2: true\\n port: 9443\\nworker:\\n runAsUser: 0\\n seLinuxType: spc_t\\n setFirmwareClassPath: /var/lib/firmware\u0026#34;}]\u0026#39; If you follow this approach, the KMM pod must be deleted so that the new configuration is taken into effect as soon as the operator recreates it, so that the systems get it applied during the loading of the modules.\nIntel Data Center GPU Flex 140 kernel module The Intel GPU kernel module deployment might be a bit tricky because it\u0026rsquo;s a driver and it has more ties to the specific kernel version being used that we\u0026rsquo;ll be setting in the commands for the build process, as we\u0026rsquo;ll see in the Containerfile used for the build.\nUsing prebuilt drivers In this approach, we use KMM to deploy the built and certified drivers already created by Intel which provides the container for each kernel version via a CI/CD pipeline. End users can directly consume that container via this Module definition:\napiVersion: kmm.sigs.x-k8s.io/v1beta1 kind: Module metadata: name: intel-dgpu namespace: openshift-kmm spec: moduleLoader: container: modprobe: moduleName: i915 firmwarePath: /firmware kernelMappings: - regexp: \u0026#39;^.*\\.x86_64$\u0026#39; containerImage: registry.connect.redhat.com/intel/intel-data-center-gpu-driver-container:2.2.0-$KERNEL_FULL_VERSION selector: intel.feature.node.kubernetes.io/gpu: \u0026#34;true\u0026#34; Compiling your own driver For the sake of demonstration, here we\u0026rsquo;ll be building our own driver image using in-cluster builds.\nFor building the kernel module, we can use the following Containerfile based on the upstream instructions that we\u0026rsquo;re already defining as configmap:\napiVersion: v1 kind: ConfigMap metadata: name: intel-dgpu-dockerfile-configmap namespace: openshift-kmm data: dockerfile: |- # Intel Data Center GPU driver components combinations. ARG I915_RELEASE=I915_23WW51.5_682.48_23.6.42_230425.56 ARG FIRMWARE_RELEASE=23WW49.5_682.48 # Intel Data Center GPU Driver for OpenShift version. ARG DRIVER_VERSION=2.2.0 # RHCOS Kernel version supported by the above driver version. ARG KERNEL_FULL_VERSION # Red Hat DTK image is used as builder image to build kernel driver modules. # Appropriate DTK image is provided with the OCP release, to guarantee compatibility # between the built kernel modules and the OCP version\u0026#39;s RHCOS kernel. # DTK_AUTO is populated automatically with the appropriate DTK image by KMM operator. ARG DTK_AUTO FROM ${DTK_AUTO} as builder ARG I915_RELEASE ARG FIRMWARE_RELEASE ARG KERNEL_FULL_VERSION WORKDIR /build # Building i915 driver RUN git clone -b ${I915_RELEASE} --single-branch https://github.com/intel-gpu/intel-gpu-i915-backports.git \\ \u0026amp;amp;\u0026amp;amp; cd intel-gpu-i915-backports \\ \u0026amp;amp;\u0026amp;amp; install -D COPYING /licenses/i915/COPYING \\ \u0026amp;amp;\u0026amp;amp; export LEX=flex; export YACC=bison \\ \u0026amp;amp;\u0026amp;amp; export OS_TYPE=rhel_9 \u0026amp;amp;\u0026amp;amp; export OS_VERSION=\u0026#34;9.2\u0026#34; \\ \u0026amp;amp;\u0026amp;amp; cp defconfigs/i915 .config \\ \u0026amp;amp;\u0026amp;amp; make olddefconfig \u0026amp;amp;\u0026amp;amp; make modules -j $(nproc) \u0026amp;amp;\u0026amp;amp; make modules_install # Copy out-of-tree drivers to /opt/lib/modules/${KERNEL_FULL_VERSION}/ RUN for file in $(find /lib/modules/${KERNEL_FULL_VERSION}/updates/ -name \u0026#34;*.ko\u0026#34;); do \\ cp $file /opt --parents; done # Create the symbolic link for in-tree dependencies RUN ln -s /lib/modules/${KERNEL_FULL_VERSION} /opt/lib/modules/${KERNEL_FULL_VERSION}/host RUN depmod -b /opt ${KERNEL_FULL_VERSION} # Firmware RUN git clone -b ${FIRMWARE_RELEASE} --single-branch https://github.com/intel-gpu/intel-gpu-firmware.git \\ \u0026amp;amp;\u0026amp;amp; install -D /build/intel-gpu-firmware/COPYRIGHT /licenses/firmware/COPYRIGHT \\ \u0026amp;amp;\u0026amp;amp; install -D /build/intel-gpu-firmware/COPYRIGHT /build/firmware/license/COPYRIGHT \\ \u0026amp;amp;\u0026amp;amp; install -D /build/intel-gpu-firmware/firmware/dg2* /build/firmware/ \\ \u0026amp;amp;\u0026amp;amp; install -D /build/intel-gpu-firmware/firmware/pvc* /build/firmware/ # Packaging Intel GPU driver components in the base UBI image for certification FROM registry.redhat.io/ubi9/ubi-minimal:9.2 ARG DRIVER_VERSION ARG KERNEL_FULL_VERSION ARG I915_RELEASE ARG FIRMWARE_RELEASE # Required labels for the image metadata LABEL vendor=\u0026#34;Intel®\u0026#34; LABEL version=\u0026#34;${DRIVER_VERSION}\u0026#34; LABEL release=\u0026#34;${KERNEL_FULL_VERSION}\u0026#34; LABEL name=\u0026#34;intel-data-center-gpu-driver-container\u0026#34; LABEL summary=\u0026#34;Intel® Data Center GPU Driver Container Image\u0026#34; LABEL description=\u0026#34;Intel® Data Center GPU Driver container image designed for Red Hat OpenShift Container Platform. \\ The driver container is based on Intel Data Center GPU driver components - i915 driver release:${I915_RELEASE}, \\ and Firmware release:${FIRMWARE_RELEASE}. This driver container image is supported for RHOCP 4.14 RHCOS kernel version: ${KERNEL_FULL_VERSION}.\u0026#34; RUN microdnf update -y \u0026amp;amp;\u0026amp;amp; rm -rf /var/cache/yum RUN microdnf -y install kmod findutils \u0026amp;amp;\u0026amp;amp; microdnf clean all COPY --from=builder /licenses/ /licenses/ COPY --from=builder /opt/lib/modules/${KERNEL_FULL_VERSION}/ /opt/lib/modules/${KERNEL_FULL_VERSION}/ COPY --from=builder /build/firmware/ /firmware/i915/ Let\u0026rsquo;s also define the imagestream for storing the generated driver image:\napiVersion: image.openshift.io/v1 kind: ImageStream metadata: labels: app: intel-dgpu-driver-container-kmmo name: intel-dgpu-driver-container-kmmo namespace: openshift-kmm spec: {} Reference this Containerfile in the following YAML:\n--- apiVersion: kmm.sigs.x-k8s.io/v1beta1 kind: Module metadata: name: intel-dgpu-on-premise namespace: openshift-kmm spec: moduleLoader: container: imagePullPolicy: Always modprobe: moduleName: i915 firmwarePath: /firmware kernelMappings: - regexp: \u0026#39;^.*\\.x86_64$\u0026#39; containerImage: image-registry.openshift-image-registry.svc:5000/openshift-kmm/intel-dgpu-driver-container-kmmo:$KERNEL_FULL_VERSION build: dockerfileConfigMap: name: intel-dgpu-dockerfile-configmap selector: intel.feature.node.kubernetes.io/gpu: \u0026#34;true\u0026#34; This file will use the above Containerfile to build the module and store the image in the repository. Note the selector field. It has been modified to use NFD for discovery and only load the kernel module where needed.\nWe can check that the kernel module has been loaded by connecting to the node and checking the status:\nsh-5.1# lsmod|grep i915 i915 3977216 0 intel_vsec 20480 1 i915 compat 24576 2 intel_vsec,i915 video 61440 1 i915 drm_display_helper 172032 2 compat,i915 cec 61440 2 drm_display_helper,i915 i2c_algo_bit 16384 2 ast,i915 drm_kms_helper 192512 5 ast,drm_display_helper,i915 drm 581632 7 drm_kms_helper,compat,ast,drm_shmem_helper,drm_display_helper,i915 Alternatively, we can check the label added by KMM on the nodes:\n$ oc describe node |grep kmm kmm.node.kubernetes.io/openshift-kmm.intel-dgpu-on-premise.ready= Once the kernel module is deployed, use an application to verify HW acceleration is provided by the GPU.\nNote\nHere we\u0026rsquo;re directly using the /dev filesystem for accessing the GPU. The recommended way is to use the Intel Device Plugins Operator and then add a CR to expose gpu.intel.com/i915 to the kubelet for workload consumption as described in the repository.\nVerify the deployment Simple approach We\u0026rsquo;ll be using clinfo to get information from our card. To do so, we\u0026rsquo;ll create the image and the namespace and then run the utility inside a privileged pod similar to the application that we can use as a more complex approach.\nLet\u0026rsquo;s create the BuildConfiguration and ImageStream based on this one by creating a clinfobuild.yaml:\napiVersion: image.openshift.io/v1 kind: ImageStream metadata: name: intel-dgpu-clinfo namespace: openshift-kmm spec: {} --- apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: intel-dgpu-clinfo namespace: openshift-kmm spec: output: to: kind: ImageStreamTag name: intel-dgpu-clinfo:latest runPolicy: Serial source: dockerfile: \u0026#34;ARG BUILDER=registry.access.redhat.com/ubi9-minimal:latest \\nFROM ${BUILDER} \\n\\nARG OCL_ICD_VERSION=ocl-icd-2.2.13-4.el9.x86_64\\nARG CLINFO_VERSION=clinfo-3.0.21.02.21-4.el9.x86_64\\n\\nRUN microdnf install -y \\\\\\n glibc \\\\\\n yum-utils \\n\\n# install intel-opencl, ocl-icd and clinfo\\nRUN dnf install -y \u0026#39;dnf-command(config-manager)\u0026#39; \u0026amp;amp;\u0026amp;amp; \\\\\\n \\ dnf config-manager --add-repo https://repositories.intel.com/gpu/rhel/9.0/lts/2350/unified/intel-gpu-9.0.repo \u0026amp;amp;\u0026amp;amp; \\\\\\n dnf install -y intel-opencl \\\\\\n https://mirror.stream.centos.org/9-stream/AppStream/x86_64/os/Packages/$OCL_ICD_VERSION.rpm \\ \\\\\\n https://dl.fedoraproject.org/pub/epel/9/Everything/x86_64/Packages/c/$CLINFO_VERSION.rpm \u0026amp;amp;\u0026amp;amp; \\\\\\n dnf clean all \u0026amp;amp;\u0026amp;amp; dnf autoremove \u0026amp;amp;\u0026amp;amp; rm -rf /var/lib/dnf/lists/* \u0026amp;amp;\u0026amp;amp; \\\\\\n \\ rm -rf /etc/yum.repos.d/intel-graphics.repo \\n\u0026#34; type: Dockerfile strategy: dockerStrategy: buildArgs: - name: BUILDER value: registry.access.redhat.com/ubi9-minimal:latest - name: OCL_ICD_VERSION value: ocl-icd-2.2.13-4.el9.x86_64 - name: CLINFO_VERSION value: clinfo-3.0.21.02.21-4.el9.x86_64 type: Docker triggers: - type: ConfigChange Let\u0026rsquo;s apply it with:\n$ oc create -f clinfobuild.yaml And then define the privileged pod that will run the tool with this pod defined by job.yaml:\napiVersion: v1 kind: Pod metadata: name: intel-dgpu-clinfo spec: containers: - name: clinfo-pod image: image-registry.openshift-image-registry.svc:5000/openshift-kmm/intel-dgpu-clinfo:latest command: [\u0026#34;clinfo\u0026#34;] resources: securityContext: privileged: true runAsUser: 0 runAsGroup: 110 volumeMounts: - name: dev mountPath: /dev volumes: - name: dev hostPath: path: /dev Let\u0026rsquo;s create the pod with:\n$ oc create -f job.yaml And then examine the output of the pod by running:\n$ oc logs pod intel-dgpu-clinfo Number of platforms 1 Platform Name Intel(R) OpenCL HD Graphics Platform Vendor Intel(R) Corporation Platform Version OpenCL 3.0 Platform Profile FULL_PROFILE ... ... Platform Name Intel(R) OpenCL HD Graphics Number of devices 1 Device Name Intel(R) Data Center GPU Flex Series 140 [0x56c1] Device Vendor Intel(R) Corporation Device Vendor ID 0x8086 Device Version OpenCL 3.0 NEO ... ... Platform Name Intel(R) OpenCL HD Graphics Device Name Intel(R) Data Center GPU Flex Series 140 [0x56c1] ICD loader properties ICD loader Name OpenCL ICD Loader ICD loader Vendor OCL Icd free software ICD loader Version 2.2.12 ICD loader Profile OpenCL 2.2 NOTE: your OpenCL library only supports OpenCL 2.2, but some installed platforms support OpenCL 3.0. Programs using 3.0 features may crash or behave unexpectedly Using OpenVINO application for image text to image with stable diffusion An application using Intel\u0026rsquo;s OpenVINO software will be used to showcase the functionality of the GPU acceleration in the processing that will use some keywords introduced to generate images.\nRequirements In the following paragraphs we\u0026rsquo;ll be covering the requirements that we\u0026rsquo;ll be preparing for the final step of validating the proper setup of our driver.\nImageStream We need to define an ImageStream to store our container with this YAML:\napiVersion: image.openshift.io/v1 kind: ImageStream metadata: labels: app: jupyter-demo name: jupyter-demo namespace: openshift-kmm spec: {} Containerfile First prepare the container image containing all the required bits and pieces for storing in a registry for later use with this BuildConfig:\nkind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: \u0026#34;jupyter-demo\u0026#34; spec: source: dockerfile: | FROM quay.io/jupyter/base-notebook USER root RUN apt update \u0026amp;amp;\u0026amp;amp; \\ apt install -y gpg-agent git wget \u0026amp;amp;\u0026amp;amp; \\ apt clean RUN wget -qO - https://repositories.intel.com/gpu/intel-graphics.key | gpg --dearmor --output /usr/share/keyrings/intel-graphics.gpg RUN echo \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy/production/2328 unified\u0026#34; \u0026amp;gt; /etc/apt/sources.list.d/intel-gpu-jammy.list RUN apt update \u0026amp;amp;\u0026amp;amp; \\ apt install -y \\ intel-opencl-icd intel-level-zero-gpu level-zero \\ intel-media-va-driver-non-free libmfx1 libmfxgen1 libvpl2 \\ libegl-mesa0 libegl1-mesa libegl1-mesa-dev libgbm1 libgl1-mesa-dev libgl1-mesa-dri \\ libglapi-mesa libgles2-mesa-dev libglx-mesa0 libigdgmm12 libxatracker2 mesa-va-drivers \\ mesa-vdpau-drivers mesa-vulkan-drivers va-driver-all vainfo hwinfo clinfo \\ libglib2.0-0 \u0026amp;amp;\u0026amp;amp; \\ apt clean USER jovyan RUN pip install --no-cache-dir \u0026#34;diffusers\u0026amp;gt;=0.14.0\u0026#34; \u0026#34;openvino\u0026amp;gt;=2023.3.0\u0026#34; \u0026#34;transformers \u0026amp;gt;= 4.31\u0026#34; accelerate \u0026#34;urllib3==1.26.15\u0026#34; ipywidgets opencv-python scipy RUN mkdir -p /home/jovyan/.cache/huggingface strategy: type: Docker output: to: kind: ImageStreamTag name: jupyter-demo:latest Note\nIn the above BuildConfig, there might be newer versions of the software installed by pip. It may be necessary to update and use a newer version. We\u0026rsquo;re going to use OpenVINO project notebooks, which already execute some pip commands to install required libraries in any case.\nLet\u0026rsquo;s now start the build of the image with:\n$ oc start-build jupyter-demo And once it\u0026rsquo;s finished, we can check that the image appears with:\n$ oc get is NAME IMAGE REPOSITORY TAGS UPDATED jupyter-demo image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo latest 2 minutes ago If we want to check the builds with oc logs -f build/\u0026amp;lt;\u0026amp;lt;buildname\u0026amp;gt;, we\u0026rsquo;ll see an output similar to this one:\n$ oc logs -f build/jupyter-demo-1 time=\u0026#34;2024-03-13T12:50:47Z\u0026#34; level=info msg=\u0026#34;Not using native diff for overlay, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled\u0026#34; I0313 12:50:47.551349 1 defaults.go:112] Defaulting to storage driver \u0026#34;overlay\u0026#34; with options [mountopt=metacopy=on]. Caching blobs under \u0026#34;/var/cache/blobs\u0026#34;. Pulling image quay.io/jupyter/base-notebook ... Trying to pull quay.io/jupyter/base-notebook:latest... Getting image source signatures ... ... STEP 9/11: RUN mkdir -p /home/jovyan/.cache/huggingface --\u0026amp;gt; 284cd3e642a7 STEP 10/11: ENV \u0026#34;OPENSHIFT_BUILD_NAME\u0026#34;=\u0026#34;jupyter-demo-1\u0026#34; \u0026#34;OPENSHIFT_BUILD_NAMESPACE\u0026#34;=\u0026#34;openshift-kmm\u0026#34; --\u0026amp;gt; 9bba674b8144 STEP 11/11: LABEL \u0026#34;io.openshift.build.name\u0026#34;=\u0026#34;jupyter-demo-1\u0026#34; \u0026#34;io.openshift.build.namespace\u0026#34;=\u0026#34;openshift-kmm\u0026#34; COMMIT temp.builder.openshift.io/openshift-kmm/jupyter-demo-1:d93bad68 --\u0026amp;gt; c738f8d15e38 Successfully tagged temp.builder.openshift.io/openshift-kmm/jupyter-demo-1:d93bad68 c738f8d15e38ea41f9a17082a435b4e8badf2e7d0569f34c332cf09102b0992d Pushing image image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo:latest ... Getting image source signatures Copying blob sha256:c37f7a4129892837c4258c045d773d933f9307d7dcf6801d80a2903c38e7936c ... ... sha256:59ebd409476f3946cadfccbea9e851574c50b8ef6959f62bdfa2dd708423da30 Copying config sha256:c738f8d15e38ea41f9a17082a435b4e8badf2e7d0569f34c332cf09102b0992d Writing manifest to image destination Successfully pushed image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo@sha256:f9ee5ae8fa9db556e90908b278c7ebb2d2ad271e11da82cfad44620d65834bf8 Push successful We need to define a StorageClass to use an LVM for the underlying storage. Then, a volume creation with persistent storage (PVC) is registered so that the space is allocated and prepared for our application and a secondary one for the cache of the application.\nWe\u0026rsquo;ll use that space later on to download the Jupyter notebooks that we\u0026rsquo;ll be using for the GPU demonstration.\nFor the following items, apply each one with:\n$ oc apply -f \u0026amp;lt;file.yaml\u0026amp;gt; StorageClass Let\u0026rsquo;s create a file with the following contents defining our Storage Class named sc.yaml\nallowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: lvms-vg1 parameters: csi.storage.k8s.io/fstype: xfs topolvm.io/device-class: vg1 provisioner: topolvm.io reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer Persistent Volume Claim Similarly, we need to create a file for registering the storage that we\u0026rsquo;ll be naming pvc.yaml with the following contents:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc spec: storageClassName: lvms-vg1 accessModes: - ReadWriteOnce resources: requests: storage: 20Gi PVC for cache Finally, the application uses some cache, so another PVC will be created using a file named cache.yaml with the following contents:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: huggingface-cache spec: storageClassName: lvms-vg1 accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Make sure you\u0026rsquo;ve applied the above files as instructed before following from this point.\nAt this point, we\u0026rsquo;ve created the Build, ImageStream, StorageClass, PVC, and PVC for Cache; because we launched the build, we will also have the ImageStream populated, so we\u0026rsquo;re ready to run the application.\nRunning the application A Jupyter notebook is an interactive workbook where commands, outputs like visualizations, etc., can be shown alongside the code.\nIt\u0026rsquo;s commonly used in data science analysis as it allows you to quickly edit and amend the commands and refresh the output with the new values.\nUsing the image generated in the previous Containerfile and the previous storage PVCs, we can create a pod with this YAML:\napiVersion: v1 kind: Pod metadata: name: kmm-demo-jupyter spec: containers: - name: kmm-demo-jupyter image: image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo args: # Password is paella in case you want to reuse - start-notebook.py - --PasswordIdentityProvider.hashed_password=\u0026#39;argon2:$argon2id$v=19$m=10240,t=10,p=8$00Ynt8+Jk4sMtJUM+7Us5Q$ycb5PzmA7IH9yfOPAIfUjMNvDzXHKiMXPvM6+R5nucQ\u0026#39; env: - name: GRANT_SUDO value: \u0026#34;yes\u0026#34; - name: NB_GID value: \u0026#34;110\u0026#34; ports: - containerPort: 8888 hostPort: 8888 resources: limits: cpu: \u0026#34;20\u0026#34; memory: \u0026#34;64Gi\u0026#34; securityContext: privileged: true runAsUser: 0 runAsGroup: 110 volumeMounts: - name: dev mountPath: /dev - name: huggingface-cache mountPath: /home/jovyan/.cache/huggingface - name: work mountPath: /home/jovyan/work volumes: - name: dev hostPath: path: /dev - name: huggingface-cache persistentVolumeClaim: claimName: huggingface-cache - name: work persistentVolumeClaim: claimName: pvc Once the app is ready we can forward a local port (for example, to reach the application), but first we\u0026rsquo;ll prepare the examples we\u0026rsquo;ll be using with the Jupyter notebooks from the OpenVINO project by getting inside our pod with:\n$ oc rsh kmm-demo-jupyter \u0026hellip;then, once we\u0026rsquo;re inside the pod:\n$ pwd /home/jovyan $ ls work $ git clone https://github.com/openvinotoolkit/openvino_notebooks chown -R jovyan:users openvino_notebooks Note\nIf you don\u0026rsquo;t specify an authentication method on the pod, a token will be printed on the pod logs, that should be used when reaching the Jupyter interface.\nUsing the following command, we\u0026rsquo;ll be forwarding a port from your computer to the pod itself so it will be easier to interact with the Jupyter notebook running there:\n$ oc port-forward kmm-demo-jupyter 8888:8888 Once done, on your local computer browser open http://localhost:8888 to access the notebook.\nIn the example above, the hashed password is paella, and it\u0026rsquo;s the one we\u0026rsquo;ll be using to access the Jupyter notebook.\nFrom within the browser, you can access the previous URL, navigate the notebooks, and select 225-stable-diffusion-text-to-image so that the final URL is:\nhttp://localhost:8888/lab/tree/openvino_notebooks/notebooks/225-stable-diffusion-text-to-image/225-stable-diffusion-text-to-image.ipynb\nSkip over the explanation steps and navigate to the area with the GPU selection drop-down (Figure 1).\nFigure 1: GPU selection dropdown\n\u0026hellip;and later, the keyword section (Figure 2).\nFigure 2: keyword selection for imput\nIn this section, you can describe what kind of image should be generated. This is where the real magic happens. In this case, we will use the prompt Valencia fallas sunny day and see what kind of image is generated. See Figure 3.\nFigure 3: Output of the pipeline for the initial set of words Of course, you can go back, edit the input keywords, and try new ones (Figure 4)\nFigure 4: Output of the pipeline for the additional set of words\nWrap up We hope that you\u0026rsquo;ve enjoyed this read and realize how KMM, NFD, and DTK make managing custom drivers across many nodes across a cluster much easier than having to log into each node individually to install drivers.\nWe hope that you\u0026rsquo;ve enjoyed this read and realize how quick and convenient it is to use KMM, NFD, and DTK to enable support for accelerators in your OpenShift infrastructure.\nBut I do still want to upgrade my cluster! Don\u0026rsquo;t worry, KMM automatically will check the modules configured for your hosts and the kernel they are running—if you\u0026rsquo;re using the prebuilt images, KMM will download and enable the prebuilt image and if you\u0026rsquo;re using a custom build, a new build process will happen so that the new image is available\u0026hellip; and using the labels added by KMM you can schedule your workloads on the nodes that have the driver ready for consumption.\nIt\u0026rsquo;s still recommended to do a staggered upgrade, so only a few nodes are updated before moving into others to avoid, for example, a new kernel having some issues with the build process or no prebuilt driver being available because it is still under the certification and validation process\u0026hellip; rendering workloads requiring a specific device driver to become unschedulable.\nOnce you\u0026rsquo;ve checked that the driver is available at the preceding link and checked the official documentation on the OpenShift Upgrade process, be ready!\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/","summary":"Learn how to enable GPU acceleration with the Kernel Module Management (KMM) Operator so you can can run AI workloads on top of Intel Data Center GPU Flex 140.","title":"Enable GPU acceleration with the Kernel Module Management Operator"},{"content":"In order to test locally a Gemfile, define local path for the gems to avoid attempting to write to system-wide folders:\nbundle config set --local path \u0026#39;/home/username/.gem\u0026#39; bundle install ","permalink":"https://iranzo.io/tips/bundle-localuser/","summary":"\u003cp\u003eIn order to test locally a Gemfile, define local path for the gems to avoid attempting to write to system-wide folders:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebundle config set --local path \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;/home/username/.gem\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebundle install\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Install gems on local user folder instead of system wide"},{"content":"Telegram is being used frequently at schools for allowing an easy communication flow using the \u0026lsquo;channels\u0026rsquo;, where teachers can send information to families without sharing their personal contact details so that they can\u0026rsquo;t be contacted outside of the official tools.\nThis comfort, came, probably because of not knowing the problems/dangers that were not taken into consideration.\nFor making it easier for parents to join the channel of the class, many times, public channels are created\u0026hellip; this allows to use a shorter \u0026lsquo;alias\u0026rsquo; but that everyone else can search and find\u0026hellip; and here is the problem\u0026hellip; \u0026rsquo;everyone\u0026rsquo; it\u0026rsquo;s not just parents or other teachers\u0026hellip; if you do the test by searching for clase de or classe dels you\u0026rsquo;ll find lot of groups:\nThose groups, apart from general information, can also contain pictures or videos from the class that shouldn\u0026rsquo;t be available publicly for everyone else in Internet.\nAdvices Use Telegram instead of Whatsapp Bigger control in group, channel creation with options, permissions, etc. No need to share your phone number Ability to share channels for information sharing without your number being public If a device is reinstalled or replaced, when account is configured, all attachments, media, etc are still available. Don\u0026rsquo;t create channels or public groups It will make it harder to provide access via the link, but it\u0026rsquo;s not that hard to create a QR or share with some parents so that they can share between them, but in the other hand, no external members, nor internet search engines, etc. will be able to find them (and will not be able to see the content without joining).\nWhen a channel or group is public, without being a member, messages can be seen as well as group members, etc.\nDo not forward messages When you forward a message, depending on the configuration from the person who sent the initial message, their name and even a link to their profile to start a conversation can be shared\u0026hellip; for example, your partner records a video that you want send to the group, and when you forward it to your group\u0026hellip; people in the group can click on message forward from and end up sending messages to your partner, to another teacher, etc.\nInstead, download the content, then re-upload it to the group.\nUse invitation links with limit of users or expiration date When you create an invitation link to a group or channel, by default, the link is valid forever:\nClick on Create a new link and setup some limits:\nFor example, for a school group, it might make sense to make it valid for joining for 1 or 2 months, and being used maximum by 50 users (2 parents/kid), so, in this way, we can avoid that if the join link gets exposed, many people could join the group with unwanted users.\nIf you think that a link has been exposed, using the \u0026lsquo;3 dots\u0026rsquo; menu, you can cancel it, remove it, so that no one else could use until a new one is created.\nValidate group members As you\u0026rsquo;ve the phones of the parents from the data they filled, add them to your agenda, add the name of the parent, the kid, so that when you check the list of members you can find out names that shouldn\u0026rsquo;t be there.\nUse pinned messages It\u0026rsquo;s very easy to send loads of information, together with pictures, material to bring for arts\u0026amp;crafts, etc\u0026hellip; it\u0026rsquo;s good to make use of pineed messages (one or more) with general information or relevant for a period like:\nMeeting days and schedule Time table Official center communication channels Official calendar (bank holidays, etc.) Library or other services information. Rules for behaviour And others, that can be pinned when it\u0026rsquo;s needed to bring materials, an authorization for a trip, etc\u0026hellip; in this way, it\u0026rsquo;s easy to \u0026lsquo;browse\u0026rsquo; between important ones, while other stuff stays in between in the message flow.\nSet groups and channels to be time bound If a group is used during one school year, once it finishes, it makes sense to keep it for a little longer to allow parents to download materials, pictures as a memory or even to share the link for the new group for the next school year\u0026hellip; but after some period (for example another year)\u0026hellip; delete the group/channel\u0026hellip; leaving them forever will not be very useful unless you want to reuse materials, and, in that case, better to keep it in another group for \u0026lsquo;archival\u0026rsquo; purposes.\nSend a reminder (and set it as pinned) explaining when the group will be removed and once again in one month before that day.\nAutomatic deletion In the same line, you can establish in the channels that messages are deleted automatically, allowing this cleanup to happen by itself:\nPeriod of time:\nOnce it\u0026rsquo;s enabled, it appears indicated in the lower part when sending new messages:\nIn this case, it\u0026rsquo;s set for 1d (1 day).\nChats or groups instead of channels If for whatever reason (I wouldn\u0026rsquo;t recommend), you want to create a chat where users could participate, consider seriously to enable the \u0026lsquo;anonymous\u0026rsquo; permission: In this way, even if you\u0026rsquo;re in the group with everyone else, whenever you write, the group name will appear instead of yours, blocking in that way that people could contact you directly via private messages but still interact via the group.\nAs a user they\u0026rsquo;ll see:\nWhen you write, they\u0026rsquo;ll see:\nBut you\u0026rsquo;ll still be able to see everyone:\nSo, in this way, you can participate, provide information, etc. but without being contacted on your personal contact.\n1 to 1 messages Within telegram you can also send private messages and get answers in private without sharing your phone number.\nOnly if you add the contact and choose to share number the other person will be able to see it and call you outside the Telegram application.\nThink about hiding members list If anyone can see members of groups, it\u0026rsquo;s possible to have users receiving spam\u0026hellip; a way to avoid it is to make that only administrators can see the members list.\nPictures in low and high resolution When a picture is sent, sending them compressed is nice to see them, but for keeping those as memories, it\u0026rsquo;s better to send them in full size as \u0026lsquo;file\u0026rsquo;\u0026hellip; many times it\u0026rsquo;s sad not to be able to keep those memories in full resolution as, by default, those are sent reduced.\nOnce uploaded in high resolution you can even remove them from your phone, as they\u0026rsquo;ll be available in the channel and you can download them if is ever needed.\nUse scheduled messages Telegram allows to write messages and program the date/hour those are being sent:\nOnce selected, choose the moment:\nThis allows us to, whenever we\u0026rsquo;ve the time, program a message or messages with reminders (materials, trips, special activities, etc.) without depending on writing them on that very same moment those are happening, giving us a lot more flexibility for information delivery planning and when it\u0026rsquo;s going to better to send it:\nHave a little period of time and sending 20 messages in that moment will just cause confusion and make some messages to be unseen by recipients. Using that time to schedule messages at the best timing for each one, will make them that recipients will pay attention to each message and the information is not lost between other messages. Use text instead of images If you add text when you\u0026rsquo;re adding an image, you can use later the search function to locate it, but if you just put images\u0026hellip; you\u0026rsquo;ll have to check one by one to find the information. You can combine both and put the picture with the caption to get best of both options!\nUse polls Telegram allows you to create polls to ask about information/feedback based on the vote responses\u0026hellip; take an advantage out of it!\nUse a calendar for publishing events If you\u0026rsquo;ve read Showing calendar events in Telegram you\u0026rsquo;ll know that you can use bots that will take care of publishing events in channels or groups\u0026hellip; take an advantage so that in the same moment you manage your agenda, you can plan events that could be interesting for families.\n","permalink":"https://iranzo.io/blog/2024/01/05/advice-for-telegram-usage-in-education/","summary":"\u003cp\u003eTelegram is being used frequently at schools for allowing an easy communication flow using the \u0026lsquo;channels\u0026rsquo;, where teachers can send information to families without sharing their personal contact details so that they can\u0026rsquo;t be contacted outside of the official tools.\u003c/p\u003e\n\u003cp\u003eThis comfort, came, probably because of not knowing the problems/dangers that were not taken into consideration.\u003c/p\u003e\n\u003cp\u003eFor making it easier for parents to join the channel of the class, many times, public channels are created\u0026hellip; this allows to use a shorter \u0026lsquo;alias\u0026rsquo; but that everyone else can search and find\u0026hellip; and here is the problem\u0026hellip; \u0026rsquo;everyone\u0026rsquo; it\u0026rsquo;s not just parents or other teachers\u0026hellip; if you do the test by searching for \u003ccode\u003eclase de\u003c/code\u003e or \u003ccode\u003eclasse dels\u003c/code\u003e you\u0026rsquo;ll find lot of groups:\u003c/p\u003e","title":"Advice for Telegram usage in education"},{"content":"Red Hat has a program named Co.Lab which provides kits under request that employees can use to guide a STEM session on schools to perform different range of activities.\nWe performed 2 sessions this year, one for the \u0026ldquo;We Are Red Hat Week\u0026rdquo; (WARHW), which happens every year around Halloween, when the first official release was launched back in 31st October 1994 (\u0026lsquo;Halloween Release\u0026rsquo;).\nFor the one at the office in Valencia, the range of ages was a bit wider so we did different kits and the parents were in charge of directing the lab for the kids.\nLater, we performed one at a primary school where 7 years-old kids were constructing a \u0026lsquo;Conversation Machine\u0026rsquo;.\nWe followed this presentation to provide some guidance and introduction based on the work by my colleagues from Barcelona as well as the video that explains the kit.\nThe kit itself contained everything we needed: some instructions about the program, some LEDs, resistors, battery, breadboard, switches and the breadboard to assemble everything.\nOf course\u0026hellip; doing it for almost 50 kids required a bigger box:\nAgain, all of this was provided thanks to Red Hat.\nThe activity was carried with the help of several parents that got an early kit to start practicing at home to be ready to help the students during the experience and finally a simplified version of the circuit was agreed:\nWith this one, we reduced the wiring, so they had more room to place the components so that we also upgraded to use 4 switches and 4 LEDs instead of the original plan which was aiming for just 3.\nTo be honest, the guidance from the parents during the construction made it a very pleasant experience, not a lot of frustration from the kids, as they were able to progress on their own (it was funny to see how they were eager to start opening the boxes for the kits before listening to the explanation) and how they were trying to get it done as soon as possible, but in the end, more or less everyone finished at the same time so we were ready for the last step\u0026hellip;.\nPlaying with it!\nWe decided to take advantage of the 4th led (red, yellow, green and blue) and play a \u0026lsquo;hot-cold\u0026rsquo; game where one teacher or adult left the classroom, the students agreed on someone, and when joining back, the adult should guess who was the selected person to be found\u0026hellip;\nUsing the led lights the kids were indicating how \u0026lsquo;hot, warm, cold, or frozen\u0026rsquo; we were in terms of distance.\nThey really enjoyed a lot the assembling and the game and everyone wanted to be the next one selected for being found by others!\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2023/11/10/red-hat-co.lab-for-the-day-of-science-for-development-conversation-machine/","summary":"\u003cp\u003eRed Hat has a program named Co.Lab which \u003ca href=\"https://www.sparkfun.com/search/results?term=co.lab\"\u003eprovides kits\u003c/a\u003e under request that employees can use to guide a STEM session on schools to perform different range of activities.\u003c/p\u003e\n\u003cp\u003eWe performed 2 sessions this year, one for the \u0026ldquo;We Are Red Hat Week\u0026rdquo; (\u003ccode\u003eWARHW\u003c/code\u003e), which happens every year around Halloween, when the first official release was launched back in 31st October 1994 (\u003ca href=\"https://www.redhat.com/en/blog/spooktacular-tale-red-hats-halloween-release\"\u003e\u0026lsquo;Halloween Release\u0026rsquo;\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eFor the one at the office in Valencia, the range of ages was a bit wider so we did different kits and the parents were in charge of directing the lab for the kids.\u003c/p\u003e","title":"Red Hat Co.Lab for the day of Science for Development \"Conversation Machine\""},{"content":"With recent releases of OpenShift like 4.13 you can use CoreOS Layering to apply custom images to the nodes.\nThe feature allows to build, via a Dockerfile a custom image that can later be applied to our nodes.\nLet\u0026rsquo;s review the steps:\nFirst we need to find the base image being used in our environment with oc adm release info quay.io/openshift-release-dev/ocp-release:4.13.5-aarch64 --image-for=rhel-coreos Then we use the returned value in the FROM line in our Dockerfile If we want to add custom packages, we should have a server which is reachable and run createrepo on the folder containing the rpm\u0026rsquo;s so that rpm-ostree can download them for installation. Example dockerfile:\n# Filled from the value obtained at 1st step FROM quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:394b83e0d67ea49314ba250e6e32710c5a7b807a19746c19f4f16d350b8636dd RUN echo -e \u0026#39;[rt]\\nbaseurl=http://192.168.53.117/RTARM/\\nenabled=1\\ngpgcheck=0\\n\u0026#39; \u0026gt; /etc/yum.repos.d/rtkernel.repo # Note the following command is documented with a different approach, but this one is working, a bug was raised for fixing official docs. RUN rpm-ostree cliwrap install-to-root / # Perform the package installation step, in this case, removing the original kernel and installing another kernel instead RUN rpm-ostree override remove kernel{,-core,-modules,-modules-extra,-modules-core} --install=gobject-introspection --install=hdparm --install=kernel-rt --install=kernel-rt-core --install=kernel-rt-modules-core --install=libperf --install=libtraceevent --install=python3-dbus --install=python3-linux-procfs --install=python3-perf --install=python3-six --install=realtime-setup --install=tuna --install=tuned --install=tuned-profiles-realtime --install=virt-what --install=python3-pyudev --install=python3-gobject-base --install=kernel-rt-modules --install=python3-gobject-base-noarch RUN rpm-ostree cleanup -m RUN ostree container commit Later, to build the container we will execute podman build . --authfile /var/lib/kubelet/config.json so that it has access to download the required images (FROM field).\nAnd we\u0026rsquo;re ready to push our newly created image to the registry, for example, if our image id is: b2d0af2a733\nSo as last step we can perform podman push b2d0af2a733 quay.io/youruser/yourimage:tag to upload the image for later consumption.\nThen, we prepare a manifest for applying the change by MCO:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: master name: rhcos-kernelrt spec: osImageURL: myregistry/user/kcoreos-9:kernelrt And later apply with oc apply -f mycustomkernel.yaml\u0026hellip; MCO will take over and will start rolling out the change as we can monitor with oc describe mc rendered-master-XXXXXX.\nIf you want to rollback the change, we can of course, perform oc delete -f mycustomerkernel.yaml and MCO will revert the change.\nWe can check current booted OS image with rpm-ostree status:\nsh-5.1# rpm-ostree status State: idle Deployments: * ostree-unverified-registry:myregistry/user/kcoreos-9:kernelrt Digest: sha256:802d2f6bf8a6aef02c90ee1212de03f1888772dec6b34ab0dae97272cd6a917c Version: 413.92.202307140015-0 (2023-11-07T17:16:18Z) Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2023/11/08/openshift-layered-images-for-patching/","summary":"\u003cp\u003eWith recent releases of OpenShift like 4.13 you can use \u003ca href=\"https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html/post-installation_configuration/coreos-layering\"\u003eCoreOS Layering\u003c/a\u003e to apply custom images to the nodes.\u003c/p\u003e\n\u003cp\u003eThe feature allows to build, via a \u003ccode\u003eDockerfile\u003c/code\u003e a custom image that can later be applied to our nodes.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s review the steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFirst we need to find the base image being used in our environment with \u003ccode\u003eoc adm release info quay.io/openshift-release-dev/ocp-release:4.13.5-aarch64 --image-for=rhel-coreos\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eThen we use the returned value in the \u003ccode\u003eFROM\u003c/code\u003e line in our Dockerfile\u003c/li\u003e\n\u003cli\u003eIf we want to add custom packages, we should have a server which is reachable and run \u003ccode\u003ecreaterepo\u003c/code\u003e on the folder containing the rpm\u0026rsquo;s so that \u003ccode\u003erpm-ostree\u003c/code\u003e can download them for installation.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eExample dockerfile:\u003c/p\u003e","title":"OpenShift Layered Images for patching"},{"content":"Hi had the chance to buy a Dlink camera, waterproof, with magnetic base and rated for exteriors for a really great price a while ago D-Link DCS 8600LH 🛒#ad and later a camera with pan and tilt from Dlink too D-Link DCS-6500LH 🛒#ad\nThe camera, of course has an application for configuration, viewing the image, etc named \u0026lsquo;mydlink\u0026rsquo; which has a web counterpart at https://mydlink.com/, but unfortunately, when you try to access the website, there\u0026rsquo;s little you can do:\nWhich translates to:\nAn incompatible browser or operating system has been detected! You are using an incompatible browser or operating system and the mydlink web portal may not look, behave or function as intended. What operating systems and browsers does the D-Link mydlink web portal support? We recommend that you use Google Chrome on Windows or macOS to access the mydlink 2nd generation portal. You can also use our mobile apps to access your devices. Check mydlink mobile apps. Since the old days of ActiveX and other plugins, most of the websites should be compatible, specially when Flash disappeared\u0026hellip; so this was sounding more like an excuse rather than a technical reason, so I did reach to DLink support about this with little cooperation rather than getting the same played message from their support representative saying that there are other models for cameras with an extra letter that are compatible.\nFunny thing, is that using a browser extension like user agent switcher, you can instruct the browser (Chrome in this case) a different user agent, which I changed to: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\u0026hellip; and guess what happened?\nRight, the website started working fine\u0026hellip; so for making it into a permanent solution for an introduced \u0026lsquo;false\u0026rsquo; problem by dLink, I created a rule: With this, every time I visit mydlink website\u0026hellip; the extension uses the user agent for Chrome for Windows\u0026hellip; and the website works the way it should.\nYes, not only seeing camera names, but also being able to watch the feeds, etc from any browser\u0026hellip; no need to use your mobile device for everything when you\u0026rsquo;ve bigger screens at you.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2023/11/05/mydlink-website-from-linux/","summary":"\u003cp\u003eHi had the chance to buy a Dlink camera, waterproof, with magnetic base and rated for exteriors for a really great price a while ago \u003ca href=\"https://www.amazon.es/dp/B07KTT14BH?tag=redken-21\"\u003eD-Link DCS 8600LH 🛒#ad\u003c/a\u003e and later a camera with pan and tilt from Dlink too \u003ca href=\"https://www.amazon.es/dp/B08VNL3NHL?tag=redken-21\"\u003eD-Link DCS-6500LH 🛒#ad\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe camera, of course has an application for configuration, viewing the image, etc named \u0026lsquo;mydlink\u0026rsquo; which has a web counterpart at \u003ca href=\"https://mydlink.com/\"\u003ehttps://mydlink.com/\u003c/a\u003e, but unfortunately, when you try to access the website, there\u0026rsquo;s little you can do:\u003c/p\u003e","title":"mydlink Website from Linux"},{"content":"As you might have experienced\u0026hellip; using a recent system to connect to a legacy one could be complicated as some insecure protocols have been disabled, with a message like:\nUnable to negotiate with 192.168.2.82 port 22: no matching host key type found. Their offer: ssh-rsa,ssh-dss Create an entry like this in your .ssh/config file, so that insecure methods can be used to connect to a specific host:\nHost 192.168.2.82 HostKeyAlgorithms=+ssh-rsa KexAlgorithms=+diffie-hellman-group1-sha1 PubkeyAcceptedKeyTypes=+ssh-rsa User root or alternatively on the command line:\nssh -oHostKeyAlgorithms=+ssh-rsa -oPubkeyAcceptedKeyTypes=+ssh-rsa root@192.168.2.8 ","permalink":"https://iranzo.io/tips/no-matching-key-found/","summary":"\u003cp\u003eAs you might have experienced\u0026hellip; using a recent system to connect to a legacy one could be complicated as some insecure protocols have been disabled, with a message like:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eUnable to negotiate with 192.168.2.82 port 22: no matching host key type found. Their offer: ssh-rsa,ssh-dss\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCreate an entry like this in your \u003ccode\u003e.ssh/config\u003c/code\u003e file, so that insecure methods can be used to connect to a specific host:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eHost 192.168.2.82\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tHostKeyAlgorithms=+ssh-rsa\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tKexAlgorithms=+diffie-hellman-group1-sha1\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  PubkeyAcceptedKeyTypes=+ssh-rsa\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tUser root\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eor alternatively on the command line:\u003c/p\u003e","title":"No matching key found"},{"content":"I had two Raspberry Pi systems running Raspbian and they were failing to find updates for newer packages.\nAs Debian stable was upgraded too, moving from buster to bullseye the packages failed to get the newer ones.\nWarning\nBeware as this procedure might upgrade the system but might no render a bootable Raspberry Pi\nA way to fix it is, to first, change references, if any, to the old codename version by running:\nsed -i \u0026#39;s/buster/bullseye/g\u0026#39; /etc/apt/sources.list sed -i \u0026#39;s/buster/bullseye/g\u0026#39; /etc/apt/sources.list.d/raspi.list And then, accept the updated new files for the new one:\napt-get update --allow-releaseinfo-change Finally, we can perform the upgrade by first, installing some dependencies that will be required:\napt update apt install libgcc-8-dev gcc-8-base apt full-upgrade With this, we get the new repository metadata, install the new required dependencies and perform the upgrade itself.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/tips/raspbian-upgrade/","summary":"\u003cp\u003eI had two Raspberry Pi systems running Raspbian and they were failing to find updates for newer packages.\u003c/p\u003e\n\u003cp\u003eAs Debian \u003ccode\u003estable\u003c/code\u003e was upgraded too, moving from \u003ccode\u003ebuster\u003c/code\u003e to \u003ccode\u003ebullseye\u003c/code\u003e the packages failed to get the newer ones.\u003c/p\u003e\n\u003cdiv class=\"admonition warning\"\u003e\n    \u003cp class=\"admonition-title\"\u003eWarning\u003c/p\u003e\n    \u003cp class=\"admonition\"\u003eBeware as this procedure might upgrade the system but might no render a bootable Raspberry Pi\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003eA way to fix it is, to first, change references, if any, to the old codename version by running:\u003c/p\u003e","title":"Upgrade Debian from buster to bullseye"},{"content":" Attention\nFirst published at https://cloud.redhat.com/blog/scale-testing-kernel-module-management\nIntroduction Kernel Module Management (KMM) Operator manages, builds, signs and deploys out-of-tree kernel modules and device plugins on OpenShift Container Platform clusters.\nKMM adds, for the HUB/Spoke scenario, a new ManagedClusterModule which describes an out-of-tree kernel module and its associated device plugin. You can use ManagedClusterModule resources to configure how to load the module, define ModuleLoader images for kernel versions, and include instructions for building and signing modules for specific kernel versions.\nKMM is designed to accommodate multiple kernel versions at once for any kernel module, allowing for seamless node upgrades and reduced application downtime.\nFor more information about it, refer to the product documentation available at Kernel Module Management Operator | Specialized hardware and driver enablement | OpenShift Container Platform 4.12\nDon\u0026rsquo;t forget that also KMM is a community project at kubernetes-sigs/kernel-module-management and that it can be tested on upstream Kubernetes and has a slack community channel at Kubernetes Slack.\nVocabulary In order to make it easier, we\u0026rsquo;re going to use some acronyms and products along this text, find here the most common ones:\nTerm Definition KMM Kernel Module Management ACM Advance Cluster Management OpenShift Red Hat\u0026rsquo;s Kubernetes-based product HUB Central management Cluster that via ACM manages some Spokes Spoke Cluster managed via ACM from a management cluster referred to as Hub SNO Single-Node OpenShift CRD Custom Resource Definition EDGE Relevant to Telco 5G and other use cases, refers to systems that are placed close to the end user making use of the services to get better performance OOT Out-of-tree, referred to Kernel Module The test goal One of the new features coming in KMM 1.1 is the ability to work in Hub-Spoke architectures, by leveraging Advanced Cluster Management capabilities, deploying KMM in this hub-spoke architecture is like a walk in the park.\nThis kind of setup is very common at the EDGE, where data centers have a more resourceful set of servers, while EDGE devices are more resource-constrained, and everything that can be saved, can be used to provide a better experience to the closest users.\nIn this mode, KMM is able to build new kernel drivers for specific releases on the hub cluster, and then, deliver the built images to the spokes clusters, which, using fewer resources can still benefit from hardware enablement which automatically gets updated for each kernel when a newer OpenShift version image gets released.\nThe KMM team wanted to perform these tests and grab metrics on the behavior in a large-scale environment that allowed them to evaluate the action points on the HUB-Spoke scenario.\nThe goal for this test is to reach 1000 nodes deployed with the KMM module and monitor the actual resource utilization for the whole process.\nTest environment We reached out to the ScaleLab team at https://scalelab.redhat.com/ to explain our use case and to request a loan of machines we could use for our testing.\nThe team provided 69 systems in total, all equal, and with the following specifications:\nDell R650 with 512Gb of RAM, 3TB NVME, 21.8Tb SSD + 1 447GB SSD, powered by the Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz processor reported as 112 CPU\u0026rsquo;s (dual processor, 56 cores, 112 threads). In order to set up the environment, we used 4 hosts for base infrastructure, designed to become:\n1 Bastion for deploying other hosts and interacting with the environment (Ansible playbook execution, mirror registry, etc.). 3 nodes for creating an OpenShift Baremetal Compact cluster that will hold the hub cluster. The remaining 65 hosts were configured as hypervisors for running virtual machines with KVM to be used as Single Node OpenShift (SNO) clusters.\nNode Hardware CPU RAM(GiB) Disk(GiB) Count Bastion Dell R650 112 512 446 SSD / 2x1.8TB SSD / 2.9TB NVMe 1 Hub BM Cluster Dell R650 112 512 446 SSD / 2x1.8TB SSD / 2.9TB NVMe 3 SNO HV Dell R650 112 512 446 SSD / 2x1.8TB SSD / 2.9TB NVMe 65 SNO VM Libvirt KVM 8 18 120 1755 (max) It might sound easy to achieve, but in reality, there are several things to take into account in order to set up the whole environment:\nConfigure all the relevant hosts and get access to them using Ansible. Enable the required packages and services for virtualization. Define the network cards for the VM\u0026rsquo;s to be interconnected and able to reach the hub. Have proper sizing and placing of the VM\u0026rsquo;s so that the faster hard drives are used for holding the VM disks and avoid extra workload because of VM density causing disk pressure on the same drive. Most of this logic is already present in the scripts at https://github.com/redhat-performance/jetlag/ repository, which is pretty tied to the SCALELAB environment that we were going to use for this setup to prepare and configure the relevant resources for this task.\nIn total we got 27 VMs per host, getting to a total of 1755 potential SNOs. Note the word potential here… we\u0026rsquo;re setting up the infrastructure to mimic real hardware:\nRemotely power on and off the VMs (using sushy-tools to interact with libvirt). KVM for running the VMs. Etc. Each SNO is configured with 8 VCPU and 18Gb of RAM, which is in line with the bare minimum requirements to get OpenShift deployed.\nFor example, we had to alter the limits for the KMM deployment in order to allow it to use more memory during the module build, which was initially limited to 128Mb of RAM and was not enough for the compilation of the module.\nWe were pretty lucky and after the whole process of deployment in this restricted scenario, configuration, ACM deployment, and provisioning of the clusters, only one cluster failed ‘sno01412’, out of 1755 SNOs in total.\nNote that some other spokes will be discarded in the next paragraphs.\nFor this setup, and to avoid external issues, we decided to do the installation following the disconnected IPv6 approach, that is, the bastion is running an internal registry which is later configured in all clusters via a CatalogSource:\napiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: rh-du-operators namespace: openshift-marketplace spec: displayName: disconnected-redhat-operators image: d20-h01-000-r650:5000/olm-mirror/redhat-operator-index:v4.12 publisher: Red Hat sourceType: grpc updateStrategy: registryPoll: interval: 1h This process is performed automatically as part of the scripts used to deploy the environment, but we wanted to validate and fix some hosts that failed to apply the policy that automatically was configuring it.\nIn order to prepare the setup, we also had to mirror into our registry the KMM Operator, and for the reporting, we also added the Grafana operator.\nAs a side issue, we ran out of disk space in the bastion, as we were only using the 500 GB SSD, so we\u0026rsquo;ve noted this to automatically set additional space on /var/lib/containers on the bastion using extra disks.\nAlso, as we switched from IPv4 to IPv6, we were required to perform some manual cleanup of leftovers that could have caused some of the instabilities we saw in previous attempts.\nThe Grafana instance OpenShift already includes a Grafana dashboard, together with ACM, that can show some information about the bare-metal cluster we will use for management.\nHowever, in order to be able to customize the dashboard, we are required to install a custom instance. This custom instance is what we\u0026rsquo;ll show in the next screenshots to highlight the behavior of the cluster and the numbers obtained.\nOur results As a briefing, we\u0026rsquo;ve 1754 valid possible SNOs (deployed and in operation).\nOut of those, 6 spokes were removed, as they failed in different steps (test upgrade of OpenShift, operators in a non-working state, etc).\nSo 1748, is the real number of spokes available for deploying KMM\nKMM installation For deploying KMM we used the operator, to get it added on the Hub.\nUsing ACM, we defined a policy to also add it to the managed clusters, so that\u0026rsquo;s what we\u0026rsquo;ll be seeing in the next graphs.\nFirst, the KMM Hub installation started at 11:42, and we can see some data on the following graph:\nAs we started the spokes installation in the period between 12:17 and 12:23 we can also see that after the initial setup of the hub and some activity during the spokes installation, the usage of resources is more or less steady, we can see a bump to a new level around 13:30, but this was once all the activity for deployment has finished.\nWe can compare this pattern as well with the SNO\u0026rsquo;s resources usage:\nThe SNOs were already active but without too much load in RAM or CPU, as a big part of the installation is just deploying the required components in the clusters.\nIf we focus on the average graphs to avoid the spikes of regular activity we can see some patterns:\nAnd for the spokes:\nIn general, Hub has been unaffected, and in the SNOs, we have a bit more activity in average and RAM usage, but if we check the numbers it\u0026rsquo;s around 200Mb of RAM usage in difference, so not a big deal in terms of resource consumption.\nIn terms of KMM controllers, we can see it really clearly here:\nThe number of KMM controllers increased in a really short period of time and got to the total number of operative SNOs\nModule installation Now that KMM has been installed, we need to actually deploy a module, this will cause KMM to compile it, and prepare the relevant images that the Spokes will consume.\nThe module used for this test is a really simple module called kmm-kmod which is part of the KMM project. It just prints a “Hello World” and a “Goodbye World” message to the kernel log but it is suitable for testing building and loading kernel modules with KMM like any other ‘production’ module would be.\nIn our case, we first got the module compiled and installed prior to our testing so that we can compare the workload increase when the module is already prepared in the hub and ready for distribution.\nIn this case, the container that was already working in the HUB started to be deployed and we can check the count change here:\nAround 13:30, all SNOs got the module container up and running and repeated the graphs provided earlier for the whole period, we can see that hub increased memory usage\nAnd for the spokes:\nAs we can see that once KMM was installed, a bump in the memory (under 200Mb) happened, but no appreciable change once the module was loaded.\nNote that in this situation, the hub creates the module (compilation, etc), builds the image, and then ACM does the deployment to the spokes.\nFrom the numbers, we see that 100% of the operative SNOs deployed the KMM controller and loaded the module within a really short timeframe with no noticeable impact.\nModule upgrade One of the tests we wanted to perform was to get SNOs to do a kernel upgrade, in this way, we can test KMM by doing the new module compilation itself and then, delivering the updated kernel module to the spoke clusters.\nThis, of course, has an implication… a way to get a newer kernel on the SNO nodes… is by actually upgrading OpenShift itself.\nThis, which might sound easy, means having to mirror the required images for the newer OpenShift release, apply an ICSP to all the SNOs, as well as adding the signatures for validating the new image, and of course, launch the upgrade itself.\nThe Hub started the upgrade at 16:20, moving from a starting OpenShift version 4.12.3 to the final version 4.12.6 which was previously mirrored, with the relevant signatures added, etc.\nThe upgrade took around 90 minutes in total for the Hub, and once the Hub moved to 4.12.6, it recompiled the Kernel module.\nIn our tests, we tested manually on the first SNO (sno0001), and once validated, this was launched for all the remaining and active nodes.\nThe timing was the following:\n18:22 First SNO (sno0001) and others start applying YAML for version signatures and for ICSP (just missing the upgrade itself). 18:24 script to launch all the upgrades in parallel started. 19:38: 1500 updated, 249 in progress. 19:49: 4 nodes failed the OpenShift Upgrade. 22:51: all SNOs, including the failing ones, are upgraded to 4.12.6. Out of the total count, 4 nodes didn\u0026rsquo;t come back (API not responding), a manual reboot of the SNO\u0026rsquo;s got them into API responding, and apparently good progress towards finishing the upgrade to the 4.12.6 release.\nThe upgrade itself, as it took time, caused no appreciable load on the Hub itself, but as highlighted, several SNO hosts did not perform the upgrade properly.\nFinally as just a confirmation about what we were expecting:\nThe Hub did perform a build once the first SNO required the new kernel, but the spokes did no builds at all (which is the exact use case of HUB-Spoke architecture)\nSummary Milestones Check the graphs and milestones for the whole process\n11:42 KMM installation at HUB. 12:17-12:23 KMM installation at SNOs (controller). 13:30 KMM KMOD added and deployed to all SNOs. 16:20 HUB OpenShift upgrade from 4.12.3 to 4.12.6. 17:50 Hub upgrade finished. 18:22 First SNO (sno0001) and others start applying YAML for version signatures and for ICSP (just missing the upgrade itself). 18:24 script to launch all the upgrades in parallel started. 19:38: 1500 updated, 249 in progress. 19:49: 4 nodes failed the OpenShift Upgrade. 22:51: all SNOs, including the failing ones, are upgraded to 4.12.6. Metrics about spokes\nMetric Spokes # Potential 1755 Spokes not working properly 7 Operative 1748 KMM installed 1748 KMOD deployed 1748 KMOD updated 1748 Results For KMM Operator deployment at Hub, OperatorHub via UI has been used and a Policy has been applied with oc client in order to deploy the controller at spokes:\nKMM DEPLOYMENT Deployment time CPU utilization post-deployment MEM utilization post-deployment Hub \u0026lt;1min 1.5% 627 MB Peak 3% 200 Mb During 0.4% 200 MB After \u0026lt; 7 min negligible 200 MB For KMM-KMOD deployment a ManagedClusterModule has been applied so the image is built on Hub and then deployed to all spokes:\nKMM KMOD Deployment Build time Deployment time CPU utilization MEM utilization Hub \u0026lt;2min N/A 30% peak Spoke (per) N/A \u0026lt; 1 min after build Spoke (avg. ) N/A \u0026lt; 1min 0.08% 80Mb Spoke (total) N/A 11 mins 0.2% No appreciable change in RAM usage For the KMM-KMOD upgrade, we used the different kernel versions between RHCOS shipped on OCP 4.12.3 and 4.12.6. Both Hub and Spokes were upgraded to 4.12.6 so the new kernel version was detected by KMM and a new KMM-KMOD was built at Hub and automatically deployed to all spokes:\nNote that some values are reported as N/A for the spokes as the operation came as part of the OpenShift Upgrade itself.\nKMM KMOD Upgrade Build time Deployment time CPU utilization MEM utilization Hub \u0026lt;2.5 mins N/A Peak of 60% (one host, one core) for a brief time during compilation Peak of 4.3Gb and stabilizing on 3.3Gb Spoke (per) N/A N/A N/A N/A Spoke (avg.) N/A N/A N/A N/A Spoke (total) N/A N/A N/A N/A Lessons learned As highlighted in the previous paragraphs this test has been affected by several infrastructure issues:\nOpenShift deployment (SNO installation) by ACM. OpenShift Upgrades (SNOs failing in the upgrade and requiring manual intervention). Also, some other difficulties played in this:\nThe infrastructure uses a dense number of VMs to simulate the clusters and this can cause: Bottlenecks on the network during the installation of SNOs. Netmask requiring adjustment when using IPv4, or using IPv6. Using IPv6 requires using a disconnected environment, which requires extra work for ‘simple\u0026rsquo; things like: Mirroring the images. Deploying new operators (like the custom Grafana for customizing the dashboard). We\u0026rsquo;ve contributed back some of the found issues to the jetlag repository to reduce some of the problems found, but still, there were a lot of manual tasks required for setting up the environment itself.\nIn the end, the KMM test was very smooth and the results were along the expected lines: little to no impact at all on the clusters, but all the preparation work involved in the setup, and troubleshooting everything else involved, took most of the time.\nIt\u0026rsquo;s worth considering investing extra time in shaping those scripts and setups to make them more straightforward when a team is going to scale test a product and avoid the hassle of dealing with all the nits and picks from the environment itself which could be considered as infrastructure and not really a part of the test itself.\nThanks We want to give special thanks to the following who helped contribute to the success of this endeavor:\nScale Lab team Alex Krzos Edge Pillar, Partner Accelerator team ","permalink":"https://iranzo.io/blog/2023/06/21/kmm-1.1-scale-testing/","summary":"\u003cdiv class=\"admonition attention\"\u003e\n    \u003cp class=\"admonition-title\"\u003eAttention\u003c/p\u003e\n    \u003cp class=\"admonition\"\u003eFirst published at \u003ca href=\"https://cloud.redhat.com/blog/scale-testing-kernel-module-management\"\u003ehttps://cloud.redhat.com/blog/scale-testing-kernel-module-management\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eKernel Module Management (KMM) Operator manages, builds, signs and deploys out-of-tree kernel modules and device plugins on OpenShift Container Platform clusters.\u003c/p\u003e\n\u003cp\u003eKMM adds, for the HUB/Spoke scenario, a new ManagedClusterModule which describes an out-of-tree kernel module and its associated device plugin. You can use ManagedClusterModule resources to configure how to load the module, define \u003ca href=\"https://kmm.sigs.k8s.io/documentation/module_loader_image/\"\u003eModuleLoader\u003c/a\u003e images for kernel versions, and include instructions for building and signing modules for specific kernel versions.\u003c/p\u003e","title":"KMM 1.1 Scale testing"},{"content":"If you\u0026rsquo;ve a Telegram group, it might be interesting the ability of https://t.me/redken_bot for adding a calendar ical that automates publishing each day the events in the agenda for the day.\nIf you did read Python and iCalendar ICS processing, part of the basis in that article are part of the bot and are easily used:\nFor configuring, only a few simple steps are required:\nHave a calendar ICS/webcal accessible (for example a public Google Calendar one) Have a Telegram group where we do want to publish the events Add @redken_bot to the group Specify the URL del calendar and the name Lets see some screenshots of the process for you to check how easily it can be achieved.\nCreation of a calendar for publishing First, we\u0026rsquo;ll create a calendar in Google Calendar (or use an existing one in any service that provides an URL that requires no authentication):\nCreate the calendar: View options of an existing one: The part we\u0026rsquo;re interested in, once created, is this: the private URL that appears in the options:\nIf we click on copy, we\u0026rsquo;ll get the URL to the clipboard. This URL gives access to all events, both private and public, so if you want to create a calendar for a group, it\u0026rsquo;s recommended to start from a new one instead of sharing by mistake your personal one.\nAdd @redken_bot to the Telegram group As easy as adding new members and searching for @redken_bot in the field:\nSometimes, it might ask if we can also forward old messages: After a while, we\u0026rsquo;ll get the welcome message: Don\u0026rsquo;t worry about language, the bot will automatically detect the language used in the group and, if unconfigured, after 24 hours will set it automatically to the most used one.\nAdd the calendar to @redken_bot In the chat, write the command /ical add \u0026lt;name\u0026gt; \u0026lt;URL\u0026gt; : @Redken_bot will answer with a message like:\nFrom that day, every morning @Redken_bot will send a reminder of the events for the day with location, name and how long does it last:\nThis calendar can be managed by several members (in Google Calendar case), so it\u0026rsquo;s an ideal way for a team to work on adding events or reminders of the events that will get published every day.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2023/03/21/showing-calendar-events-in-telegram/","summary":"\u003cp\u003eIf you\u0026rsquo;ve a Telegram group, it might be interesting the ability of \u003ca href=\"https://t.me/redken_bot\"\u003ehttps://t.me/redken_bot\u003c/a\u003e for adding a calendar \u003ccode\u003eical\u003c/code\u003e that automates publishing each day the events in the agenda for the day.\u003c/p\u003e\n\u003cp\u003eIf you did read \u003ca href=\"/blog/2019/09/17/python-and-icalendar-ics-processing/\"\u003ePython and iCalendar ICS processing\u003c/a\u003e, part of the basis in that article are part of the bot and are easily used:\u003c/p\u003e\n\u003cp\u003eFor configuring, only a few simple steps are required:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHave a calendar ICS/webcal accessible (for example a public Google Calendar one)\u003c/li\u003e\n\u003cli\u003eHave a Telegram group where we do want to publish the events\u003c/li\u003e\n\u003cli\u003eAdd \u003ccode\u003e@redken_bot\u003c/code\u003e to the group\u003c/li\u003e\n\u003cli\u003eSpecify the URL del calendar and the name\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLets see some screenshots of the process for you to check how easily it can be achieved.\u003c/p\u003e","title":"Showing calendar events in Telegram"},{"content":"For using Ansible it\u0026rsquo;s required to have a working set of ssh-keys already deployed.\nIf you get a set of systems that have not been provisioned by you and are missing the SSH keys, having it fixed might take a while if doing it manually. Good news is that you can use a script in expect to cover this part:\n#!/usr/bin/expect -f # set Variables set password [lrange $argv 0 0] set ipaddr [lrange $argv 1 1] # now connect to remote system spawn ssh-copy-id root@$ipaddr match_max 100000 # Check for initial connection (add key of host) set timeout 5 expect \u0026#34;yes/no\u0026#34; { send -- \u0026#34;yes\\r\u0026#34; } # Check for password prmpt set timeout 120 # Look for passwod prompt expect \u0026#34;password:\u0026#34; { send -- \u0026#34;$password\\r\u0026#34; } # send blank line (\\r) to come back send -- \u0026#34;\\n\u0026#34; expect eof This script, when used like:\nsshkeyscopy letmein mynewhost Will connect to the specified host, using letmein as password to authenticate, and use the ssh-copy-id command to load your ssh keys.\nTo further automate, we can create an ansible playbook like this:\n--- - hosts: all user: root vars: rootpassword: letmein tasks: - name: Copy ssh keys shell: sshkeyscopy {{ rootpassword }} {{ item }} with_items: \u0026#34;{{ groups[\u0026#39;all\u0026#39;] }}\u0026#34; delegate_to: localhost Using this playbook, we will delegate to localhost the connection to all of the specified hosts in the inventory and use this expect script to load the keys.\nOnce it\u0026rsquo;s done, we can test that we can ssh into the hosts without password being prompted.\n","permalink":"https://iranzo.io/tips/automatesshkeys/","summary":"\u003cp\u003eFor using Ansible it\u0026rsquo;s required to have a working set of ssh-keys already deployed.\u003c/p\u003e\n\u003cp\u003eIf you get a set of systems that have not been provisioned by you and are missing the SSH keys, having it fixed might take a while if doing it manually. Good news is that you can use a script in \u003ccode\u003eexpect\u003c/code\u003e to cover this part:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-expect\" data-lang=\"expect\"\u003e#!/usr/bin/expect -f\n# set Variables\nset password [lrange $argv 0 0]\nset ipaddr [lrange $argv 1 1]\n\n# now connect to remote system\nspawn ssh-copy-id root@$ipaddr\nmatch_max 100000\n\n# Check for initial connection (add key of host)\nset timeout 5\nexpect \u0026#34;yes/no\u0026#34; { send -- \u0026#34;yes\\r\u0026#34; }\n\n# Check for password prmpt\nset timeout 120\n# Look for passwod prompt\nexpect \u0026#34;password:\u0026#34; { send -- \u0026#34;$password\\r\u0026#34; }\n# send blank line (\\r) to come back\nsend -- \u0026#34;\\n\u0026#34;\nexpect eof\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis script, when used like:\u003c/p\u003e","title":"Automating SSH keys loading for Ansible usage"},{"content":"Lately, there\u0026rsquo;s a lot of interest on ChatGPT, and yes, it\u0026rsquo;s really impressive the way it processes the context to provide answers to the questions.\nTo give it a try, and put into play the requirements for writing a Risu plugin, I wrote some of the requirements in a paragraph:\nI want to create a bash script for checking system status.\nThe script should use return codes to indicate success, failure, information, error or skipped via the values stored in the variables $RC_OKAY, $RC_SKIPPED, $RC_ERROR, $RC_FAILED and $RC_INFO.\nThe Path to check for the files in the sosreport are specified in the var $RISU_ROOT when we\u0026rsquo;re running against a sosreport or in the regular system locations when we\u0026rsquo;re running against a live system.\nWe can know if we\u0026rsquo;re running against a live system by checking the value of the variable RISU_LIVE which is 0 when we\u0026rsquo;re running against a sosreport or 1 when we\u0026rsquo;re running on a live system.\nIf the script needs to output any relevant information in case of Error, being skipped, etc, it should always write to the standard error instead of the standard output.\nand then, send it to ChatGPT:\nMore or less those are the requirements for the plugins, so we were ready to mention the next step\u0026hellip; what we want to achieve:\nAnd the answer we get:\nAs we can see, the answer is pretty good, uses the error codes, but the interesting part is allowing to fine tune the answers:\nso the answer becomes:\nIt has improved a lot, but doesn\u0026rsquo;t work fine on the sosreport part, but to be honest, the scripting part has been more or less laid and with some minor tunings, something usable can be achieved.\nIf we analyze it, it includes the logic for checking if running against a live environment or not, stores the output in a var, and based on the value it processes several ranges and gets the output printed and different error codes used, which is not that different of what a manually written plugin does.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2023/03/02/using-chatgpt-for-writing-risu-plugins/","summary":"\u003cp\u003eLately, there\u0026rsquo;s a lot of interest on \u003ca href=\"https://chat.openai.com/chat\"\u003eChatGPT\u003c/a\u003e, and yes, it\u0026rsquo;s really impressive the way it processes the context to provide answers to the questions.\u003c/p\u003e\n\u003cp\u003eTo give it a try, and put into play the requirements for writing a \u003ca href=\"/tags/risu/\"\u003eRisu\u003c/a\u003e plugin, I wrote some of the requirements in a paragraph:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI want to create a bash script for checking system status.\u003c/p\u003e\n\u003cp\u003eThe script should use return codes to indicate success, failure, information, error or skipped via the values stored in the variables $RC_OKAY, $RC_SKIPPED, $RC_ERROR, $RC_FAILED and $RC_INFO.\u003c/p\u003e","title":"Using ChatGPT for writing Risu Plugins"},{"content":"With podman we can setup containers for being used for non root users by performing some simple steps:\nInstall required packages dnf -y install slirp4netns fuse-overlayfs crun podman shadow-utils Force the number of user namespaces (might be required on some environments):\necho \u0026#34;user.max_user_namespaces=28633\u0026#34; \u0026gt; /etc/sysctl.d/userns.conf sysctl -p /etc/sysctl.d/userns.conf Delegate Allows to define which resources are available1:\nmkdir -p /etc/systemd/system/user@.service.d cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/systemd/system/user@.service.d/delegate.conf [Service] Delegate=cpu cpuset io memory pids EOF To verify it has been done correctly, logout and login with the user and execute:\ncat \u0026#34;/sys/fs/cgroup/user.slice/user-$(id -u).slice/user@$(id -u).service/cgroup.controllers\u0026#34; The output will be: cpuset cpu memory pids\nSet uids/gids For user to do proper mapping on the containers2, we need to define non-overlapping ranges for our user, let\u0026rsquo;s say kni and store it in the files: /etc/subuid and /etc/subgid:\ncat /etc/subuid kni:200000:65536 cat /etc/subgid kni:200000:65536 Ranges should not overlap with real users in the system, or the container\nWrap up After following above steps, it should be possible to run containers with rootless users, we can verify we can get the ranges with:\npodman run --rm --cpus=0.42 --memory=42m --pids-limit 42 -w /sys/fs/cgroup docker.io/library/alpine cat cpu.max memory.max pids.max The output should be something like:\n42000 100000 44040192 42 Enjoy! (and if you do, you can Buy Me a Coffee ) https://github.com/containers/podman/blob/main/troubleshooting.md#26-running-containers-with-resource-limits-fails-with-a-permissions-error\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCheck https://github.com/containers/podman/blob/main/troubleshooting.md#10-rootless-setup-user-invalid-argument\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iranzo.io/blog/2023/01/27/enable-rootless-podman-on-fedora/","summary":"\u003cp\u003eWith podman we can setup containers for being used for non root users by performing some simple steps:\u003c/p\u003e\n\u003ch2 id=\"install-required-packages\"\u003eInstall required packages\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ednf -y install slirp4netns fuse-overlayfs crun podman shadow-utils\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eForce the number of user namespaces (might be required on some environments):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user.max_user_namespaces=28633\u0026#34;\u003c/span\u003e \u0026gt; /etc/sysctl.d/userns.conf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esysctl -p /etc/sysctl.d/userns.conf\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"delegate\"\u003eDelegate\u003c/h2\u003e\n\u003cp\u003eAllows to define which resources are available\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emkdir -p /etc/systemd/system/user@.service.d\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecat \u003cspan style=\"color:#e6db74\"\u003e\u0026lt;\u0026lt; EOF \u0026gt; /etc/systemd/system/user@.service.d/delegate.conf\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e[Service]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eDelegate=cpu cpuset io memory pids\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eEOF\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo verify it has been done correctly, logout and login with the user and execute:\u003c/p\u003e","title":"Enable rootless podman on Fedora"},{"content":"Following on the Using Kcli to prepare for OCM testing, we\u0026rsquo;re going to prepare KMM testing in Hub-Spoke approach.\nFirst we need to prepare our .docker/config.json with the contents of our OpenShift pull secret used with Kcli.\nmkdir -p ~/.docker/ cp openshift_pull.json ~/.docker/config.json Warning advisories Note\nSemi-scripted version available at automate.sh\nWarning\nWe\u0026rsquo;re using pre-release bits of the software, that\u0026rsquo;s why we need to define a custom catalog for both the Hub and the Spokes. Once KMM is released it will be available from the official one and just the Policy will be needed.\nDanger\nEnsure that podman login quay.io is working correctly so that we can push the built images.\nUsing `podman` instead of `docker`\nIf you\u0026rsquo;re using podman like we\u0026rsquo;ll do, we need to create a symbolic link for the command to the docker name, as the Makefile does use docker.\nln -s /usr/bin/podman /usr/bin/docker Testing KMM Hub\u0026amp;Spoke on OpenShift KMM allows OpenShift to build required Kernel modules for the nodes, for example to take advantage of certain network cards or hardware that is required for operations.\nDependencies registry.ci.openshift.org token - Documentation quay.io account repository accessibility\nif the repositories are not created beforehand, please make sure to convert them to public ones after pushing to them\nSetup the Hub Build and push the KMM-Hub container image, the KMM-Hub OLM bundle and catalog from the midstream repository:\nMYUSER=iranzo git clone git@github.com:rh-ecosystem-edge/kernel-module-management.git cd kernel-module-management # Build KMM-Hub container image HUB_IMG=quay.io/${MYUSER}/kernel-module-management-hub:latest make docker-build-hub # Push KMM-Hub container image HUB_IMG=quay.io/${MYUSER}/kernel-module-management-hub:latest make docker-push-hub # Generate KMM-Hub OLM bundle HUB_IMG=quay.io/${MYUSER}/kernel-module-management-hub:latest BUNDLE_IMG=quay.io/${MYUSER}/kernel-module-management-bundle-hub make bundle-hub # Build KMM-Hub OLM bundle container BUNDLE_IMG=quay.io/${MYUSER}/kernel-module-management-bundle-hub make bundle-build-hub # Push KMM-Hub OLM bundle container BUNDLE_IMG=quay.io/${MYUSER}/kernel-module-management-bundle-hub make bundle-push # Build KMM-Hub OLM catalog CATALOG_IMG=quay.io/${MYUSER}/kernel-module-management-catalog-hub:latest BUNDLE_IMGS=quay.io/${MYUSER}/kernel-module-management-bundle-hub make catalog-build # Push KMM-Hub OLM catalog CATALOG_IMG=quay.io/${MYUSER}/kernel-module-management-catalog-hub:latest make catalog-push Deploy the KMM-Hub OLM catalog on the Hub:\nAs mentioned earlier we need to perform this step as we\u0026rsquo;re consuming pre-release bits, so the images are not available in the regular OpenShift catalog.\ncat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: kmm-hub-catalog namespace: openshift-marketplace spec: sourceType: grpc image: quay.io/${MYUSER}/kernel-module-management-catalog-hub:latest displayName: KMM Hub Catalog publisher: ${MYUSER} updateStrategy: registryPoll: interval: 5m EOF Deploy the KMM-Hub operator by creating an OLM Subscription:\ncat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: kmm-operator namespace: openshift-operators spec: channel: alpha installPlanApproval: Automatic name: kernel-module-management-hub source: kmm-hub-catalog sourceNamespace: openshift-marketplace EOF Deploy the ACM Policy that adds the required permissions to the Spoke klusterlet, in order for the latter to be able to CRUD KMM Module CRs:\ncat \u0026lt;\u0026lt;EOF | oc apply -f - --- apiVersion: policy.open-cluster-management.io/v1 kind: Policy metadata: name: allow-klusterlet-deploy-kmm-modules spec: remediationAction: enforce disabled: false policy-templates: - objectDefinition: apiVersion: policy.open-cluster-management.io/v1 kind: ConfigurationPolicy metadata: name: klusterlet-deploy-modules spec: severity: high object-templates: - complianceType: mustonlyhave objectDefinition: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: kmm-module-manager rules: - apiGroups: [kmm.sigs.x-k8s.io] resources: [modules] verbs: [create, delete, get, list, patch, update, watch] - complianceType: mustonlyhave objectDefinition: apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: klusterlet-kmm namespace: open-cluster-management-agent subjects: - kind: ServiceAccount name: klusterlet-work-sa namespace: open-cluster-management-agent roleRef: kind: ClusterRole name: kmm-module-manager apiGroup: rbac.authorization.k8s.io --- apiVersion: apps.open-cluster-management.io/v1 kind: PlacementRule metadata: name: all-clusters-except-local spec: clusterSelector: matchExpressions: - key: name operator: NotIn values: - local-cluster --- apiVersion: policy.open-cluster-management.io/v1 kind: PlacementBinding metadata: name: bind-klusterlet-kmm-all-clusters placementRef: apiGroup: apps.open-cluster-management.io kind: PlacementRule name: all-clusters-except-local subjects: - apiGroup: policy.open-cluster-management.io kind: Policy name: allow-klusterlet-deploy-kmm-modules EOF Create a namespace on the Hub, in which the Build and Sign jobs will be created:\noc create ns kmm-jobs-test Setup the Spoke Build and push the KMM container image, the KMM OLM bundle and catalog from midstream repository:\nAgain, we need to perform this step as we\u0026rsquo;re consuming pre-release bits, so the images are not available in the regular OpenShift catalog.\n# Build KMM container image IMG=quay.io/${MYUSER}/kernel-module-management:latest make docker-build # Push KMM container image IMG=quay.io/${MYUSER}/kernel-module-management:latest make docker-push # Generate KMM OLM bundle IMG=quay.io/${MYUSER}/kernel-module-management:latest BUNDLE_IMG=quay.io/${MYUSER}/kernel-module-management-bundle make bundle # Build KMM OLM bundle container BUNDLE_IMG=quay.io/${MYUSER}/kernel-module-management-bundle make bundle-build # Push KMM OLM bundle container BUNDLE_IMG=quay.io/${MYUSER}/kernel-module-management-bundle make bundle-push # Build KMM OLM catalog CATALOG_IMG=quay.io/${MYUSER}/kernel-module-management-catalog:latest BUNDLE_IMGS=quay.io/${MYUSER}/kernel-module-management-bundle make catalog-build # Push KMM OLM catalog CATALOG_IMG=quay.io/${MYUSER}/kernel-module-management-catalog:latest make catalog-push Deploy the KMM catalog on the Spoke:\ncat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: kmm-catalog namespace: openshift-marketplace spec: sourceType: grpc image: quay.io/${MYUSER}/kernel-module-management-catalog:latest displayName: KMM Catalog publisher: ${MYUSER} updateStrategy: registryPoll: interval: 5m EOF Deploy the KMM operator by creating an OLM Subscription:\ncat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: kernel-module-management namespace: openshift-operators spec: channel: alpha config: env: - name: KMM_MANAGED value: \u0026#34;1\u0026#34; installPlanApproval: Automatic name: kernel-module-management source: kmm-catalog sourceNamespace: openshift-marketplace EOF Create a namespace on the Spoke, in which the Module will be created\noc create ns kmm-tests Create a ManagedClusterModule on the Hub The following ManagedClusteModule includes a selector for the default clusterset.\nWarning\nIn this example, we\u0026rsquo;ve created manually a docker secret in order to allow KMM-Hub to push the newly generated module container image to my quay.io repository. For example:\ncat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: v1 data: .dockerconfigjson: THISISWHEREMYSECRET GOES kind: Secret metadata: name: ${MYUSER}-quay-cred namespace: kmm-jobs-test type: kubernetes.io/dockerconfigjson EOF Note\nPay attention to the cluster selector in the example below:\nselector: cluster.open-cluster-management.io/clusterset: default So that it targets one of your clusters.\nDeploy a ManagedClusterModule to test the setup e2e:\nAnd now, the manifest itself:\ncat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: mod-example namespace: kmm-jobs-test data: dockerfile: | FROM image-registry.openshift-image-registry.svc:5000/openshift/driver-toolkit as builder ARG KERNEL_VERSION ARG MY_MODULE WORKDIR /build RUN git clone https://github.com/cdvultur/kmm-kmod.git WORKDIR /build/kmm-kmod RUN cp kmm_ci_a.c mod-example.c RUN make FROM registry.redhat.io/ubi8/ubi-minimal ARG KERNEL_VERSION ARG MY_MODULE RUN microdnf -y install kmod COPY --from=builder /usr/bin/kmod /usr/bin/ RUN for link in /usr/bin/modprobe /usr/bin/rmmod; do \\ ln -s /usr/bin/kmod \u0026#34;\\$link\u0026#34;; done COPY --from=builder /etc/driver-toolkit-release.json /etc/ COPY --from=builder /build/kmm-kmod/*.ko /opt/lib/modules/\\${KERNEL_VERSION}/ RUN depmod -b /opt \\${KERNEL_VERSION} --- apiVersion: hub.kmm.sigs.x-k8s.io/v1beta1 kind: ManagedClusterModule metadata: name: mod-example spec: spokeNamespace: kmm-tests jobNamespace: kmm-jobs-test selector: cluster.open-cluster-management.io/clusterset: default moduleSpec: imageRepoSecret: name: ${MYUSER}-quay-cred moduleLoader: container: modprobe: moduleName: mod-example imagePullPolicy: Always kernelMappings: - regexp: \u0026#39;^.+$\u0026#39; containerImage: quay.io/${MYUSER}/module:\\${KERNEL_FULL_VERSION} pull: insecure: true insecureSkipTLSVerify: true build: buildArgs: - name: MY_MODULE value: mod-example.o dockerfileConfigMap: name: mod-example selector: node-role.kubernetes.io/worker: \u0026#34;\u0026#34; EOF Check the respective build on the Hub:\noc get builds -n kmm-jobs-test After the build is completed, the respective ManifestWork will be created:\noc get manifestworks -n \u0026lt;spoke-cluster-name\u0026gt; Check whether the KMM Module is created on the Spoke:\noc get modules -n kmm-tests Check the KMM ModuleLoader DaemonSet on the Spoke:\noc get ds -n kmm-tests Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2023/01/19/kernel-module-management-testing/","summary":"\u003cp\u003eFollowing on the \u003ca href=\"/blog/2022/12/23/using-kcli-to-prepare-for-open-cluster-management-testing/\"\u003eUsing Kcli to prepare for OCM testing\u003c/a\u003e, we\u0026rsquo;re going to prepare KMM testing in Hub-Spoke approach.\u003c/p\u003e\n\u003cp\u003eFirst we need to prepare our \u003ccode\u003e.docker/config.json\u003c/code\u003e with the contents of our OpenShift pull secret used with \u003ccode\u003eKcli\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emkdir -p ~/.docker/\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecp openshift_pull.json ~/.docker/config.json\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"warning-advisories\"\u003eWarning advisories\u003c/h2\u003e\n\u003cdiv class=\"admonition note\"\u003e\n    \u003cp class=\"admonition-title\"\u003eNote\u003c/p\u003e\n    \u003cp class=\"admonition\"\u003eSemi-scripted version available at \u003ca href=\"automate.sh\"\u003eautomate.sh\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv class=\"admonition warning\"\u003e\n    \u003cp class=\"admonition-title\"\u003eWarning\u003c/p\u003e\n    \u003cp class=\"admonition\"\u003eWe\u0026rsquo;re using pre-release bits of the software, that\u0026rsquo;s why we need to define a custom catalog for both the Hub and the Spokes. Once KMM is released it will be available from the official one and just the Policy will be needed.\u003c/p\u003e","title":"Kernel Module Management testing"},{"content":"Management Integration whereabouts What we\u0026rsquo;ve done so far? ZTP Factory Workflow Kernel Module Management NVIDIA Bluefield support OpenShift Virtualization with vGPU Other supporting tasks ZTP Factory Workflow ZTP Stands for Zero Touch Provisioning: Automated OpenShift deployment that setup registry mirror, nodes,etc out of a configuration yaml.\nThe architecture was described as Openshift Container Agnostic TOPology Integrated Chassis. The architecture was based on a 3+1 chassis which had everything needed for the automation with nics for internal/external networks.\nZTPFW (cont) Simplifies hardware manufacturer setup in an automated and repetitive way OpenShift clusters that are ready to be shipped to customer premises to final setup and utilization. Uses OpenShift, ACM, Quay, etc. Used as foundation layer for other efforts Automatic reconnection to ACM Hub ZTP Use cases Prepare clusters ready to be deployed close to customer needs (concerts, supermarkets, etc.) Disconnected environments that might reconnect after time for updates, (ACM Spoke import), f.e. Cargo ship, etc. Kernel Module Management It\u0026rsquo;s an operator that builds custom kernel modules to provide support for specific hardware in the systems.\nAbout to get 1.0 released with some features coming for 1.1:\nDisconnected operation Hub-Spoke approach (Hub can build the modules for resource-stretched spokes) Nvidia Bluefield2 DPU What is a DPU? :\nDPU ( Data processing unit) it\u0026rsquo;s a PCIE card that runs as a system-on-chip inside the host. The System is based on ARM and hosts its own OS and even a BMC. How can it be used? The DPU is can be used in several ways:\nOffload traffic from the host to the DPU - SSL encrypt/decrypt, IPSEC, etc.. NVME over LAN Security enforcement Software defined infrastructure How is DPU used in Openshift? The Current design is using Shared-OVN with 2 clusters:\nInfrastructure cluster Tenant cluster The cluster will communicate between them and setup a shared OVN network for offloading POD’s traffic\nLinks:\nNvidia DPU DMA-BUF for GPU RDMA DMA what?\nRDMA is a kernel feature to share buffers between hosts. The general idea is setting up a peer-to-peer DMA between NIC and transferring / sharing buffers for GPU workloads.\nStarting in RHCOS 9 :\nOpenFabrics GPU DMA Openshift Virtualization with vGPU In the old way, each VM we create has a PCIE pass-through to GPU. which means that only one application / service can run on one GPU.\nThe Nvidia GPU operator is now extended and supports vGPU - taking one physical GPU and sharing to multiple instances.\nMIG with GPU operator The next evolution of GPU is using MIG ( multi instance GPU ) which partition a GPU based on Ampere architecture ( A100 + ) to different hardware slices which is more secure /isolated and better performance.\nLinks:\nNvidia OCP Virt Nvidia MIG Other tasks Scale testing of ACM Deployment Scale testing of KMM deployment Questions? ","permalink":"https://iranzo.io/blog/2023/01/19/management-integration-whereabouts/","summary":"\u003ch2 id=\"management-integration-whereabouts\"\u003eManagement Integration whereabouts\u003c/h2\u003e\n\u003chr\u003e\n\u003ch3 id=\"what-weve-done-so-far\"\u003eWhat we\u0026rsquo;ve done so far?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eZTP Factory Workflow\u003c/li\u003e\n\u003cli\u003eKernel Module Management\u003c/li\u003e\n\u003cli\u003eNVIDIA Bluefield support\u003c/li\u003e\n\u003cli\u003eOpenShift Virtualization with vGPU\u003c/li\u003e\n\u003cli\u003eOther supporting tasks\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\n\n\u003csection data-shortcode-section\u003e\n\u003ch3 id=\"ztp-factory-workflow\"\u003eZTP Factory Workflow\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eZTP Stands for Zero Touch Provisioning: Automated OpenShift deployment that setup registry mirror, nodes,etc out of a configuration yaml.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe architecture was described as Openshift Container Agnostic TOPology Integrated Chassis. The architecture was based on a 3+1 chassis which had everything needed for the automation with nics for internal/external networks.\u003c/p\u003e","title":"Management Integration whereabouts"},{"content":"RHEL9 by default uses read-only socket which is not usable by some tools like Kcli\u0026hellip; to enable it use:\nsystemctl enable --now libvirtd.socket libvirtd-ro.socket systemctl stop libvirtd.service systemctl enable --now virtproxyd.socket virtproxyd-ro.socket systemctl stop virtproxyd.service ","permalink":"https://iranzo.io/tips/rhel9-libvirtd/","summary":"\u003cp\u003eRHEL9 by default uses read-only socket which is not usable by some tools like \u003ca href=\"/tags/kcli/\"\u003eKcli\u003c/a\u003e\u0026hellip; to enable it use:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esystemctl enable --now libvirtd.socket libvirtd-ro.socket\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esystemctl stop libvirtd.service\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esystemctl enable --now virtproxyd.socket virtproxyd-ro.socket\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esystemctl stop virtproxyd.service\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Enable Libvirt rw socket on RHEL9"},{"content":"Kcli allows to quickly interact with different virtualization platforms to build machines with some specific configurations, and via the use of plans it allows to automate most of the setup required to have an environment ready.\nIn our case, let\u0026rsquo;s setup an environment to practice with Open Cluster Management but instead of using kind clusters, let\u0026rsquo;s use VM\u0026rsquo;s.\nNote\nWe\u0026rsquo;ll require to setup an openshift_pull.json file for Kcli to consume when accessing the required resources for this to work. That file, contains the credentials for accessing several container registries used for the deployment.\nNote\nThe script described below can be downloaded from kcli.sh.\nLet\u0026rsquo;s first cover the prerequisites for the different pieces we\u0026rsquo;re going to use:\n# Install tmux dnf -y install tmux # Upgrade packages dnf -y upgrade # Enable epel dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm # Install a proper editor dnf -y install joe # Install container engine dnf -y install podman # Install go dnf -y install go # Extend path echo \u0026#39;PATH=$PATH:~/go/bin\u0026#39; \u0026gt;~/.bashrc At this point we\u0026rsquo;ve our system ready to use go, and some other utilities available.\nLet\u0026rsquo;s now continue with clusteradm and some other utilities like kubectl:\n# Install clusteradm curl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh | bash # Install kubectl curl -L https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl \u0026gt;/usr/bin/kubectl chmod +x /usr/bin/kubectl Now, let\u0026rsquo;s go to install Kcli requirements from its documentation:\n# Prepare Kcli installation sudo yum -y install libvirt libvirt-daemon-driver-qemu qemu-kvm sudo usermod -aG qemu,libvirt $(id -un) sudo newgrp libvirt sudo systemctl enable --now libvirtd # Optional if using docker daemon sudo groupadd docker sudo usermod -aG docker $(id -un) sudo systemctl restart docker sudo dnf -y copr enable karmab/kcli sudo dnf -y install kcli At this step, we should make sure that our host, has the default virt-pool configured so that we can continue with the creation of the cluster.\nFor doing so, we\u0026rsquo;ll use the following plan, defined with a Jinja template.\nAs you can see, we first define some parameters for the whole cluster, specially the number of machines to create, the Kcli network to use, addressing, etc.\nparameters: cluster: cluster domain: karmalabs.corp number: 2 network: default cidr: 192.168.122.0/24 {{ network }}: type: network cidr: {{ cidr }} Then, we define the network and define the first stanza for the hub cluster.\n{% set num = 0 %} {% set api_ip = cidr|network_ip(200 + num ) %} {% set cluster = \u0026#39;hub\u0026#39; %} hub: type: cluster kubetype: openshift domain: {{ domain }} ctlplanes: 1 api_ip: {{ api_ip }} numcpus: 16 memory: 32768 api-hub: type: dns net: {{ network }} ip: {{ api_ip }} alias: - api.{{ cluster }}.{{ domain }} - api-int.{{ cluster }}.{{ domain }} {% if num == 0 %} apps-hub: type: dns net: {{ network }} ip: {{ api_ip }} alias: - console-openshift-console.apps.{{ cluster }}.{{ domain }} - oauth-openshift.apps.{{ cluster }}.{{ domain }} - prometheus-k8s-openshift-monitoring.apps.{{ cluster }}.{{ domain }} - canary-openshift-ingress-canary.apps.{{ cluster }}.{{ domain }} - multicloud-console.apps.{{ cluster }}.{{ domain }} {% endif %} And now, we\u0026rsquo;ll iterate to generate the stanzas for the spoke clusters:\n{% for num in range(1, number) %} {% set api_ip = cidr|network_ip(200 + num ) %} {% set cluster = \u0026#34;cluster\u0026#34; %} cluster{{ num }}: type: cluster kubetype: openshift domain: {{ domain }} ctlplanes: 1 api_ip: {{ api_ip }} numcpus: 16 memory: 32768 api-cluster{{ num}}: type: dns net: {{ network }} ip: {{ api_ip }} alias: - api.{{ cluster }}{{ num }}.{{ domain }} - api-int.{{ cluster }}{{ num }}.{{ domain }} {% endfor %} This definition uses a new feature provided by Kcli which allows to start the deployment in parallel, so let\u0026rsquo;s get ready for it:\n# Download openshift-install to avoid bug when downloading in parallel during plan creation for command in oc openshift-install; do kcli download ${command} mv ${command} /usr/bin/ done # Create the plan kcli create plan -f kcli-plan-hub-spoke.yml Here, Kcli will have created the different VM\u0026rsquo;s, kubeconfig files, etc to get access to the environment, so that we can continue with the Open Cluster Management part:\n# Prepare clusteradm on HUB export KUBECONFIG=/root/.kcli/clusters/hub/auth/kubeconfig clusteradm init --wait kubectl -n open-cluster-management get # Add the Policy framework clusteradm install hub-addon --names governance-policy-framework # Get values we\u0026#39;ll need for adding spokes apiserver=$(clusteradm get token | grep -v token= | tr \u0026#34; \u0026#34; \u0026#34;\\n\u0026#34; | grep apiserver -A1 | tail -1) MAXSPOKE=5 # Join the spokes to the cluster for spoke in $(seq 1 ${MAXSPOKE}); do export KUBECONFIG=/root/.kcli/clusters/hub/auth/kubeconfig token=$(clusteradm get token } | grep token= | cut -d \u0026#34;=\u0026#34; -f 2-) export KUBECONFIG=/root/.kcli/clusters/cluster${spoke}/auth/kubeconfig clusteradm join --hub-token ${token} --hub-apiserver ${apiserver} --wait --cluster-name \u0026#34;cluster${spoke}\u0026#34; # --force-internal-endpoint-lookup done Each host (spoke) will connect to the hub and reach it to request a signed certificate and being accepted as spoke, we can perform some diagnosis when checking the klusterlet status:\n# Check clusterlet status for spoke in $(seq 1 ${MAXSPOKE}); do export KUBECONFIG=/root/.kcli/clusters/cluster${spoke}/auth/kubeconfig kubectl get klusterlet done And the pending Certificate Signing Requests (CSR):\n# Check pending CSR export KUBECONFIG=/root/.kcli/clusters/hub/auth/kubeconfig kubectl get csr Last step, is to accept, from the HUB, all the requests received from the spoke clusters.\n# Accept joins from HUB for spoke in $(seq 1 ${MAXSPOKE}); do export KUBECONFIG=/root/.kcli/clusters/hub/auth/kubeconfig clusteradm accept --clusters cluster${spoke} done With this, we\u0026rsquo;ve an environment with several spoke clusters and one hub, that we can use to test the scenarios described at https://open-cluster-management.io/scenarios/.\nEnjoy! (and if you do, you can Buy Me a Coffee ) Pablo\n","permalink":"https://iranzo.io/blog/2022/12/23/using-kcli-to-prepare-for-open-cluster-management-testing/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/karmab/Kcli\"\u003eKcli\u003c/a\u003e allows to quickly interact with different virtualization platforms to build machines with some specific configurations, and via the use of \u003ccode\u003eplans\u003c/code\u003e it allows to automate most of the setup required to have an environment ready.\u003c/p\u003e\n\u003cp\u003eIn our case, let\u0026rsquo;s setup an environment to practice with \u003ca href=\"https://open-cluster-management.io/getting-started/quick-start/\"\u003eOpen Cluster Management\u003c/a\u003e but instead of using kind clusters, let\u0026rsquo;s use VM\u0026rsquo;s.\u003c/p\u003e\n\u003cdiv class=\"admonition note\"\u003e\n    \u003cp class=\"admonition-title\"\u003eNote\u003c/p\u003e\n    \u003cp class=\"admonition\"\u003eWe\u0026rsquo;ll require to setup an \u003ccode\u003eopenshift_pull.json\u003c/code\u003e file for Kcli to consume when accessing the required resources for this to work. That file, contains the credentials for accessing several container registries used for the deployment.\u003c/p\u003e","title":"Using Kcli to prepare for Open Cluster Management testing"},{"content":"Some years ago, I added a script for updating headers for (C) in the python files I was developing for Risu.\nIn this way, the header got the list of authors and years working on the files updated automatically.\nWith the pass of the years, the list started to became a bit too long, so I worked on creating code for getting ranges instead.\nThis is the code I used:\ndef getranges(data): \u0026#34;\u0026#34;\u0026#34; From list of strings representing numbers, get ranges and return list of strings :param data: list of strings representing numbers :return: list of strings with number ranges when \u0026gt; 1 \u0026#34;\u0026#34;\u0026#34; # Convert to integers data = [int(i) for i in data] result = [] if not data: return result # Prepare iteration loop idata = iter(data) first = prev = next(idata) first = first prev = prev # Process next item for following in idata: if following - prev == 1: # Years are continuum, just update previous prev = following else: # Years are not continuum, end range and start again if first == prev: result.append(first) else: if first + 1 == prev: # Only one item in difference, append items individually result.append(first) result.append(prev) else: result.append(\u0026#34;%s-%s\u0026#34; % (first, prev)) first = prev = following # Catchall for regular execution or last remaining range if first == prev: result.append(first) else: if first + 1 == prev: # Only one item in difference, append items individually result.append(first) result.append(prev) else: result.append(\u0026#34;%s-%s\u0026#34; % (first, prev)) # Convert back to text result = [str(i) for i in result] return result With it, previous headers like:\n# Copyright (C) 2018, 2019, 2020, 2021, 2022 Pablo Iranzo Gómez \u0026lt;Pablo.Iranzo@gmail.com\u0026gt; Now appear as:\n# Copyright (C) 2018-2022 Pablo Iranzo Gómez \u0026lt;Pablo.Iranzo@gmail.com\u0026gt; Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/11/25/python-generate-ranges-from-items/","summary":"\u003cp\u003eSome years ago, I added a script for updating headers for \u003ccode\u003e(C)\u003c/code\u003e in the python files I was developing for \u003ca href=\"/tags/risu/\"\u003eRisu\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn this way, the header got the list of authors and years working on the files updated automatically.\u003c/p\u003e\n\u003cp\u003eWith the pass of the years, the list started to became a bit too long, so I worked on creating code for getting ranges instead.\u003c/p\u003e\n\u003cp\u003eThis is the code I used:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egetranges\u003c/span\u003e(data):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    From list of strings representing numbers, get ranges and return list of strings\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param data: list of strings representing numbers\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :return: list of strings with number ranges when \u0026gt; 1\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Convert to integers\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [int(i) \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e data]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    result \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e data:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e result\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Prepare iteration loop\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    idata \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e iter(data)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    first \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e prev \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e next(idata)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    first \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e first\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    prev \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e prev\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Process next item\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e following \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e idata:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e following \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e prev \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Years are continuum, just update previous\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            prev \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e following\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Years are not continuum, end range and start again\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e first \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e prev:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                result\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(first)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e first \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e prev:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#75715e\"\u003e# Only one item in difference, append items individually\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    result\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(first)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    result\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(prev)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    result\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e (first, prev))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            first \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e prev \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e following\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Catchall for regular execution or last remaining range\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e first \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e prev:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        result\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(first)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e first \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e prev:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# Only one item in difference, append items individually\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            result\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(first)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            result\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(prev)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            result\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e (first, prev))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Convert back to text\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    result \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [str(i) \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e result]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e result\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWith it, previous headers like:\u003c/p\u003e","title":"[python] Generate ranges from items"},{"content":"Let\u0026rsquo;s say that we want to keep our system updated with some code which is not distributed as a regular package, but as a code in a repository (which unfortunately, it\u0026rsquo;s a pretty common situation).\nAs a part of the ansible playbooks used for the hosts, I can add a snippet like this:\ngitrepos: - { url: \u0026#34;https://github.com/myrepo/repo.git\u0026#34;, tag: \u0026#34;tagtocheckout\u0026#34;, folder: \u0026#34;/root/path-for-check-out\u0026#34;, chdir: \u0026#34;subdir to enter\u0026#34;, build: \u0026#34;make build\u0026#34;, exec: \u0026#34;build/mybuiltbinary\u0026#34;, } With this definition in the host inventory, we can then in our playbook to perform several steps:\nFirst Checkout the repository, note that we loop over gitrepos variable and use the items defined. We also set ignore_errors to ensure our playbook run doesn\u0026rsquo;t halt in case of any mistake here.\nAlso, we register the output in the repos variable for later processing (we\u0026rsquo;ll see why later).\n- name: Checkout git repos at specific versions git: repo: \u0026#34;{{ item.url }}\u0026#34; dest: \u0026#34;{{ item.folder }}\u0026#34; version: \u0026#34;{{ item.tag}}\u0026#34; with_items: \u0026#34;{{ gitrepos }}\u0026#34; ignore_errors: true register: repos when: gitrepos != False Next, as we\u0026rsquo;ll be building the binary from the repo, we want to make sure that previous built ones are absent, so that we can force rebuilding it.\nNote that were appending the chdir path if it\u0026rsquo;s defined\u0026hellip; it\u0026rsquo;s a special use case, because some repositories, contain different set of code in sub-folders instead of being on different repositories, so this helps in this situation. Of course, we\u0026rsquo;re doing this, only when a new release has been checked out in prior step (repos.changed).\n- name: Remove previous binary if tag changed to get it recompiled file: name: \u0026#34;{{ item.folder }}/{% if item.chdir is defined %}/{{item.chdir}}{% endif %}{{ item.exec }}\u0026#34; state: absent when: repos.changed with_items: \u0026#34;{{ gitrepos }}\u0026#34; Now we\u0026rsquo;re ready to build the code, as we\u0026rsquo;ve defined also the chdir we get into the relevant folder and run the build command to generate the binary\u0026hellip; as a result it must create a binary in item.exec so that we can validate it worked or not and of course, only if we\u0026rsquo;ve defined a build command.\n- name: Build git repos shell: cmd: \u0026#34;{{ item.build }}\u0026#34; chdir: \u0026#34;{{ item.folder }}{% if item.chdir is defined %}/{{item.chdir}}{% endif %}\u0026#34; creates: \u0026#34;{{ item.exec }}\u0026#34; with_items: \u0026#34;{{ gitrepos }}\u0026#34; ignore_errors: true when: gitrepos != False and item.build is defined and item.build != False Last step\u0026hellip; as we got the binary built, we might want to copy it to a folder into our path so that it can be used, in this example, to the go/bin folder:\n- name: copy built command copy: remote_src: yes dest: \u0026#34;/root/go/bin/\u0026#34; src: \u0026#34;{{item.folder}}{% if item.chdir is defined %}/{{item.chdir}}{% endif %}/{{ item.exec }}\u0026#34; mode: \u0026#34;0755\u0026#34; with_items: \u0026#34;{{ gitrepos }}\u0026#34; ignore_errors: true when: gitrepos != False and item.build is defined and item.build != False By defining several repo stanzas, we can automate the process\u0026hellip; and this is just the first step, as we can also define the checkout tag by first querying the repo latest release (if we want to live on the bleeding edge), so this whole process keeps your system using latest tools available, and built from source.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/11/09/automate-code-build-and-deployment-with-ansible/","summary":"\u003cp\u003eLet\u0026rsquo;s say that we want to keep our system updated with some code which is not distributed as a regular package, but as a code in a repository (which unfortunately, it\u0026rsquo;s a pretty common situation).\u003c/p\u003e\n\u003cp\u003eAs a part of the ansible playbooks used for the hosts, I can add a snippet like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003egitrepos\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003eurl\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://github.com/myrepo/repo.git\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003etag\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;tagtocheckout\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003efolder\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/root/path-for-check-out\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003echdir\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;subdir to enter\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003ebuild\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;make build\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003eexec\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;build/mybuiltbinary\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWith this definition in the host inventory, we can then in our playbook to perform several steps:\u003c/p\u003e","title":"Automate code build and deployment with ansible"},{"content":"A colleague reported some issues in the OpenShift troubleshooting and diagnosis scripts at OpenShift-checks.\nSome time ago I did contribute some changes to use functions and allow using the RISU wrapper to the scripts, helping consuming the results via RISU\u0026rsquo;s HTML interface.\nAs my colleague reported, for some plugins, the output of the command was not shown in the HTML Interface.\nAfter some investigation, it was found that parallel execution for the plugins was causing no output to be shown, but when filtering to individual ones via risu -i XXXXXXX/plugin -l it was working fine\u0026hellip; the problem was not the check itself, as both of them worked fine when executed individually but failed when executing them together.\nAs a way forward, a small patch that allowed to limit the number of concurrent plugin executions was added to Risu, and effectively, when limiting to one plugin at a time, both of them returned results.\nFurther investigation found the reason\u0026hellip; many checks in openshift-checks do use oc debug command to launch commands interactively in the hosts, and as each plugin does their own set of checks vs grabbing all the data and then doing processing, this was causing issues. In the case of the included wrapper, it still could cause issues as some parallelization was included, but when used together with risu.py the problem became more apparent.\nFinally the workaround used is a small function added that checks if there\u0026rsquo;s a running oc debug check, and if so, waiting a random delay of seconds, which effectively doesn\u0026rsquo;t affect the execution, but allows to use the plugins normally:\nocdebugorwait() { instances=$(pgrep -f \u0026#39;oc debug\u0026#39; | wc -l) while [ \u0026#34;${instances}\u0026#34; != \u0026#34;0\u0026#34; ]; do # Waiting for oc debug to not be anymore sleep $(($RANDOM % 10)) instances=$(pgrep -f \u0026#39;oc debug\u0026#39; | wc -l) done When invoked in the scripts with ocdebugorwait the code enters this loop before attempting the oc debug command.\nWith this patch, the reported output is obtained, and of course, properly shown in the HTML interface while doing troubleshooting with Risu. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/11/03/openshifts-oc-debug-and-parallel-execution/","summary":"\u003cp\u003eA colleague reported some issues in the OpenShift troubleshooting and diagnosis scripts at \u003ca href=\"https://github.com/RHsyseng/openshift-checks/\"\u003eOpenShift-checks\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSome time ago I did contribute some changes to use functions and allow using the \u003ca href=\"https://github.com/risuorg/risu\"\u003eRISU\u003c/a\u003e wrapper to the scripts, helping consuming the results via RISU\u0026rsquo;s HTML interface.\u003c/p\u003e\n\u003cp\u003eAs my colleague reported, for some plugins, the output of the command was not shown in the HTML Interface.\u003c/p\u003e\n\u003cp\u003eAfter some investigation, it was found that parallel execution for the plugins was causing no output to be shown, but when filtering to individual ones via \u003ccode\u003erisu -i XXXXXXX/plugin -l\u003c/code\u003e it was working fine\u0026hellip; the problem was not the check itself, as both of them worked fine when executed individually but failed when executing them together.\u003c/p\u003e","title":"OpenShift's oc debug and parallel execution"},{"content":"Last year, together with my colleagues Miguel and Scott, we released with the help of Packt team the RHEL8 Administration.\nDuring this one, with the collaboration of Pedro, a long-time colleague from the University LUG, we worked on the updated Red Hat Enterprise Linux 9 Administration book that it\u0026rsquo;s now available for preorder on Amazon.\nAs with the previous version, the book targets users willing to learn skills to administer Red Hat Enterprise Linux or compatible systems. It is a hands-on guide to the administration and can be used as reference thanks to the real-life examples provided along the text.\nIt has been updated to cover the differences that came with RHEL9, new additions, changes in the configurations, etc and it features two chapters dedicated to exercises to check your knowledge acquired in the book.\nAvailable at Amazon: Red Hat Enterprise Linux 9 Administration\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/10/12/book-red-hat-enterprise-linux-9-administration/","summary":"\u003cp\u003eLast year, together with my colleagues Miguel and Scott, we released with the help of Packt team the \u003ca href=\"/blog/2021/09/11/book-red-hat-enterprise-linux-8-administration/\"\u003eRHEL8 Administration\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eDuring this one, with the collaboration of Pedro, a long-time colleague from the University LUG, we worked on the updated \u003ca href=\"https://s.admins.guru/buy-on-amazon-rhel9\"\u003eRed Hat Enterprise Linux 9 Administration\u003c/a\u003e book that it\u0026rsquo;s now available for preorder on Amazon.\u003c/p\u003e\n\u003cp\u003eAs with the previous version, the book targets users willing to learn skills to administer Red Hat Enterprise Linux or compatible systems. It is a hands-on guide to the administration and can be used as reference thanks to the real-life examples provided along the text.\u003c/p\u003e","title":"[Book] Red Hat Enterprise Linux 9 Administration"},{"content":"For my ansible playbooks, I wanted to be able to add several new templates to be copied to target system, and additionally be able to perform some commands for them without having to specify each individual file/template to copy.\nMy approach:\nDefine for the hosts I want to find templates/playbooks define a var named extras for the relevant hosts:\nextras: - ntp - certificates The names defined (in above example ntp and certificates) are just name of folders laying inside tasks/templates/${folder} that are searched and included or excluded based on extras values.\n--- - name: Find candidate templates find: paths: - \u0026#34;{{playbook_dir}}/tasks/templates/\u0026#34; recurse: yes patterns: \u0026#34;*.jinja\u0026#34; register: templates delegate_to: localhost when: extras is defined - name: Copy templates from folder into path template: mode: \u0026#34;{{ item[1].mode }}\u0026#34; src: \u0026#34;{{ item[1].path }}\u0026#34; dest: \u0026#34;{{ item[1].path | replace(playbook_dir,\u0026#39;\u0026#39;) | replace(\u0026#39;/tasks/templates\u0026#39;,\u0026#39;\u0026#39;) | replace(\u0026#39;.jinja\u0026#39;,\u0026#39;\u0026#39;) |replace(\u0026#39;/\u0026#39; + item[0] + \u0026#39;/\u0026#39;,\u0026#39;/\u0026#39;)|replace(\u0026#39;//\u0026#39;,\u0026#39;/\u0026#39;)\\ \\ }}\u0026#34; with_nested: - \u0026#34;{{ extras }}\u0026#34; - \u0026#34;{{ templates.files }}\u0026#34; loop_control: label: \u0026#34;{{ item[1].path | replace(playbook_dir,\u0026#39;\u0026#39;) | replace(\u0026#39;/tasks/templates\u0026#39;,\u0026#39;\u0026#39;) | replace(\u0026#39;.jinja\u0026#39;,\u0026#39;\u0026#39;) |replace(\u0026#39;/\u0026#39; + item[0] + \u0026#39;/\u0026#39;,\u0026#39;/\u0026#39;)|replace(\u0026#39;//\u0026#39;,\u0026#39;/\u0026#39;)\\ \\ }}\u0026#34; when: extras is defined and templates != False and item[0] in item[1].path notify: - Restart systemd And we\u0026rsquo;ll do something similar for the playbooks inside those folders\n- name: Find candidate playbooks find: paths: - \u0026#34;{{playbook_dir}}/tasks/templates/\u0026#34; recurse: yes patterns: \u0026#34;*.yaml\u0026#34; register: playbooks delegate_to: localhost when: extras is defined # Create empty array that we\u0026#39;ll be filling - name: Filter candidate playbooks set_fact: playbook: [] when: extras is defined and playbooks != False # Build an array of all the playbooks we\u0026#39;re going to use - name: Filter candidate playbooks set_fact: playbook: \u0026#34;{{ playbook + [item[1].path] }}\u0026#34; with_nested: - \u0026#34;{{ extras }}\u0026#34; - \u0026#34;{{ playbooks.files }}\u0026#34; loop_control: label: \u0026#34;{{ item[1].path | replace(playbook_dir,\u0026#39;\u0026#39;) | replace(\u0026#39;/tasks/templates\u0026#39;,\u0026#39;\u0026#39;) }}\u0026#34; when: extras is defined and playbooks != False and item[0] in item[1].path - name: Load tasks from playbook include_tasks: \u0026#34;{{ item }}\u0026#34; loop: \u0026#34;{{ playbook }}\u0026#34; when: extras is defined and playbook != False With this approach, putting files in a tree structure like:\n./tasks ./tasks/templates ./tasks/templates/ntp ./tasks/templates/ntp/etc ./tasks/templates/ntp/etc/ntp.conf.jinja ./tasks/templates/ntp/tasks.yaml ./tasks/templates/certificates ./tasks/templates/certificates/etc ./tasks/templates/certificates/etc/pki/ ./tasks/templates/certificates/etc/pki/ca-trust/source/ ./tasks/templates/certificates/etc/pki/ca-trust/source/anchors/ ./tasks/templates/certificates/etc/pki/ca-trust/source/anchors/mycert.jinja ./tasks/templates/certificates/tasks.yaml This will make first part of the task to copy the templates (ending in .jinja) to the target location, but removing the .jinja suffix.\nFor the second part, it will include the relevant .yaml files, and load the tasks defined within and execute as part of the playbook. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/09/24/ansible-dynamically-include-jinja-templates-and-tasks/","summary":"\u003cp\u003eFor my ansible playbooks, I wanted to be able to add several new templates to be copied to target system, and additionally be able to perform some commands for them without having to specify each individual file/template to copy.\u003c/p\u003e\n\u003cp\u003eMy approach:\u003c/p\u003e\n\u003cp\u003eDefine for the hosts I want to find templates/playbooks define a var named \u003ccode\u003eextras\u003c/code\u003e for the relevant hosts:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eextras\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003entp\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#ae81ff\"\u003ecertificates\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe names defined (in above example \u003ccode\u003entp\u003c/code\u003e and \u003ccode\u003ecertificates\u003c/code\u003e) are just name of folders laying inside \u003ccode\u003etasks/templates/${folder}\u003c/code\u003e that are searched and included or excluded based on \u003ccode\u003eextras\u003c/code\u003e values.\u003c/p\u003e","title":"Ansible - dynamically include Jinja templates and tasks"},{"content":"Use sorted list for included files vs random provided by with_fileglob.\n- name: Include tasks include_tasks: \u0026#34;{{item}}\u0026#34; loop: \u0026#34;{{ query(\u0026#39;fileglob\u0026#39;, \u0026#39;tasks/*.yaml\u0026#39;) | sort }}\u0026#34; ","permalink":"https://iranzo.io/tips/include-sorted-playbooks/","summary":"\u003cp\u003eUse sorted list for included files vs random provided by \u003ccode\u003ewith_fileglob\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e- \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eInclude tasks\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003einclude_tasks\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;{{item}}\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eloop\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;{{ query(\u0026#39;fileglob\u0026#39;, \u0026#39;tasks/*.yaml\u0026#39;) | sort }}\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Include Ansible playbooks sorted"},{"content":"In order to setup disconnected registry for installation, follow this blog post by Daniel at Introducing Mirror Registry for Red Hat OpenShift.\nAt the end of the process it will output something like:\nINFO[2022-08-19 07:10:22] Quay installed successfully, permanent data is stored in /etc/quay-install INFO[2022-08-19 07:10:22] Quay is available at https://${HOSTNAME}:8443 with credentials (init, ${PASSWORDSTRING}) Once the setup is done, remember several steps:\nEdit /etc/containers/registries.conf to add relevant entries for our registry as required:\n[[registry]] insecure=true location=\u0026#34;${MYHOSTNAME}:8443\u0026#34; Add the pull-secret information to the pull-secret.txt file, along with the other entries you might had:\n\u0026#34;${MYHOSTNAME}:8443\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;BASE64_ENCODED_USERNAME:PASSWORD\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;\u0026#34; } Now, we\u0026rsquo;re ready to do the mirroring itself part, so let\u0026rsquo;s prepare the required variables:\nexport OCP_RELEASE=\u0026#34;4.10.0\u0026#34; export LOCAL_REGISTRY=\u0026#34;`hostname`:8443\u0026#34; export LOCAL_REPOSITORY=\u0026#34;ocp4/openshift4\u0026#34; export PRODUCT_REPO=\u0026#34;openshift-release-dev\u0026#34; export LOCAL_SECRET_JSON=\u0026#34;pull_secret.txt\u0026#34; export RELEASE_NAME=\u0026#34;ocp-release\u0026#34; export ARCHITECTURE=\u0026#34;x86_64\u0026#34; And, let\u0026rsquo;s perform the mirroring itself\noc adm release mirror -a ${LOCAL_SECRET_JSON} --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} Once this process finishes, it will output information regarding the ICSP (ImageContentSourcePolicy) and the snippet we should add to OpenShift installer install-config.yaml to use this mirror instead of the original registries for install time:\nTo use the new mirrored repository to install, add the following section to the install-config.yaml: imageContentSources: - mirrors: - ${HOSTNAME}:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - ${HOSTNAME}:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev To use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy: apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: example spec: repositoryDigestMirrors: - mirrors: - ${HOSTNAME}:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - ${HOSTNAME}:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/08/19/setup-a-quay-mirror-for-offline-installations-with-mirror-registry/","summary":"\u003cp\u003eIn order to setup disconnected registry for installation, follow this blog post by Daniel at \u003ca href=\"https://cloud.redhat.com/blog/introducing-mirror-registry-for-red-hat-openshift\"\u003eIntroducing Mirror Registry for Red Hat OpenShift\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAt the end of the process it will output something like:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-log\" data-lang=\"log\"\u003eINFO[2022-08-19 07:10:22] Quay installed successfully, permanent data is stored in /etc/quay-install\nINFO[2022-08-19 07:10:22] Quay is available at https://${HOSTNAME}:8443 with credentials (init, ${PASSWORDSTRING})\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOnce the setup is done, remember several steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eEdit \u003ccode\u003e/etc/containers/registries.conf\u003c/code\u003e to add relevant entries for our registry as required:\u003c/p\u003e","title":"Setup a Quay mirror for offline installations with mirror-registry"},{"content":"Check agent status per state\nwatch -d \u0026#34;oc get agent -A -o jsonpath=\u0026#39;{range .items[*]}{@.status.debugInfo.state}{\\\u0026#34;\\n\\\u0026#34;}{end}\u0026#39; |sort | uniq --count\u0026#34; ","permalink":"https://iranzo.io/tips/acm-host-status/","summary":"\u003cp\u003eCheck agent status per state\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ewatch -d \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;oc get agent -A -o jsonpath=\u0026#39;{range .items[*]}{@.status.debugInfo.state}{\\\u0026#34;\\n\\\u0026#34;}{end}\u0026#39; |sort | uniq --count\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Check Agent status per state"},{"content":"I did the RHCE exam some time ago, and still there are some tricks and advices I tell the people to bear in mind some of the things I used and that were also provided in the Red Hat Enterprise 8 Administration book:\nDon\u0026rsquo;t remember every step, it\u0026rsquo;s not effective, for example as I don\u0026rsquo;t recall syntax for BIND, I do remember package that has some files with examples and I use that one to check what I need to do Install mlocate and run updatedb as soon as you start, then you can use locate \u0026lt;file\u0026gt; to find out files in your system Use your preferred editor\u0026hellip; it\u0026rsquo;s common to use vi or vim as it\u0026rsquo;s pretty standard, but if you\u0026rsquo;re used to another, make yourself comfortable in the system. As one instructor like to say: \u0026ldquo;Anyone with unlimited amount of time will be able to pass the exam\u0026rdquo;. RHCE is a performance-based exam, that means that you need to cover all the required goals within the exam duration, and in the end, the goals is to accomplish, not to do in the smarter way. For example, if you\u0026rsquo;re told to configure resolv.conf you can either use nmcli to modify the settings or you can pipe the results to it via echo nameserver 1.1.1.1 \u0026gt; /etc/resolv.conf, in the end, both will have the same effect, and of course, using nmcli will be smarter when you\u0026rsquo;re keeping multiple systems and using automation\u0026hellip; but for the exam, the goal is to focus on the fastest path to master at it. You can find more tricks at Red Hat Enterprise 8 Administration Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/07/13/rhce-and-rhcsa-tips-and-tricks/","summary":"\u003cp\u003eI did the RHCE exam some time ago, and still there are some tricks and\nadvices I tell the people to bear in mind some of the things I used and that were also provided in the \u003ca href=\"https://s.admins.guru/buyonamazon\"\u003eRed Hat Enterprise 8 Administration\u003c/a\u003e book:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDon\u0026rsquo;t remember every step, it\u0026rsquo;s not effective, for example as I don\u0026rsquo;t recall syntax for BIND, I do remember package that has some files with examples and I use that one to check what I need to do\u003c/li\u003e\n\u003cli\u003eInstall \u003ccode\u003emlocate\u003c/code\u003e and run \u003ccode\u003eupdatedb\u003c/code\u003e as soon as you start, then you can use \u003ccode\u003elocate \u0026lt;file\u0026gt;\u003c/code\u003e to find out files in your system\u003c/li\u003e\n\u003cli\u003eUse your preferred editor\u0026hellip; it\u0026rsquo;s common to use \u003ccode\u003evi\u003c/code\u003e or \u003ccode\u003evim\u003c/code\u003e as it\u0026rsquo;s pretty standard, but if you\u0026rsquo;re used to another, make yourself comfortable in the system.\u003c/li\u003e\n\u003cli\u003eAs one instructor like to say: \u0026ldquo;Anyone with unlimited amount of time will be able to pass the exam\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eRHCE is a performance-based exam, that means that you need to cover all the required goals within the exam duration, and in the end, the goals is to accomplish, not to do in the \u003ccode\u003esmarter\u003c/code\u003e way.\n\u003cul\u003e\n\u003cli\u003eFor example, if you\u0026rsquo;re told to configure \u003ccode\u003eresolv.conf\u003c/code\u003e you can either use \u003ccode\u003enmcli\u003c/code\u003e to modify the settings or you can pipe the results to it via \u003ccode\u003eecho nameserver 1.1.1.1 \u0026gt; /etc/resolv.conf\u003c/code\u003e, in the end, both will have the same effect, and of course, using \u003ccode\u003enmcli\u003c/code\u003e will be smarter when you\u0026rsquo;re keeping multiple systems and using automation\u0026hellip; but for the exam, the goal is to focus on the fastest path to master at it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can find more tricks at \u003ca href=\"https://s.admins.guru/buyonamazon\"\u003eRed Hat Enterprise 8 Administration\u003c/a\u003e\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"RHCE and RHCSA tips and tricks"},{"content":"I loved the Bus and I was behind it for my city setup, so I went for it and got the Lego 60335 Train station with bus.\nIt contains three instruction manuals for the components:\nCart for repairs that can run over the tracks or the road Bus Train station itself The train station.\nIt features a cafeteria with a croissant, coffee machine and another desk for the tickets.\nHas ticket machine and a place to sit while waiting for the train\nFeatures a road crossing with barriers using the new road plates\nOf course, it features an access ramp from the street to the train platform\nThe maintenance truck with the portable WC. The truck itself has a set of wheels that can be lowered so that it can move over the rails or bring them up so that it can drive in the road.\nThe city bus, features double doors for the central area, and single one for the driver entrance with the ticket machine\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/06/18/lego-60335-train-station-with-bus/","summary":"\u003cp\u003eI loved the Bus and I was behind it for my city setup, so I went for it and got the \u003ca href=\"https://www.amazon.es/dp/B09RG1XMCT?tag=redken-21\"\u003eLego 60335 Train station with bus\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIt contains three instruction manuals for the components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCart for repairs that can run over the tracks or the road\u003c/li\u003e\n\u003cli\u003eBus\u003c/li\u003e\n\u003cli\u003eTrain station itself\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe train station.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eIt features a cafeteria with a croissant, coffee machine and another desk for the tickets.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHas ticket machine and a place to sit while waiting for the train\u003c/p\u003e","title":"Lego 60335 Train station with bus"},{"content":"I\u0026rsquo;ve been building the Lego Volkswagen T2 🛒#ad and took some pics about the final result and the comparison with the T1.\nFeelings:\nThe rear door (above the motor) dismantles itself just by looking at it, the mechanism is interesting as it uses some sticks that get inserted inside the support, but the frame holding it, specially in the upper side is really weak\u0026hellip; The sliding door on the side is nice, and has a mechanism to \u0026lsquo;open it\u0026rsquo; by pushing a small button in the lower part of the van The bed opens a bit better than on T1, which also was easy to get dismantled, same as the table that can be folded which is a lot nicer. The details on the fridge are really nice, and the one with the gas seems just to cover the spot The steering wheel is not just decorative.. as with recent Creator Expert series, it actually moves the front wheels. The roof, instead of being \u0026lsquo;pushed\u0026rsquo; or \u0026lsquo;dropped\u0026rsquo;, has a sliding mechanism that holds it to the lower part, so in order to remove it, you need to open the rear door, and then slide it out. Interestingly it also has a spring that keeps the roof open. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/05/26/lego-volkswagen-t2/","summary":"\u003cp\u003eI\u0026rsquo;ve been building the \u003ca href=\"https://www.amazon.es/dp/B09BLSYTRR?tag=redken-21\"\u003eLego Volkswagen T2 🛒#ad\u003c/a\u003e and took some pics about the final result and the comparison with the \u003ca href=\"/blog/2020/10/02/lego-volkswagen-t1-camper-van-led-kit/\"\u003eT1\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFeelings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe rear door (above the motor) dismantles itself just by looking at it, the mechanism is interesting as it uses some sticks that get inserted inside the support, but the frame holding it, specially in the upper side is really weak\u0026hellip;\u003c/li\u003e\n\u003cli\u003eThe sliding door on the side is nice, and has a mechanism to \u0026lsquo;open it\u0026rsquo; by pushing a small button in the lower part of the van\u003c/li\u003e\n\u003cli\u003eThe bed opens a bit better than on T1, which also was easy to get dismantled, same as the table that can be folded which is a lot nicer.\u003c/li\u003e\n\u003cli\u003eThe details on the fridge are really nice, and the one with the gas seems just to cover the spot\u003c/li\u003e\n\u003cli\u003eThe steering wheel is not just decorative.. as with recent Creator Expert series, it actually moves the front wheels.\u003c/li\u003e\n\u003cli\u003eThe roof, instead of being \u0026lsquo;pushed\u0026rsquo; or \u0026lsquo;dropped\u0026rsquo;, has a sliding mechanism that holds it to the lower part, so in order to remove it, you need to open the rear door, and then slide it out. Interestingly it also has a spring that keeps the roof open.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/FK1auNFt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/FK1auNFt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/FK1auNF.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/uMZ6CY3t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/uMZ6CY3t.jpg\" alt=\"Chairs in the roof\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/uMZ6CY3.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/Hm5CSCJt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/Hm5CSCJt.jpg\" alt=\"Cockpit area (driver seat)\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/Hm5CSCJ.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/c6DJpv8t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/c6DJpv8t.jpg\" alt=\"Cockpit area\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/c6DJpv8.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/eWoyUf7t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/eWoyUf7t.jpg\" alt=\"Interior (fridge)\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/eWoyUf7.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/QhNAeiyt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/QhNAeiyt.jpg\" alt=\"Interior, gas bottle\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/QhNAeiy.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/yyvxuE6t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/yyvxuE6t.jpg\" alt=\"Kitchen and sink\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/yyvxuE6.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/7TJSgB6t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/7TJSgB6t.jpg\" alt=\"Upper view, with the foldable table and the couch that can transform in a bed\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/7TJSgB6.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/jyV63UTt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/jyV63UTt.jpg\" alt=\"Rear view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/jyV63UT.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/GkzUsVGt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/GkzUsVGt.jpg\" alt=\"Motor area\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/GkzUsVG.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/b7eKHztt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/b7eKHztt.jpg\" alt=\"Extra stickers for decoration\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/b7eKHzt.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego Volkswagen T2"},{"content":"Most systems, based on RHEL, were not able to upgrade without reinstallation, or best said, not supported. The new version, that was released at around 18 months later contained so many changes that it was hard to test the upgrades themselves until leapp was introduced.\nCheck the lifecycle here: https://access.redhat.com/support/policy/updates/errata\nHowever, the biggest problem could be the incompatibility of packages or package formats\u0026hellip; but as usually there were some middle layers it was possible to upgrade without reinstallation by performing some manual steps.\nWarning\nThis procedure is completely unsupported, it should not be used on production systems and doesn\u0026rsquo;t ensure service availability during the upgrade. Or even system availability once finished\u0026hellip;\nIn the past, the method that could be used, was to provide the upgradeany parameter to anaconda installer via the command line options, but this still required to boot from an ISO (or physical media) to perform the upgrade, and have little chance of recovery in case of any failure during the installation as anaconda environment was very limited in recovery options.\nIn brief:\nWe\u0026rsquo;ll move in unsupported ways from one release to another Package dependencies will require removing some packages or forcing installations ignoring dependencies that might cause issues At some point, if a power cut happens, the system might not be recoverable as it will be missing important parts. There could be leftovers in the system This will be a lengthy, process and prone to failure Upgrade the systems always before you reach to this situation, this is not something you want to do on regular basis because product has reached End-Of-Life (EOL) It should be easier to work on automating deployments and configurations so in the future you can redeploy on newer releases or newer hardware and get a track of all the performed configurations, etc, ideally on a control version system for traceability. Downloading files from older servers might be difficult, or even connecting to them, because of now obsoleted SSL or other crypto libraries. Don\u0026rsquo;t do this unless you want to fully risk the system you had For each release, try to download .0 and .latest versions, we will use .0 when upgrading from prior major, and before moving to next major version, we\u0026rsquo;ll update to the latest minor\u0026hellip; in this way we might catch some errors of packages not upgraded or procedures\u0026hellip; like kernel scripts that might install on the .0 but end up in an unbootable system, moving from .0 to .latest will allows us to reduce the chances of hitting this kind of issues. Preparation steps In order to start, it is safe to perform some steps:\nPerform a backup and prepare for reinstallation Ensure there\u0026rsquo;s enough free disk Get a copy of the install media on the system we\u0026rsquo;re going to update and have it at hand in case we need to recover Use tmux1 or screen as screen multiplexers so that we can recover in case of network connectivity issues Ensure that tools like wget are installed Have access to remote console, either via BMC or the virtualization platform if that\u0026rsquo;s the case. Check the documentation for the next release to see what are the breaking changes included in the release notes for each version, this will give us some hints about packages that we were using and are no longer available in our system. Upgrade the system To have a smoother experience, first step is to fully update the system to the latest version of the packages\n# EL5 onwards yum update --skip-broken Once we made sure that no further updates are available we\u0026rsquo;re ready for the next steps.\nDownload the install media to the system As indicated in the previous section, we need to download the install media to the system, this will make sure that we\u0026rsquo;ll have available a copy we can use without requiring network access to install whatever is needed in our host.\nWe\u0026rsquo;ll mount it to /mnt and create a repository file for using it in each release step. For mounting, we\u0026rsquo;ll execute:\nmount -o loop /root/rhel${VERSION}.iso /mnt Attempting the update The package managers yum and dnf include a flag that might be really useful as long as we\u0026rsquo;ve compatible repositories or we\u0026rsquo;ve made available our ISO install media on our system and created a custom repo definition for it.\nWe\u0026rsquo;ll follow this procedure on a RHEL base system, starting with RHEL 4.8 until RHEL 9 Beta.\nLet\u0026rsquo;s get ready on the environment\u0026hellip;\nRHEL 4.8 installation Proceed to install RHEL 4.8 via the ISO on the system so that we\u0026rsquo;ve the test-bed ready for our procedure. Note\nAs you might have experienced\u0026hellip; using a recent system to connect to a legacy one could be complicated as some insecure protocols have been disabled.\nCreate an entry like this in your .ssh/config file, so that insecure methods can be used to connect:\nHost 192.168.2.82 HostKeyAlgorithms=+ssh-rsa KexAlgorithms=+diffie-hellman-group1-sha1 PubkeyAcceptedKeyTypes=+ssh-rsa User root or alternatively on the command line:\nssh -oHostKeyAlgorithms=+ssh-rsa -oPubkeyAcceptedKeyTypes=+ssh-rsa root@192.168.2.8 RHEL 4.8 used up2date as method for connecting to RHN, however, as it is out of the support cycle I wouldn\u0026rsquo;t expect to get RHN working for it.\nAt that time, CentOS had the chance to create repositories and copy over the yum command so that it could be used on it, but let\u0026rsquo;s try to use what we\u0026rsquo;ve in the system\u0026hellip; first of all, we need to load the RHEL ISO image into our system.\nAs we don\u0026rsquo;t have access to RHN, let\u0026rsquo;s loopback mount the ISO image and let\u0026rsquo;s install screen:\n[root@rhel-instance mnt]# mount /root/rhel4.8.iso /mnt [root@rhel-instance mnt]# cd /mnt [root@rhel-instance mnt]# rpm -Uvh RedHat/RPMS/screen-4.0.2-5.x86_64.rpm Preparing... ########################################### [100%] 1:screen ########################################### [100%] [root@rhel-instance mnt]# From this point, we should execute screen and continue working from within that shell\nWe can now cd into our user home folder and umount /mnt to start the process.\nRHEL 4.8 to RHEL 5.x upgrade First, let\u0026rsquo;s mount RHEL 5.x CD-ROM on /mnt and let\u0026rsquo;s start over\u0026hellip;\nIn RHEL5, we used to have the main OS packages in the Server/ folder so let\u0026rsquo;s cd into it via cd /mnt/Server.\nIf you\u0026rsquo;re familiar with rpm the first attempt would be to use rpm -Fvh *.rpm to freshen all the packages, that is, install the updated versions and carry on\u0026hellip; but we\u0026rsquo;ll get into a dependency hell, and if you attempt it, lot of the installed packages might have been already removed, and not be possible to upgrade\u0026hellip;\nTo make our life easier, we need to do whatever we can to get yum and rpm packages updated to RHEL 5 versions, so that we can later use yum for the dependency resolution.\nNow, we begin a loop of package additions and removals to get to some state we can move forward\u0026hellip;\nWhen we first attempt to update rpm, yum, etc. we begging requiring python and upgrading it will complain about some of the system-config-lvm or system-config-network\u0026hellip; so the procedure is a try-and-error approach:\nrpm -e system-config-lvm rpm -e system-config-network rpm -e system-config-packages rpm -e gnome-python2-bonobo gnome-python2 gnome-python2-canvas And then start attempting to upgrade yum and rpm while you get into dependency hell like this:\nSo\u0026hellip; we realized it is a pain, so let\u0026rsquo;s grab some \u0026lsquo;CentOS\u0026rsquo; packages from the CentOS 4.9 VAULT that we can use to get yum working:\npython-elementtree-1.2.6-5.el4.centos.x86_64.rpm python-sqlite-1.1.7-1.2.1.x86_64.rpm python-urlgrabber-2.9.8-2.noarch.rpm sqlite-3.3.6-2.x86_64.rpm yum-2.4.3-4.el4.centos.noarch.rpm yum-metadata-parser-1.0-8.el4.centos.x86_64.rpm Let\u0026rsquo;s copy them to our system (in case we haven\u0026rsquo;t downloaded them there) and let\u0026rsquo;s install them\u0026hellip; using --nodeps as the packages require yumconf that we will not find as a built package:\n# Disable SELinux Enforcing setenforce 0 # Preferably, configure SElinux to be in permissive mode via editing the /etc/selinux/config file and setting it to \u0026#39;SELINUX=permissive\u0026#39; # Install the packages for yum rpm -Uvh --nodeps python-elementtree-1.2.6-5.el4.centos.x86_64.rpm python-sqlite-1.1.7-1.2.1.x86_64.rpm python-urlgrabber-2.9.8-2.noarch.rpm sqlite-3.3.6-2.x86_64.rpm yum-2.4.3-4.el4.centos.noarch.rpm yum-metadata-parser-1.0-8.el4.centos.x86_64.rpm Now, let\u0026rsquo;s create a repo file at /etc/yum.repos.d/cdrom.repo with the following contents:\n[cdrom] name=CDROM baseurl=file:///mnt/Server/ enabled=1 gpgcheck=0 From this point, we could use yum to install packages\u0026hellip;\nFirst packages to upgrade should be\u0026hellip; again\u0026hellip; yum and rpm and it will start several dependency loop checks that we\u0026rsquo;ll need to resolve\u0026hellip; but in a lot easier way than having to manually specify the packages to upgrade.\nAs we\u0026rsquo;re on a server, let\u0026rsquo;s remove some more packages to get this progressing:\nrpm -e pcmcia-cs pm-utils bluez-utils rpm -e xorg-x11-xfs chkfontpath urw-fonts rpm -e kudzu system-config-mouse system-config-network-tui system-config-soundcard --nodeps rpm -e autofs libgnomeui up2date Now, let\u0026rsquo;s update some packages:\nyum update hwdata --\u0026gt; Restarting Dependency Resolution with new changes. --\u0026gt; Populating transaction set with selected packages. Please wait. ---\u0026gt; Package glibc-common.x86_64 0:2.5-123 set to be updated --\u0026gt; Running transaction check --\u0026gt; Processing Conflict: glibc-common conflicts glibc \u0026lt; 2.5 --\u0026gt; Processing Dependency: glibc-common = 2.3.4-2.43 for package: glibc --\u0026gt; Restarting Dependency Resolution with new changes. --\u0026gt; Populating transaction set with selected packages. Please wait. ---\u0026gt; Package glibc.i686 0:2.5-123 set to be updated --\u0026gt; Running transaction check Dependencies Resolved ============================================================================= Package Arch Version Repository Size ============================================================================= Updating: hwdata noarch 0.213.30-1.el5 cdrom 477 k Updating for dependencies: glibc x86_64 2.5-123 cdrom 4.8 M glibc i686 2.5-123 cdrom 5.4 M glibc-common x86_64 2.5-123 cdrom 16 M module-init-tools x86_64 3.3-0.pre3.1.63.el5 cdrom 446 k Transaction Summary ============================================================================= Install 0 Package(s) Update 5 Package(s) Remove 0 Package(s) Total download size: 28 M Is this ok [y/N]: y Downloading Packages: Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Updating : glibc-common ####################### [ 1/10] Updating : glibc ####################### [ 2/10] Updating : glibc [ 3/10]warning: /etc/localtime created as /etc/localtime.rpmnew Updating : glibc [ 3/10]warning: /etc/nsswitch.conf created as /etc/nsswitch.conf.rpmnew Updating : glibc ####################### [ 3/10] Updating : module-init-tools ####################### [ 4/10] Updating : hwdata ####################### [ 5/10] Cleanup : glibc ####################### [ 6/10] Cleanup : module-init-tools ####################### [ 7/10] Cleanup : glibc-common ####################### [ 8/10] Cleanup : glibc ####################### [ 9/10] Cleanup : hwdata ####################### [10/10] Updated: hwdata.noarch 0:0.213.30-1.el5 Dependency Updated: glibc.x86_64 0:2.5-123 glibc.i686 0:2.5-123 glibc-common.x86_64 0:2.5-123 module-init-tools.x86_64 0:3.3-0.pre3.1.63.el5 Complete! First packages updated using yum!!\nNow, we\u0026rsquo;re hitting several issues with kernel being incompatible\u0026hellip; so let\u0026rsquo;s remove it but just from the database (as well as other packages):\nrpm -e kernel --nodeps --justdb --noscripts rpm -e python-elementtree util-linux yum initscripts glib2 procps --justdb --noscripts rpm -e system-config-securitylevel After those steps\u0026hellip; that again\u0026hellip; are critical we can start updating some more packages:\nyum -y install yum hal initscripts After it finishes, we can try again to update yum:\n[root@rhel-instance ~]# yum install yum There was a problem importing one of the Python modules required to run yum. The error leading to this problem was: No module named urlgrabber Please install a package which provides this module, or verify that the module is installed correctly. It\u0026#39;s possible that the above module doesn\u0026#39;t match the current version of Python, which is: 2.4.3 (#1, Oct 23 2012, 22:02:41) [GCC 4.1.2 20080704 (Red Hat 4.1.2-54)] If you cannot solve this problem yourself, please go to the yum faq at: http://wiki.linux.duke.edu/YumFaq And BAM!, it fails\u0026hellip; let\u0026rsquo;s check RPM for yum:\n[root@rhel-instance ~]# rpm -q yum rpmdb: Program version 4.3 doesn\u0026#39;t match environment version error: db4 error(-30974) from dbenv-\u0026gt;open: DB_VERSION_MISMATCH: Database environment version mismatch error: cannot open Packages index using db3 - (-30974) error: cannot open Packages database in /var/lib/rpm package yum is not installed BAM!, rpm can\u0026rsquo;t find the packages\u0026hellip;\nLet\u0026rsquo;s remove the old database of rpm:\nrm -fv /var/lib/rpm/__db* [root@rhel-instance ~]# cd /var/lib/rpm/ [root@rhel-instance rpm]# ls Basenames __db.000 __db.002 Dirnames Group Name Providename Pubkeys Requireversion Sigmd5 Conflictname __db.001 __db.003 Filemd5s Installtid Packages Provideversion Requirename Sha1header Triggername [root@rhel-instance rpm]# ls -l total 19800 -rw-r--r-- 1 root root 1445888 Mar 25 21:58 Basenames -rw-r--r-- 1 root root 12288 Mar 25 21:58 Conflictname -rw-r--r-- 1 root root 0 Mar 25 22:00 __db.000 -rw-r--r-- 1 root root 16384 Mar 25 20:02 __db.001 -rw-r--r-- 1 root root 1318912 Mar 25 20:02 __db.002 -rw-r--r-- 1 root root 663552 Mar 25 20:02 __db.003 -rw-r--r-- 1 root root 352256 Mar 25 21:58 Dirnames -rw-r--r-- 1 root root 2613248 Mar 25 21:58 Filemd5s -rw-r--r-- 1 root root 12288 Mar 25 21:58 Group -rw-r--r-- 1 root root 16384 Mar 25 21:58 Installtid -rw-r--r-- 1 root root 24576 Mar 25 21:58 Name -rw-r--r-- 1 root root 14315520 Mar 25 21:58 Packages -rw-r--r-- 1 root root 180224 Mar 25 21:58 Providename -rw-r--r-- 1 root root 65536 Mar 25 21:58 Provideversion -rw-r--r-- 1 root root 12288 Mar 25 21:57 Pubkeys -rw-r--r-- 1 root root 188416 Mar 25 21:58 Requirename -rw-r--r-- 1 root root 102400 Mar 25 21:58 Requireversion -rw-r--r-- 1 root root 45056 Mar 25 21:58 Sha1header -rw-r--r-- 1 root root 45056 Mar 25 21:58 Sigmd5 -rw-r--r-- 1 root root 12288 Mar 25 21:58 Triggername [root@rhel-instance rpm]# rm -fv __db.00* removed `__db.000\u0026#39; removed `__db.001\u0026#39; removed `__db.002\u0026#39; removed `__db.003\u0026#39; [root@rhel-instance rpm]# rpm -q yum yum-3.2.22-40.el5 Now, as you can see, we\u0026rsquo;ve yum again :-) (but it still doesn\u0026rsquo;t work)\u0026hellip;\nLet\u0026rsquo;s manually install the missing libraries:\nrpm -Uvh python-urlgrabber-3.1.0-6.el5.noarch.rpm m2crypto-0.16-9.el5.x86_64.rpm Next one failing after this\u0026hellip; is sqlite\u0026hellip;\nLet\u0026rsquo;s reinstall it:\nrpm -Uvh --force python-sqlite-1.1.7-1.2.1.x86_64.rpm After this step, yum works again, so let\u0026rsquo;s find next target for installation\u0026hellip; but first let\u0026rsquo;s install some more packages with rpm:\nrpm -Uvh rhn-client-tools-0.4.20.1-9.el5.noarch.rpm python-dmidecode-3.10.13-1.el5_5.1.x86_64.rpm rhn-check-0.4.20.1-9.el5.noarch.rpm rhn-setup-0.4.20.1-9.el5.noarch.rpm rhnsd-4.7.0-14.el5.x86_64.rpm yum-rhn-plugin-0.5.4.1-7.el5.noarch.rpm rhnlib-2.5.22.1-6.el5.noarch.rpm Let\u0026rsquo;s install some packages as well as some of the already installed ones\u0026hellip;\n[root@localhost Server]# rpm -Uvh libpcap-0.9.4-15.el5.x86_64.rpm ppp-2.4.4-2.el5.x86_64.rpm kernel-2.6.18-398.el5.x86_64.rpm mkinitrd-5.1.19.6-82.el5.x86_64.rpm procps-3.2.7-26.el5.x86_64.rpm nash-5.1.19.6-82.el5.x86_64.rpm kpartx-0.4.7-63.el5.x86_64.rpm e2fsprogs-1.39-37.el5.x86_64.rpm device-mapper-multipath-0.4.7-63.el5.x86_64.rpm selinux-policy-targeted-2.4.6-351.el5.noarch.rpm hmaccalc-0.9.6-4.el5.x86_64.rpm iscsi-initiator-utils-6.2.0.872-16.el5.x86_64.rpm lksctp-tools-1.0.6-3.el5.x86_64.rpm selinux-policy-targeted-2.4.6-351.el5.noarch.rpm quota-3.13-8.el5.x86_64.rpm krb5-workstation-1.6.1-78.el5.x86_64.rpm NetworkManager-0.7.0-13.el5.x86_64.rpm parted-1.8.1-30.el5.x86_64.rpm util-linux-2.13-0.59.el5_8.x86_64.rpm cryptsetup-luks-1.0.3-8.el5.x86_64.rpm openssh-server-4.3p2-82.el5.x86_64.rpm gtk2-2.10.4-30.el5.x86_64.rpm gnupg-1.4.5-18.el5_10.1.x86_64.rpm e2fsprogs-libs-1.39-37.el5.x86_64.rpm isdn4k-utils-3.2-56.el5.x86_64.rpm libsysfs-2.1.0-1.el5.x86_64.rpm policycoreutils-1.33.12-14.13.el5.x86_64.rpm selinux-policy-2.4.6-351.el5.noarch.rpm audit-libs-python-1.8-2.el5.x86_64.rpm libselinux-python-1.33.4-5.7.el5.x86_64.rpm libsemanage-1.9.1-4.4.el5.x86_64.rpm --force Let\u0026rsquo;s get remove some other conflict packages\u0026hellip;\n[root@localhost Server]# rpm -Uvh gettext-0.17-1.el5.x86_64.rpm libgomp-4.4.7-1.el5.x86_64.rpm For Updating the filesystem package we need to make a trick\u0026hellip; find the file on the /mnt folder\u0026hellip; copy it over to /root and umount the CD-ROM\nThen install it from the root folder:\nrpm -Uvh filesystem.rpm Once finished, remount the CD-ROM and let\u0026rsquo;s continue with some other dependencies:\nrpm -Uvh openib-1.5.4.1-4.el5.noarch.rpm pycairo-1.2.0-1.1.x86_64.rpm rpm -Uvh gnome-python2-canvas-2.16.0-1.fc6.x86_64.rpm yelp-2.16.0-30.el5_9.x86_64.rpm firstboot-1.4.27.9-1.el5.x86_64.rpm rhpxl-0.41.1-12.el5.x86_64.rpm metacity-2.16.0-16.el5.x86_64.rpm gnome-python2-2.16.0-1.fc6.x86_64.rpm gnome-doc-utils-0.8.0-2.fc6.noarch.rpm scrollkeeper-0.3.14-9.el5.x86_64.rpm firstboot-tui-1.4.27.9-1.el5.x86_64.rpm system-config-display-1.0.48-4.el5.noarch.rpm system-config-network-1.3.99.23-1.el5.noarch.rpm redhat-artwork-5.0.9-2.el5.x86_64.rpm docbook-dtds-1.0-30.1.noarch.rpm xorg-x11-fonts-Type1-7.1-2.1.el5.noarch.rpm pkgconfig-0.21-2.el5.x86_64.rpm system-config-soundcard-2.0.6-1.el5.noarch.rpm alsa-utils-1.0.17-7.el5.x86_64.rpm xml-common-0.6.3-18.noarch.rpm openjade-1.3.2-27.x86_64.rpm redhat-menus-6.7.8-3.el5.noarch.rpm redhat-logos-4.9.16-1.noarch.rpm sgml-common-0.6.3-18.noarch.rpm xorg-x11-server-Xorg-1.1.1-48.101.el5_10.3.x86_64.rpm system-config-securitylevel-tui-1.6.29.1-6.el5.x86_64.rpm xorg-x11-utils-7.1-2.fc6.x86_64.rpm xorg-x11-server-utils-7.1-5.el5_6.2.x86_64.rpm xorg-x11-drv-vesa-1.3.0-8.3.el5.x86_64.rpm xorg-x11-drv-void-1.1.0-3.1.x86_64.rpm xorg-x11-drv-evdev-1.0.0.5-5.el5.x86_64.rpm xorg-x11-drv-mouse-1.1.1-1.1.x86_64.rpm cpp-4.1.2-55.el5.x86_64.rpm gtk2-engines-2.8.0-3.el5.x86_64.rpm gnome-python2-2.16.0-1.fc6.x86_64.rpm gnome-python2-bonobo-2.16.0-1.fc6.x86_64.rpm pyxf86config-0.3.31-3.el5.x86_64.rpm system-config-securitylevel-1.6.29.1-6.el5.x86_64.rpm xorg-x11-drv-keyboard-1.1.0-3.x86_64.rpm xorg-x11-fonts-base-7.1-2.1.el5.noarch.rpm gnome-python2-gnomevfs-2.16.0-1.fc6.x86_64.rpm --nodeps Let\u0026rsquo;s install some more missing dependencies with rpm:\nrpm -Uvh python-numeric-23.7-2.2.2.el5_6.1.x86_64.rpm pygobject2-2.12.1-5.el5.x86_64.rpm And let\u0026rsquo;s attempt freshen of all the remaining packages:\nrpm -Fvh *.rpm # Let\u0026#39;s also install createrepo... we\u0026#39;ll need it later rpm -ivh createrepo*.rpm After this\u0026hellip; only a few packages are missing to upgrade, yum is still not working as it complains with some problems with the rpm database, so we need to continue with rpm:\nrpm -Uvh kexec-tools-1.102pre-165.el5.x86_64.rpm busybox-1.2.0-14.el5.x86_64.rpm mesa-libGL-6.5.1-7.11.el5_9.x86_64.rpm libdrm-2.0.2-1.1.x86_64.rpm rpm -Uvh mlocate-0.15-1.el5.2.x86_64.rpm redhat-release-notes-5Server-52.x86_64.rpm tzdata-2014e-1.el5.x86_64.rpm Here, yum now reports that no packages are missing to update\u0026hellip; so let\u0026rsquo;s attempt to reboot the system and cross fingers..\nIf the system rebooted, make sure it shows the right kernel for EL5\u0026hellip; 2.6.9 is a bad value\u0026hellip; it should read 2.6.18\u0026hellip; if you\u0026rsquo;re in that case, once the system boots, mount the CD-ROM again, and force kernel reinstallation via:\nrpm -ivh --force /mnt/Server/kernel-2.6.18-398.el5.x86_64.rpm And reboot again.\nRHEL5 to RHEL6 upgrade Wow, now we\u0026rsquo;re on RHEL5, yum is already native in this version, so that we can update the cdrom.repo file to point to the root of the mount point instead of the Server folder, then run yum update and\u0026hellip; BAM!, incompatible hash for the repository\u0026hellip;\nAs we installed createrepo before we can cd into our home folder and run:\n[root@localhost ~]# cd [root@localhost ~]# mkdir el6 [root@localhost ~]# for file in /mnt/Packages/*.rpm; do ln -s $file . ;done [root@localhost ~]# createrepo -o el6 -u file:///mnt/Packages/ /mnt/Packages/ 3861/3861 - zsh-4.3.11-8.el6.x86_64.rpm Saving Primary metadata Saving file lists metadata Saving other metadata Once this is done, we need to update our cdrom.repo file to point to the location file:///root/el6 ;-)\nLet\u0026rsquo;s start again\u0026hellip; removing some packages:\nyum remove system-config-* kudzu rhnlib python-numeric rhpl avahi ipsec-tools OpenIPMI authconfig yum remove xorg-x11-server-utils xorg-x11-server-Xorg xorg-x11-utils xorg-x11-fonts-base xorg-x11-fonts-Type1 openjade rpm -e sudo-1.6.7p5-30.1.5.x86_64 --justdb # (as package-cleanup --dupes was showing it) yum remove OpenIPMI-libs-2.0.16-16.el5.x86_64 bluez-hcidump-1.32-1.x86_64 rpm -e libnotify-0.4.2-6.el5.x86_64 net-snmp-libs-5.3.2.2-22.el5_10.1.x86_64 libbonobo-2.16.0-1.1.el5_5.1.x86_64 libibcommon-1.2.0-1.el5.x86_64 libsdp-1.1.108-1.el5.x86_64 bluez-libs-3.7-1.1.x86_64 glib-1.2.10-20.el5.x86_64 libmthca-1.0.6-1.el5.x86_64 compat-libstdc++-33-3.2.3-61.x86_64 libgnomecanvas-2.14.0-4.1.x86_64 libart_lgpl-2.3.17-4.x86_64 # Remove packages that are not from x86_64 (it used to be required in older ones, but not any longer.) uname -a |grep x86_64 \u0026amp;\u0026amp; yum remove *.i{3,4,5,6}86 Ok, so we\u0026rsquo;re back on having issues with rpm features that we don\u0026rsquo;t have at EL5\u0026hellip; so we need to update rpm and required components so that we can continue installing newer packages.\nThis approach brings us to the egg-chicken issue\u0026hellip; we can\u0026rsquo;t install the packages because our installed version is old, and until we update them, we can\u0026rsquo;t install the other updates\u0026hellip; and even cpio is older than the version used, so we might need to use an intermediate machine to uncompress the rpm related packages copy them over, and use them to install the updates on the system itself.\nWe need to prepare and uncompress the following files from the install media on a newer machine\nPACKAGES=\u0026#34;db4-4.7.25-16.el6.x86_64.rpm glibc-2.12-1.7.el6.x86_64.rpm glibc-common-2.12-1.7.el6.x86_64.rpm glibc-utils-2.12-1.7.el6.x86_64.rpm libcap-2.16-5.2.el6.x86_64.rpm lua-5.1.4-4.1.el6.x86_64.rpm popt-1.13-7.el6.x86_64.rpm rpm-4.8.0-12.el6.x86_64.rpm rpm-build-4.8.0-12.el6.x86_64.rpm rpm-libs-4.8.0-12.el6.x86_64.rpm rpm-python-4.8.0-12.el6.x86_64.rpm xz-libs-4.999.9-0.3.beta.20091007git.el6.x86_64.rpm\u0026#34; for package in $PACKAGES;do echo \u0026#34;Uncompressing $package\u0026#34; rpm2cpio /mnt/Packages/$package | cpio -idmv done Once the process has finished, you\u0026rsquo;ll get a list of folders that need to be copied back to the host\u0026hellip; note, this will overwrite libraries, binaries and can render your system unusable.\nOnce that step is done\u0026hellip; your system should have RPM version 4.8.0 and you can use it to install and force install several set of packages\u0026hellip; until you can get yum working again\u0026hellip;\nIn this case, yum provided in EL6, features the distro-sync command, which is a simple way to update the system to the latest available packages in our local repository.\nYum might complain of some missing packages.. ensure to get yum packages installed, as well as expat, SSL and some other dependencies that will complain.\nOnce YUM is working and distro-sync has finished, we need to edit the cdrom.repo to act against the folder with the packages (so that it has the group information), perform a yum clean and then yum groupinstall base so that all the missing packages from a base system are installed.. resolving conflicts like the redhat-release-notes etc by manually removing and then adding the new versions of the files.\nEnsure also to remove all packages named .el5. from our system\u0026hellip; we should be on el6 now\u0026hellip;\nAnd our system is ready for next step!\nRHEL6 to RHEL7 upgrade Well, we got until this point, so we\u0026rsquo;re back into the versions that are \u0026lsquo;supported\u0026rsquo; under the ELS.\nLet\u0026rsquo;s follow the standard approach to register our system and attach to a pool:\nyum -y install subscription-manager subscription-manager register subscription-manager attach pool=XXXXXXX Then, let\u0026rsquo;s add extra repositories we might require:\nsubscription-manager repos --enable rhel-6-server-extras-rpms subscription-manager repos --enable rhel-6-server-optional-rpms # And then, let\u0026#39;s install preupgrade tools yum install preupgrade-assistant preupgrade-assistant-el6toel7 redhat-upgrade-tool Check for the next steps in the official documentation:\nhttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/upgrading_from_rhel_6_to_rhel_7/index\nEnjoy! (and if you do, you can Buy Me a Coffee ) tmux or screen allows to disconnect, re-attach or even open new terminals without having to open a new ssh connection, this makes it a lot easier to spawn a new shell and operate without risking a disconnection from our system.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iranzo.io/blog/2022/04/23/unsupported-upgrade-for-rhel-based-distributions-without-reinstallation/","summary":"\u003cp\u003eMost systems, based on RHEL, were not able to upgrade without\nreinstallation, or best said, not supported. The new version, that was\nreleased at around 18 months later contained so many changes that it was\nhard to test the upgrades themselves until \u003ccode\u003eleapp\u003c/code\u003e was introduced.\u003c/p\u003e\n\u003cp\u003eCheck the lifecycle here: \u003ca href=\"https://access.redhat.com/support/policy/updates/errata\"\u003ehttps://access.redhat.com/support/policy/updates/errata\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHowever, the biggest problem could be the incompatibility of packages or\npackage formats\u0026hellip; but as usually there were some middle layers it was\npossible to upgrade without reinstallation by performing some manual steps.\u003c/p\u003e","title":"Unsupported upgrade for RHEL-based distributions without reinstallation"},{"content":"Inside the game The Sky Walker Saga PS4, PS5, Xbox, Nintendo Switch, you can introduce some codes to unlock additional characters to use, so far this is the list I\u0026rsquo;ve found:\nCode Character/vehicle 3FCPPVX Tarkin ARVALA7 Razor Crest BAC1CKP Mister Bones C3PHOHO C-3P0 (Party special) GR2VBXF Ratts Tyerell KH7P320 Aayla Secura KORDOKU Poe Dameron (Party special) LIFEDAY GNK Droid (Party special) OKV7TLR Dengar SHUTTLE Resistance ITS SIDIOUS Palpatine SKYSAGA Temmin Wexley T9LM1QF Shmi TIPYIPS D-O (Party special) VT1LFNH Shaak Ti WBFE4GO Nute Gunray WOOKIEE Chewbacca (Party special) WROSHYR Darth Vader (Party special) XV4WND9 Holdo Z55T8CQ Poggle Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/04/10/lego-the-skywalker-saga-codes/","summary":"\u003cp\u003eInside the game The Sky Walker Saga \u003ca href=\"https://www.amazon.es/dp/B08VTB4X2J?tag=redken-21\"\u003ePS4\u003c/a\u003e, \u003ca href=\"https://www.amazon.es/dp/B08VTC518G?tag=redken-21\"\u003ePS5\u003c/a\u003e, \u003ca href=\"https://www.amazon.es/dp/B08VTBT8TY?tag=redken-21\"\u003eXbox\u003c/a\u003e, \u003ca href=\"https://www.amazon.es/dp/B08VTDBTDB?tag=redken-21\"\u003eNintendo Switch\u003c/a\u003e, you can introduce some codes to unlock additional characters to use, so far this is the list I\u0026rsquo;ve found:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eCode\u003c/th\u003e\n          \u003cth\u003eCharacter/vehicle\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e3FCPPVX\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eTarkin\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eARVALA7\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eRazor Crest\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eBAC1CKP\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eMister Bones\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eC3PHOHO\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eC-3P0 (Party special)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eGR2VBXF\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eRatts Tyerell\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eKH7P320\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eAayla Secura\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eKORDOKU\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003ePoe Dameron (Party special)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLIFEDAY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eGNK Droid (Party special)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOKV7TLR\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDengar\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSHUTTLE\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eResistance ITS\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSIDIOUS\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003ePalpatine\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSKYSAGA\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eTemmin Wexley\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eT9LM1QF\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eShmi\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eTIPYIPS\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eD-O (Party special)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eVT1LFNH\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eShaak Ti\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eWBFE4GO\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eNute Gunray\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eWOOKIEE\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eChewbacca (Party special)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eWROSHYR\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDarth Vader (Party special)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eXV4WND9\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eHoldo\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eZ55T8CQ\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003ePoggle\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego The SkyWalker Saga Codes"},{"content":"Since long ago I had it in my mind getting one remote presenter, but most presenters just had two buttons, and the ones that looked to be valid for my use case, required four and seems that only Rii had similar devices, but I didn\u0026rsquo;t went for it as it was not a huge need, so I ended up with a mini keyboard I had for Raspberry Pi and some debugging in case I had that need.\nBut yesterday I found in the recycling IT area, together with several really old computers, a Logitech R400 presentation remote. It has one of those rubber coatings that make it comfortable at hand when it\u0026rsquo;s new, but it\u0026rsquo;s sticky once it gets older (had similar experience with other stuff, even umbrellas\u0026hellip;)\nSome alcohol and a soft cloth helped removing that stickiness, so next step was to test it\u0026hellip; plugged the receiver in the USB port, and put two batteries in\u0026hellip; and it powered up, bot left and right buttons were working, and the laser pointer. Thing is that the remote has two additional buttons for starting presentation and going to blank screen that didn\u0026rsquo;t worked.\nIn a rush, I thought about opening it, thinking that the reason of having it discarded was that one, but nothing strange in the inside, everything was clean, no battery spill, etc, so I reassembled and started looking for information about it.\nUsing xev I was able to see that the key buttons were received on the computer, the one on the left, provided two different key codes and the one on the right just one, but was not mapped to anything.\nAfter some search, I found this blog post, but a copy-paste didn\u0026rsquo;t worked\u0026hellip; seems that the USB receiver had changed the identifier in the meantime so I had to update to match mines.\nFirst of all, I did checked with lsusb the device ID and Vendor ID, which was 046D and C52D, so I used those values when filling the next two files.\nFirst, I created /etc/udev/hwdb.d/99-logitech-r400.hwdb:\n# The lower left button actually emits two # different scancodes depending on the state of # the \u0026#34;presentation\u0026#34;. # E.g. one code to start and one to stop. keyboard:usb:v046DpC52D KEYBOARD_KEY_70029=up KEYBOARD_KEY_7003E=up KEYBOARD_KEY_70037=down KEYBOARD_KEY_7004B=left KEYBOARD_KEY_7004E=right This file is mapping the two events in the left button to be \u0026lsquo;up\u0026rsquo; and the one on the right to be \u0026lsquo;down\u0026rsquo;\u0026hellip; I did this because in the past I used to do some presentations using reveal.js and it was interesting to have those kind of several-level presentations, so that you could go in deep on a topic or move to next one depending on the time available\nand /etc/udev/rules.d/99-logitech-r400.rules:\nSUBSYSTEMS==\u0026#34;usb\u0026#34;, ATTRS{idVendor}==\u0026#34;046d\u0026#34;, ATTRS{idProduct}==\u0026#34;c52d\u0026#34;, IMPORT{builtin}=\u0026#34;hwdb \u0026#39;keyboard:usb:v046DpC52D\u0026#39;\u0026#34;, RUN{builtin}+=\u0026#34;keyboard\u0026#34; The first file, defines the key mappings, so that the actual 4 buttons, work as I wanted, and the second one, ensures that when the device is plugged, the mappings will be loaded\u0026hellip; after this, just unplug and replug, and voilà, the presenter was working perfectly\u0026hellip; so nice finding, nice recycling via \u0026lsquo;reuse\u0026rsquo; and new toy!.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/04/01/logitech-r400-remote-presentation-controller-on-linux/","summary":"\u003cp\u003eSince long ago I had it in my mind getting one remote presenter, but most presenters just had two buttons, and the ones that looked to be valid for my use case, required four and seems that only \u003ccode\u003eRii\u003c/code\u003e had similar devices, but I didn\u0026rsquo;t went for it as it was not a huge need, so I ended up with a mini keyboard I had for Raspberry Pi and some debugging in case I had that need.\u003c/p\u003e","title":"Logitech R400 remote presentation controller on Linux"},{"content":"Setting up WireGuard is not a difficult process but I wanted to automate it among hosts by using a simple playbook that can be executed against the hosts and get it configured and deployed in a simple way.\nI also wanted to require the minimum possible number of values in the inventory, so tried to automate lot of the information required, leaving in the end only some required values:\nwireguard: True wgrole: \u0026#39;master\u0026#39; or \u0026#39;something else\u0026#39; wgport: port number to use The first step was to create the private and public key once the wireguard package is installed.\nThis was more or less easy, just run and store the output in the folder for WireGuard.\n# Create private key wg genkey | tee privatekey # Create public key wg pubkey \u0026lt; privatekey | tee publickey Later, I could set via Ansible\u0026rsquo;s set_fact:\n- name: Set WireGuard keys set_fact: wgprivatekey: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;privatekey\u0026#39;) }}\u0026#34; wgpublickey: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;publickey\u0026#39;) }}\u0026#34; But I got to the first problem\u0026hellip; the facts are set by the host running the playbook, and for setting master/client connection I need to use the master\u0026rsquo;s public key available on the client, and the client\u0026rsquo;s public key on the master.\nAs the fact was setup at execution, other hosts couldn\u0026rsquo;t check them via hostvars\u0026hellip;\nTo solve this issue I went by creating a custom fact that is copied over the hosts, and that causes the host to refresh if something has been copied, this made the fact available in the execution:\nFacts for private key:\n#!/bin/bash echo \u0026#34;{\\\u0026#34;wgprivkey\\\u0026#34; : \\\u0026#34;$(cat /etc/wireguard/privatekey)\\\u0026#34;}\u0026#34; Fact for public key:\n#!/bin/bash echo \u0026#34;{\\\u0026#34;wgpubkey\\\u0026#34; : \\\u0026#34;$(cat /etc/wireguard/publickey)\\\u0026#34;}\u0026#34; Note that both, provide output in a JSON compatible format.\nWe now need to copy the facts to the hosts and refresh the data if needed:\n- name: Create directory for ansible custom facts file: state: directory recurse: yes path: /etc/ansible/facts.d register: facts_dir_created - name: Copy custom facts copy: src: \u0026#34;{{ item }}\u0026#34; dest: /etc/ansible/facts.d/ owner: root group: root mode: 0755 with_fileglob: - \u0026#34;facts/*.fact\u0026#34; register: facts_copied - name: \u0026#34;Re-run setup to use custom facts\u0026#34; setup: ~ when: facts_copied.changed Now\u0026hellip; all hosts have the facts for private and public key so that can be used\u0026hellip; but we need to actually create WireGuard configuration file for it\u0026hellip; but wait\u0026hellip; we need to know which host is the master that will receive all connections from the clients.\nSo, let\u0026rsquo;s detect the master by the wgrole value:\n- name: Set wireguard master server host set_fact: wgmaster: \u0026#34;{{ item }}\u0026#34; with_items: \u0026#34;{{ groups.all }}\u0026#34; when: hostvars[item].wgrole is defined and hostvars[item].wgrole == \u0026#39;master\u0026#39; and wireguard == True Above task will loop across all hosts, check for the wgrole defined and equal to master (with wireguard enabled), and set the wgmaster fact to the hostname.\nWe will also need to calculate the IP to use for the master and the client in a private range\u0026hellip; so let\u0026rsquo;s just get the number of item in the list for this:\n- name: Calculate IP for host set_fact: wgip: \u0026#34;10.0.0.{{ lookup(\u0026#39;ansible.utils.index_of\u0026#39;, groups.all, \u0026#39;eq\u0026#39;, inventory_hostname) }}\u0026#34; Now, each host will get a \u0026lsquo;fact\u0026rsquo; wgip with the content similar to 10.0.0.1 that we can use to connect them.\nSo\u0026hellip; we should have all the information to create the configuration file for each client host:\n- name: Create configuration file for client copy: dest: \u0026#34;/etc/wireguard/wg0.conf\u0026#34; mode: 0644 content: | [Interface] ListenPort = {{ wgport }} PrivateKey = {{ ansible_local.wgprivkey.wgprivkey }} [Peer] PublicKey = {{ hostvars[wgmaster].ansible_local.wgpubkey.wgpubkey }} AllowedIPs = {{ hostvars[wgmaster].wgip }}/32 Endpoint = {{ hostvars[wgmaster].inventory_hostname }}:{{ hostvars[wgmaster].wgport }} when: wireguard == True and wgrole is defined and wgrole != \u0026#39;master\u0026#39; In above example, check that we use hostvars with the wgmaster value we obtained, in order to fill-in the values for the master, and make use of ansible_local to grab the values that our custom facts generated.\nSo far, it has been more or less easy\u0026hellip; the problem is that in the master, we need to build a base section ([Interface]) and then, ad the client section ([Peer]) with the client\u0026rsquo;s public key and the IP of the client.\nNext, let\u0026rsquo;s check the one for the master:\n- name: Create configuration file for master copy: dest: \u0026#34;/etc/wireguard/wg0.conf\u0026#34; mode: 0644 content: | [Interface] ListenPort = {{ wgport }} PrivateKey = {{ hostvars[wgmaster].ansible_local.wgprivkey.wgprivkey }} {%- for item in hostvars -%} {% if hostvars[item].wgrole is defined and hostvars[item].wgrole != \u0026#39;master\u0026#39; %} [Peer] PublicKey = {{ hostvars[item].ansible_local.wgpubkey.wgpubkey }} AllowedIPs = {{ hostvars[item].wgip }} /32 Endpoint = {{ hostvars[item].inventory_hostname }}:{{ hostvars[item].wgport }} {% endif %} {%- endfor -%} when: wireguard == True and wgrole is defined and wgrole == \u0026#39;master\u0026#39; and item == wgmaster with_inventory_hostnames: - all More or less, the logic should be clear\u0026hellip; we iterate over all the hosts with wireguard: True and wgrole not master to add their section\u0026hellip;. and this only runs on the master host, a the clients would have get the previous task instead.\nOf course, extra tasks would be needed for:\nBringing service up Open firewall ports etc. Hope you liked it!, it got me busy for some time until all pieces matched together :) Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/03/17/ansible-setup-for-vpn-using-wireguard/","summary":"\u003cp\u003eSetting up WireGuard is not a difficult process but I wanted to automate it among hosts by using a simple playbook that can be executed against the hosts and get it configured and deployed in a simple way.\u003c/p\u003e\n\u003cp\u003eI also wanted to require the minimum possible number of values in the inventory, so tried to automate lot of the information required, leaving in the end only some required values:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003ewireguard\u003c/span\u003e: \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003ewgrole\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;master\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003eor \u0026#39;something else\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003ewgport\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eport number to use\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe first step was to create the private and public key once the \u003ccode\u003ewireguard\u003c/code\u003e package is installed.\u003c/p\u003e","title":"Ansible setup for VPN using WireGuard"},{"content":"During the last months, I\u0026rsquo;ve been working with my colleagues on setting up a workflow that can be used to deploy Spoke clusters in an automated way.\nMore or less, the idea behind this is that a cluster is configured via a set of configuration files and templates so that the required components are installed:\nAdvanced Cluster Management (ACM) Quay registry OpenShift Data Foundation OpenShift Pipelines (a.k.a. Tekton) etc Once the system was configured, it would become a \u0026lsquo;Hub\u0026rsquo;, and from it, several \u0026lsquo;Spokes\u0026rsquo; could be deployed using ACM, getting them configured in the process to become a cluster suitable for fully-disconnected operation.\nWhen we first started, the approach was to use GitHub Actions Runner on our local system(a beefed hypervisor) until we could get hands on physical hardware, but during part of the development, it was decided to use OpenShift Pipelines, to provide an integrated experience for the final users.\nWe\u0026rsquo;re working on this repo: https://github.com/rh-ecosystem-edge/ztp-pipeline-relocatable where we\u0026rsquo;re putting both the scripts, manifests and the documentation for the whole process.\nAt this point, we\u0026rsquo;re using the following technologies:\nOpenShift OpenShift Pipelines (Tekton) Advanced Cluster Management (ACM) Quay registry This enables us to automate the deployment on top of an installed OpenShift with defined PVCs, all the required components are installed, and a mirror registry is created and configured, so that it\u0026rsquo;s used, via ICSPs as the source for any other image and Spokes installation.\nWith a new feature in ACM, those Spokes can become disconnected, and then, an as an individual entity, can later be \u0026lsquo;adopted\u0026rsquo; by another ACM instance for doing maintenance, opening a whole range of usage.\nYou can learn more about the details in our repository.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/03/12/zero-touch-provisioning-openshift-for-edge-computing/","summary":"\u003cp\u003eDuring the last months, I\u0026rsquo;ve been working with my colleagues on setting up a workflow that can be used to deploy Spoke clusters in an automated way.\u003c/p\u003e\n\u003cp\u003eMore or less, the idea behind this is that a cluster is configured via a set of configuration files and templates so that the required components are installed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced Cluster Management (ACM)\u003c/li\u003e\n\u003cli\u003eQuay registry\u003c/li\u003e\n\u003cli\u003eOpenShift Data Foundation\u003c/li\u003e\n\u003cli\u003eOpenShift Pipelines (a.k.a. Tekton)\u003c/li\u003e\n\u003cli\u003eetc\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOnce the system was configured, it would become a \u0026lsquo;Hub\u0026rsquo;, and from it, several \u0026lsquo;Spokes\u0026rsquo; could be deployed using ACM, getting them configured in the process to become a cluster suitable for fully-disconnected operation.\u003c/p\u003e","title":"Zero Touch Provisioning OpenShift for Edge computing"},{"content":"Redken in action with some examples Let\u0026rsquo;s learn about using Redken in Telegram or Discord with some examples\u0026hellip;. but first, let\u0026rsquo;s add the bot to a new group based on the platform we\u0026rsquo;re using:\nRedken_bot in Telegram Redken in discord Let\u0026rsquo;s get hands on! First of all, some of the commands shown will require admin privileges in the chat (from the user point of view) and from Redken.\nRedken uses admin privileges to:\nread the user admin level changes allow it to remove service messages (message pinned, user joined, etc) expand short URLs into longer ones kick users out of the chat, for example when a user is reported as spammer etc When the user is an admin of the chat, some commands like /gconfig will be available, but ignored for non-admin users, we\u0026rsquo;ll come back to this later in this tutorial.\nI assume you might have checked the Redken documentation\u0026hellip; but let\u0026rsquo;s go with the easy approach:\nAs said in the docs:\nBy default, new groups where the bot is added are just ready to start being used.\nSo, let\u0026rsquo;s start with that, let\u0026rsquo;s add a bot to a new group so that we can get working with it:\nWe\u0026rsquo;ve added the bot via the redken_bot username to our group, and it replied with the welcome message.\nEach group has an unique identifier that is used by redken to store configuration, karma, etc relevant to that group, this makes each group independent from others, unless you use the advanced feature of linking them together.\nSo basic usage is to say word++ or word--:\nWe can also reply to another message with just ++ or -- to give karma to that user or with == to give karma by simulating the same message as the one replied being sent by current user:\nThe bot, also speaks different languages, by default it will answer in English, but once it learns from you and the group the bot is in, will automatically switch to the most used language.\nYou can check the available translations and even contribute to improve them!\nAnd that\u0026rsquo;s all\u0026hellip; we can keep working with the bot as it is, or go ahead with other features!\nPut some configuration in place Setting language If we want to force the language to use, we can do so with the /gconfig set lang=it command, for setting it to Italian. Removal of service messages If we want to remove the service messages, we can do so with the /gconfig set removejoinparts=true command, for example:\nReduce number of karma messages If you\u0026rsquo;ve a very active group, the amount of karma messages might be too high, in that case, set a value for modulo that will show karma counts only after multiples of modulo.\nFor example /lconfig set modulo=5 will only show karma for 0,5,10, etc karma points.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2022/01/20/in-action...@redken_bot-with-examples/","summary":"\u003ch1 id=\"redken-in-action-with-some-examples\"\u003eRedken in action with some examples\u003c/h1\u003e\n\u003cp\u003eLet\u0026rsquo;s learn about using Redken in Telegram or Discord with some examples\u0026hellip;. but first, let\u0026rsquo;s add the bot to a new group based on the platform we\u0026rsquo;re using:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://t.me/redken_bot\"\u003eRedken_bot in Telegram\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://discord.com/oauth2/authorize?client_id=826069772822773790\u0026amp;scope=bot\u0026amp;permissions=8\"\u003eRedken in discord\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"lets-get-hands-on\"\u003eLet\u0026rsquo;s get hands on!\u003c/h2\u003e\n\u003cp\u003eFirst of all, some of the commands shown will require admin privileges in the chat (from the user point of view) and from Redken.\u003c/p\u003e\n\u003cp\u003eRedken uses admin privileges to:\u003c/p\u003e","title":"In action...@redken_bot with examples"},{"content":"Recently, some colleagues commented about validating if users in a Telegram group were or not employees anymore, so that the process could be automated without having to chase down the users that left the company.\nOne of the fields that can be configured by each user, is the link to other platforms (Github, LinkedIn, Twitter, Telegram, etc), so querying an LDAP server could suffice to get the list of users.\nFirst, we need to get some data required, in our case, we do anonymous binding to our LDAP server and the field to search for containing the \u0026lsquo;other platform\u0026rsquo; links.\nWe can do a simple query like this in Python:\nimport ldap myldap = ldap.initialize(\u0026#34;ldap://myldapserver:389\u0026#34;) binddn = \u0026#34;\u0026#34; pw = \u0026#34;\u0026#34; basedn = \u0026#34;ou=users,dc=example,dc=com\u0026#34; searchAttribute = [\u0026#34;SocialURL\u0026#34;] searchFilter = \u0026#34;(SocialURL=*)\u0026#34; # this will scope the entire subtree under UserUnits searchScope = ldap.SCOPE_SUBTREE # Bind to the server myldap.protocol_version = ldap.VERSION3 myldap.simple_bind_s( binddn, pw ) # myldap.simple_bind_s() if anonymous binding is desired # Perform the search ldap_result_id = myldap.search(basedn, searchScope, searchFilter, searchAttribute) result_set = [] while True: result_type, result_data = myldap.result(ldap_result_id, 0) if result_data == []: break else: if result_type == ldap.RES_SEARCH_ENTRY: result_set.append(result_data) # Unbind from server myldap.unbind_s() At this point, the variable result_set will contain the values we want to filter, for example, the url containing the username in https://t.me/USERNAMEform and the login id.\nThis, can be then acted accordingly and kick users that are no longer (or haven\u0026rsquo;t configured Telegram username) in the LDAP directory.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/10/19/ldap-query-from-python/","summary":"\u003cp\u003eRecently, some colleagues commented about validating if users in a Telegram group were or not employees anymore, so that the process could be automated without having to chase down the users that left the company.\u003c/p\u003e\n\u003cp\u003eOne of the fields that can be configured by each user, is the link to other platforms (Github, LinkedIn, Twitter, Telegram, etc), so querying an LDAP server could suffice to get the list of users.\u003c/p\u003e","title":"LDAP query from Python"},{"content":"As said in the article about mixnodes and validators, NYM is a technology aiming for providing privacy for the communications.\nOnce you get some tokens, PUNK at this time, you can use the web wallet to check the balance of your account and delegate it to mixnodes or gateways\u0026hellip; but, using the binaries, you can additionally delegate to validators.\nFor doing this, we first need the nymd binary on our system to follow the procedure for compiling it from the documentation for validators, but skip the remaining parts https://nymtech.net/docs/run-nym-nodes/validators/.\nSpecifically, the binaries we\u0026rsquo;re interested in are:\nlibwasmvm.so nymd Restoring the wallet When you created your wallet at https://testnet-milhon-wallet.nymtech.net/ you got a mnemonic phrase that can be used to access it or to restore\u0026hellip; we need that one (still keep it private and do not share with anyone).\nSo\u0026hellip; first things first, we need to restore the wallet with:\nnymd --keyring-backend=os keys add youruser --recover\nJust make sure to use the proper keyring-backend like os or file to store the key and decide the name youruser for holding and referring to it with later commands.\nOnce the key is created (restored), it will output the punkADDRESS we need to use later on.\nSet the variables we\u0026rsquo;re going to use: OPERADDRESS=punkvaloper1875deee8zecl6smhl2zg42ulpgsjn80cj8vq3x WALLET=$YOURpunkADDRESS # address from above step BACKEND=file # the backend you use for your keyring FROM=$YOURACCOUNT # the name of the account you restored in previous step (youruser in the example above) Staking against a validator We need to know the address of the validator, so we just need to go to the explorer https://testnet-milhon-blocks.nymtech.net/validators decide which one we want to stake on, and in the details page of it, get the address, for example: punkvaloper1xq1kABCDEqumupju86ljzlj6q2lqhdz2ne76gv and let\u0026rsquo;s store it as a variable as VALIDATOR.\nTo stake, we need to also know our current balance, but as we are not running nymd but using it as client, we need to specify a validator with the 26657 port open:\nnymd query bank balances ${WALLET} --node \u0026#34;tcp://testnet-milhon-validator1.nymtech.net:26657\u0026#34; Once we know the balance, we should get a value expressed in upunk and we should consider the commission for the network fees (5000upunk) and store as a number in a variable BALANCE.\nLet\u0026rsquo;s stake with this command:\nnymd tx staking delegate --node \u0026#34;tcp://testnet-milhon-validator1.nymtech.net:26657\u0026#34; -y ${VALIDATOR} ${BALANCE} --from ${youruser} --keyring-backend=${BACKEND} --chain-id \u0026#34;testnet-milhon\u0026#34; --gas=\u0026#34;auto\u0026#34; --gas-adjustment=1.15 --fees 5000upunk This will add the delegation and will start appearing on the explorer for the chosen validator.\nClaiming rewards After we staked for a while, we might be able to claim the rewards, note that this still requires \u0026lsquo;gas\u0026rsquo; in the form of upunk.\nFirst let\u0026rsquo;s check again our balance with:\nnymd query bank balances ${WALLET} --node \u0026#34;tcp://testnet-milhon-validator1.nymtech.net:26657\u0026#34; Let\u0026rsquo;s claim the rewards:\n~/.nymd/nymd --node \u0026#34;tcp://testnet-milhon-validator1.nymtech.net:26657\u0026#34; tx distribution withdraw-rewards -y ${VALIDATOR} --from ${youruser} --keyring-backend=${BACKEND} --chain-id=\u0026#39;testnet-milhon\u0026#39; --gas=\u0026#39;auto\u0026#39; --gas-adjustment=1.15 --fees 5000upunk And let\u0026rsquo;s check again the balance with\nnymd query bank balances punkADDRESS --node \u0026#34;tcp://testnet-milhon-validator1.nymtech.net:26657\u0026#34; If everything went fine, and we got the tokens there for a while, we should see a growing number of tokens back, that\u0026hellip; can be delegated again.\nWarning\nEach transaction requires to pay a fee, so do not try to hurry too much until you make an estimation on how is the staking rewards going on.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/09/28/how-to-stake-on-nym-validator/","summary":"\u003cp\u003eAs said in the article about \u003ca href=\"/blog/2021/05/09/how-to-run-a-nym-mixnode/\"\u003emixnodes\u003c/a\u003e and \u003ca href=\"/blog/2021/05/09/how-to-run-a-nym-validator/\"\u003evalidators\u003c/a\u003e, NYM is a technology aiming for providing privacy for the communications.\u003c/p\u003e\n\u003cp\u003eOnce you get some tokens, \u003ccode\u003ePUNK\u003c/code\u003e at this time, you can use the web wallet to check the balance of your account and delegate it to mixnodes or gateways\u0026hellip; but, using the binaries, you can additionally delegate to validators.\u003c/p\u003e\n\u003cp\u003eFor doing this, we first need the \u003ccode\u003enymd\u003c/code\u003e binary on our system to follow the procedure for compiling it from the documentation for validators, but skip the remaining parts \u003ca href=\"https://nymtech.net/docs/run-nym-nodes/validators/\"\u003ehttps://nymtech.net/docs/run-nym-nodes/validators/\u003c/a\u003e.\u003c/p\u003e","title":"How to stake on NYM Validator 🐳🐳🐳"},{"content":"I wanted to write about my experience (before I forget about it), and as some colleagues asked about it\u0026hellip; here we go\u0026hellip;\nAs published in the blog entry RHEL8 Administration book, some colleagues and I wrote a book on RHEL8 administration, which can be bought here.\nMany years ago I started one about Linux, but every time a new paragraph was added, a lot of new \u0026lsquo;TO-DO\u0026rsquo; items were appended as the information growth\u0026hellip; and as it was a \u0026lsquo;solo\u0026rsquo; project, I had other stuff to work on and was parked.\nLater last year (2020), Miguel approached asking if I was interested in helping him with his book, he started it, but the schedule was a bit tight, not impossible, but, having to work on the book at night, once kids are sleeping, you might be tired of work, etc\u0026hellip; was not the best one, so after some thinking about it, I told him that I was willing to help with the task, which automatically, duplicated the available time for each chapter.\nNot all chapters were equal, I must admit, some took me more time to \u0026lsquo;start\u0026rsquo;, but I think it was a good experience, I learned a lot, and I think it will help others in the future.\nMany times I end up spending time digging on issues, trying to find the right answer, and once found, it seemed pretty obvious that it should be the way since the beginning, thing is\u0026hellip; to realize that, you need to spend the time digging, testing, etc\u0026hellip; and, even if I try to publish some stuff on the blog about that sort of \u0026rsquo;tricks\u0026rsquo; I tend to think that those are not helpful, so I end up not adding them most of the times.\nWith the book, while working on the chapters, I had time to revisit stuff, that was obvious in my head, but not for others that are in the process of learning, and to even refresh the status of projects that I wasn\u0026rsquo;t touching for a while.\nFor example, when I started working in consulting, I was doing a lot of Anaconda for automating installations, with custom scripts to detect the hardware, write to serial ports to show data on systems without monitors, but since I moved to more \u0026lsquo;professional\u0026rsquo; setups, my experience switched more to the post-installation configuration using playbooks in Ansible to do get the things done.\nLet\u0026rsquo;s back to talk about the book itself\u0026hellip;\nAfter we divided the chapters the work was more bearable and compatible with the time we had available, so the focus was working on the chapters. So, since January, I was enrolled in this effort :-)\nAs Miguel had an outline of the topics to cover for each chapter, it was easy to use the freedom we had to cover them, the examples, but of course, always having in sight the focus on the tasks that a system administrator would perform at RHCSA level. One of the metrics we had, was the expected page count, but also, it was flexible and some chapters were bigger than expected and some others, smaller, of course, depending on the topic and well\u0026hellip;. we finally exceeded in about 100 pages the expected page count.\nOn the tech side, I\u0026rsquo;m used to Markdown and I\u0026rsquo;ve used it at several roles in my career, and lately, I was doing Asciidoctor. I\u0026rsquo;m not a big expert in Asciidoctor but it had many features that I felt being useful for the book, however, to my surprise, the tool of choice was a word processor with some custom styling. I understand, that using a regular word processor makes it easier for other writers, and having version control was also useful, but still was a bit strange for me, as I was used to code reviews, easy to perform on text files, and also, doing cross-references between different documents. Even when working with SuSE on the manuals, the choice was LaTeX, which was even harder but allowed rich features on the final rendering.\nOn the other side, the Packt team provided good guidance for starting:\nStyling guide with some example documents demonstrating each style usage Shared drive for uploading the files and work on reviews And lot of support for the questions that arise And what was more invaluable: guidance by several members of the team about the styles to use, tense, writing style, etc\nThe process was quite fast, once each chapter was submitted, in few days or a week, there were some reviews and later, a technical reviewer did the same, providing a feedback form on the chapter and some other things to fix or improve.\nIn the end, going chapter after chapter, it went quickly through\u0026hellip; my biggest pain points were the chapters where I felt that there was nothing else to add, and writing just for increasing the page count was not something we had in mind, and of course, the Packt team supported us in the decision.\nI think that the hardest chapter was the two on exercises\u0026hellip; the book has two knowledge-check-in chapters and once the first was done, thinking about a second without repeating many of the stuff in the first one was not easy.\nJune arrived, all the chapters were delivered and some other reviews were still ongoing on the last chapters, so the book was almost finished and ready for the last steps.\nThe last stone in the road meant little effective work for us as it was mostly adjusting some of the wording and paperwork, but once we got clearance in September (close to three months to complete), we were able to move into the final stage.\nLooking back, it has been a good experience, which somehow was shaking the way I was writing before (as writing Knowledge Base Articles, requires using another styling for the phrasing, etc.), going more direct and engaging more with the reader, but very positive.\nFinally, when everything was done, another member of the Packt team did start over with the book, doing quality assurance, checking the content, clarity, etc, like a second Technical reviewer, before handing it over to the marketing team to work on the promotion of the book on social media.\nWill I do this once again? of course!\n","permalink":"https://iranzo.io/blog/2021/09/15/the-experience-of-writing-a-book/","summary":"\u003cp\u003eI wanted to write about my experience (before I forget about it), and as some colleagues asked about it\u0026hellip; here we go\u0026hellip;\u003c/p\u003e\n\u003cp\u003eAs published in the blog entry \u003ca href=\"/blog/2021/09/11/book-red-hat-enterprise-linux-8-administration/\"\u003eRHEL8 Administration book\u003c/a\u003e, some colleagues and I wrote a book on RHEL8 administration, which can be bought \u003ca href=\"https://s.iranzo.io/rhel8\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMany years ago I started one about Linux, but every time a new paragraph was added, a lot of new \u0026lsquo;TO-DO\u0026rsquo; items were appended as the information growth\u0026hellip; and as it was a \u0026lsquo;solo\u0026rsquo; project, I had other stuff to work on and was parked.\u003c/p\u003e","title":"The experience of writing a book"},{"content":"After some time working on it (about 6 months for the main work and some more time for the reviews) with my colleagues Miguel and Scott, we\u0026rsquo;ve finally made it thanks to the support from our families and Packt, as well as several members of RH teams that gave the clearance to get it out!\nThe book targets users willing to learn skills to administer Red Hat Enterprise Linux or compatible systems. It is a hands-on guide to the administration and can be used as reference thanks to the real-life examples provided along the text.\nIt also features two chapters dedicated to exercises to check your knowledge acquired in the book.\nAvailable at Amazon: Red Hat Enterprise Linux 8 Administration\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/09/11/book-red-hat-enterprise-linux-8-administration/","summary":"\u003cp\u003eAfter some time working on it (about 6 months for the main work and some more time for the reviews) with my colleagues Miguel and Scott, we\u0026rsquo;ve finally made it thanks to the support from our families and Packt, as well as several members of RH teams that gave the clearance to get it out!\u003c/p\u003e\n\u003cp\u003eThe book targets users willing to learn skills to administer Red Hat Enterprise Linux or compatible systems. It is a hands-on guide to the administration and can be used as reference thanks to the real-life examples provided along the text.\u003c/p\u003e","title":"[Book] Red Hat Enterprise Linux 8 Administration"},{"content":"If you\u0026rsquo;re the owner of one of the following sets:\nPorsche 911 GT3 RS 🛒#ad Lego Bugatti Chiron 🛒#ad Lego Lamborghini Sian FKP47 🛒#ad Lego Ferrari Daytona SP3 🛒#ad You might have noticed that each car has a chassis piece with a code, that code, apart of the \u0026rsquo;exclusive\u0026rsquo; set, means that you can redeem it on Lego website to get access to additional content like:\nmobile ringtones wallpapers certificate of ownership In order to redeem them, get your car\u0026rsquo;s detail:\nPorsche\u0026hellip; inside the glove box Bugatti\u0026hellip; inside the trunk Sian\u0026hellip; inside the trunk And go to the following URL\u0026rsquo;s:\nPorsche: https://www.lego.com/es-es/campaigns/technic/codebreaker Bugatti: https://www.lego.com/es-ar/campaigns/technic/bugatti-chiron/codebreaker Sian https://www.lego.com/en-us/campaigns/lamborghini-sian/owners-club Ferrari Daytona SP3 https://www.lego.com/es-es/vip-legotechnicferrari Once your name and code is provided, you\u0026rsquo;ll be redirected to a page with links to each one of the additional files\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/07/25/lego-exclusive-materials-for-technic-owners/","summary":"\u003cp\u003eIf you\u0026rsquo;re the owner of one of the following sets:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.es/dp/B01CCT2ZHC?tag=redken-21\"\u003ePorsche 911 GT3 RS 🛒#ad\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.es/dp/B0792RB3B6?tag=redken-21\"\u003eLego Bugatti Chiron 🛒#ad\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.es/dp/B0813RJRYC?tag=redken-21\"\u003eLego Lamborghini Sian FKP47 🛒#ad\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.es/dp/B09QFSCWD9?tag=rdkn-21\"\u003eLego Ferrari Daytona SP3 🛒#ad\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou might have noticed that each car has a chassis piece with a code, that code, apart of the \u0026rsquo;exclusive\u0026rsquo; set, means that you can redeem it on Lego website to get access to additional content like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emobile ringtones\u003c/li\u003e\n\u003cli\u003ewallpapers\u003c/li\u003e\n\u003cli\u003ecertificate of ownership\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn order to redeem them, get your car\u0026rsquo;s detail:\u003c/p\u003e","title":"Lego exclusive materials for Technic Owners"},{"content":"Until two weeks ago I was using an IMAP server (based on Zimbra) for my work email, but the date for migration to Gmail arrived with no choice to postpone\u0026hellip;\nI was very tied to using my current setup, where:\nofflineimap was downloading all the email to a local maildir folder, imapfilter classified the email into folders based on local options mutt accessed the maildir folder for working with the emails a script to remove duplicate emails from disk imapdedup.py before next sync But, with the change and peculiarities for Gmail, it was no longer working\u0026hellip; I was trying several times with different combinations of folder translations but each one took approximately one day to sync all emails, to find out the next issue with the folder translation.\nAfter this, I tried to just skip using my old setup and switch to web interface\u0026hellip; I was already using it for personal email for years, but with lot of mailing lists and lots of emails, didn\u0026rsquo;t worked the way I wanted.\nI finally went the intermediate way: using imapfilter directly against Gmail and customize a bit the inbox folders.\nThis allowed me to adapt the filtering, so that I can still have a readable INBOX with all the relevant mails and keep at the same time the mails sorted as I used to.\nMy inbox is now configured as multiple inbox with the following filters:\nis:starred (is:unread AND NOT label:Tags/_pending) Those show below my regular INBOX folder, allowing to still feature some emails for tracking, and all the others filtered into folders that are not in the pending sort are also displayed.\nThis is powered with some filters (Gmail side), that keep my inbox clean:\nto:(-myemail1 -myemail2) -\u0026gt; Skip Inbox, Apply label \u0026quot;Tags/_pending\u0026quot; to:(myemail or myemail2) (invite.ics OR invite.vcs) has:attachment -\u0026gt; Delete it from:(bugzilla@redhat.com) -\u0026gt; Skip Inbox, Apply label \u0026quot;Tags/_pending\u0026quot; Now, the imapfilter part via this .imapfilter/config.lua file:\n--------------- -- Options -- --------------- options.timeout = 60 options.subscribe = true options.create = true options.expunge = true ---------------- -- Accounts -- ---------------- -- Connects to \u0026#34;imap1.mail.server\u0026#34;, as user \u0026#34;user1\u0026#34; with \u0026#34;secret1\u0026#34; as -- password. MAILSERVER = IMAP { server = \u0026#39;imap.gmail.com\u0026#39;, username = \u0026#39;MYEMAIL\u0026#39;, password = \u0026#39;MYAPPPASSWORD\u0026#39;, ssl = \u0026#34;tls1\u0026#34; } -- My email myuser = \u0026#39;MYUSERINEMAILS\u0026#39; function mine(messages) email=messages:contain_cc(myuser)+messages:contain_to(myuser)+messages:contain_from(myuser) return email end function filter(messages,email,destination) messages:contain_from(email):move_messages(destination) messages:contain_to(email):move_messages(destination) messages:contain_cc(email):move_messages(destination) messages:contain_field(\u0026#39;sender\u0026#39;, email):move_messages(destination) messages:contain_field(\u0026#39;List-ID\u0026#39;, email):move_messages(destination) end function deleteold(messages,days) todelete=messages:is_older(days)-mine(messages) todelete:move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) end function deleteoldcases(messages,days) todelete=messages:is_older(days) todelete:move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) end function markread(messages) toread=messages:select_all() toread:mark_seen() end -- Define the msgs we\u0026#39;re going to work on -- Move sent messages to INBOX to later sorting -- sent = MAILSERVER[\u0026#39;sent\u0026#39;]:select_all() -- sent:move_messages(MAILSERVER[\u0026#39;INBOX\u0026#39;]) INBOX = MAILSERVER[\u0026#39;INBOX\u0026#39;]:select_all() pending = MAILSERVER[\u0026#39;Tags/_pending\u0026#39;]:select_all() todos = pending + INBOX todos:contain_subject(\u0026#39;Undelivered Mail Returned to Sender\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) todos:contain_subject(\u0026#39;[sbr-stack] Report: OSP - Stale cases: \u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) todos:contain_subject(\u0026#39;[ sbr-stack-emea ] Cron \u0026lt;root@\u0026#39;):contain_from(\u0026#39;(cron Daemon)\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) todos:contain_from(\u0026#39;drive-shares-dm-noreply@google.com\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) todos:contain_subject(\u0026#39;Supplier Remittance Advice- Autogenerated please do not respond to this mail\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/Privado/Gastos\u0026#39;]) filter(todos:is_seen(),\u0026#39;review@openstack.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/OpenStack/gerrit\u0026#39;]) -- Mark as read messages sent from my user markread(todos:contain_from(myuser)) -- Delete google calendar forwards todos:contain_to(\u0026#39;MYUSER@gapps.redhat.com\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) filter(todos:contain_subject(\u0026#39;[PNT] \u0026#39;),\u0026#39;noreply@redhat.com\u0026#39;,MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) -- Filter CPG filter(todos:contain_subject(\u0026#39;Red Hat - Group \u0026#39;),\u0026#39;noreply@redhat.com\u0026#39;,MAILSERVER[\u0026#39;Tags/WORK/Customers/CPG\u0026#39;]) -- Delete messages about New accounts created usercreated=todos:contain_subject(\u0026#39;New Red Hat user account created\u0026#39;)*todos:contain_from(\u0026#39;noreply@redhat.com\u0026#39;) usercreated:move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) -- Search messages from CPG\u0026#39;s cpg = MAILSERVER[\u0026#39;Tags/WORK/Customers/CPG\u0026#39;]:select_all() cpg:contain_subject(\u0026#39;Red Hat - Group - \u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) cpg:contain_subject(\u0026#39;(Unpublished)\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) cpg:contain_subject(\u0026#39;: Where is \u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) -- Move bugzilla messages filter(todos:contain_subject(\u0026#39;] New:\u0026#39;),\u0026#39;bugzilla@redhat.com\u0026#39;,MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/new\u0026#39;]) filter(todos,\u0026#39;bugzilla@redhat.com\u0026#39;,MAILSERVER[\u0026#39;Tags/WORK/_bugzilla\u0026#39;]) bz = MAILSERVER[\u0026#39;Tags/WORK/_bugzilla\u0026#39;]:select_all() -- Move unseen requests or answers reqans=bz:contain_subject(\u0026#39;needinfo requested:\u0026#39;):is_unseen() + bz:contain_subject(\u0026#39;needinfo canceled:\u0026#39;):is_unseen() reqans:move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/reqans\u0026#39;]) reqans=MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/reqans\u0026#39;]:is_seen() reqans:move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla\u0026#39;]) -- Clasify on product bz:contain_field(\u0026#39;X-Bugzilla-Product\u0026#39;, \u0026#39;Red Hat Customer Portal\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/portal\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Product\u0026#39;, \u0026#39;Red Hat Enterprise Linux\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhel\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Product\u0026#39;, \u0026#39;Red Hat Satellite\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhn\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Product\u0026#39;, \u0026#39;Red Hat Enterprise Virtualization Manager\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhev\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Product\u0026#39;, \u0026#39;Fedora\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/fedora\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Product\u0026#39;, \u0026#39;Red Hat Ceph Storage\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/ceph\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Product\u0026#39;, \u0026#39;Red Hat Gluster Storage\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/gluster\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Product\u0026#39;, \u0026#39;Red Hat OpenStack\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp\u0026#39;]) bz = MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp\u0026#39;]:select_all() -- Clasify on component for OSP bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-ceilometer\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/ceilometer\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-cinder\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/cinder\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-designate\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/designate\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-glance\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/glance\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-heat\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/heat\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-ironic\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/ironic\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-keystone\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/keystone\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-neutron\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/neutron\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;openstack-nova\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/nova\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;rhel-osp-director\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/director\u0026#39;]) bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;rhosp-director\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/director\u0026#39;]) bz = MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhev\u0026#39;]:select_all() bz:contain_field(\u0026#39;X-Bugzilla-Component\u0026#39;, \u0026#39;vdsm\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhev/vdsm\u0026#39;]) -- Move support messages once read into the Customer/cases folder filter(todos:contain_subject(\u0026#39;Case \u0026#39;):is_seen(),\u0026#39;support@redhat.com\u0026#39;,MAILSERVER[\u0026#39;Tags/WORK/Customers/cases\u0026#39;]) MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]:select_all():mark_seen() support = MAILSERVER[\u0026#39;Tags/WORK/Customers/cases\u0026#39;]:select_all() -- Restart the search only for messages in Other to also process if we have new rules support:contain_subject(\u0026#39;is about to breach its SLA\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) support:contain_subject(\u0026#39;has breached its SLA\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) support:contain_subject(\u0026#39; has had no activity in \u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) markread(support:contain_subject(\u0026#39;(WoC)\u0026#39;)) markread(support:contain_subject(\u0026#39;(Closed)\u0026#39;)) -- Only work on already read messages support = MAILSERVER[\u0026#39;Tags/WORK/Customers/cases\u0026#39;]:select_all():is_seen() -- Process all remaining messages in INBOX + all read messages in pending-sort for mailing lists and move to lists folder notminelistas=todos-mine(todos) notminelistas:contain_field(\u0026#39;List-ID\u0026#39;,\u0026#39;\u0026lt;\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/Lists\u0026#39;]) filter(notminelistas,\u0026#39;list\u0026#39;, MAILSERVER[\u0026#39;Tags/Lists\u0026#39;]) filter(todos:is_seen(),\u0026#39;list\u0026#39;, MAILSERVER[\u0026#39;Tags/Lists\u0026#39;]) filter(todos:is_seen(),\u0026#39;googlegroups.com\u0026#39;, MAILSERVER[\u0026#39;Tags/Lists\u0026#39;]) filter(todos,\u0026#39;bounces\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists\u0026#39;]) -- Add RH lists, INBOX and _pending and Fedora default bin for reprocessing in case a new list has been added lists = todos:is_seen() + MAILSERVER[\u0026#39;Tags/Lists\u0026#39;]:select_all() + MAILSERVER[\u0026#39;Tags/Lists/Fedora\u0026#39;]:select_all() todos:contain_subject(\u0026#39;unsubcribe\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) lists:contain_subject(\u0026#39;unsubcribe\u0026#39;):move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) -- Mailing lists filter(INBOX:is_seen(),\u0026#39;notifications@github.com\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/WORK/Tech/coderepos\u0026#39;]) filter(lists,\u0026#39;kubevirt-dev@googlegroups.com\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/WORK/SysEng/CNV/kubevirt-dev\u0026#39;]) filter(lists,\u0026#39;metal3-dev@googlegroups.com\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/WORK/SysEng/CNV/metalkube-dev\u0026#39;]) -- Fedora filter(lists,\u0026#39;kickstart-list\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/Fedora/kickstart\u0026#39;]) filter(lists,\u0026#39;ambassadors@lists.fedoraproject.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/Fedora/Ambassador\u0026#39;]) filter(lists,\u0026#39;infrastructure@lists.fedoraproject.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/Fedora/infra\u0026#39;]) filter(lists,\u0026#39;announce@lists.fedoraproject.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/Fedora/announce\u0026#39;]) filter(lists,\u0026#39;lists.fedoraproject.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/Fedora\u0026#39;]) -- OSP filter(lists,\u0026#39;openstack@lists.openstack.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/OpenStack\u0026#39;]) filter(lists,\u0026#39;openstack-operators@lists.openstack.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/OpenStack/Operators\u0026#39;]) filter(lists,\u0026#39;openstack-es@lists.openstack.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/OpenStack/es\u0026#39;]) filter(lists,\u0026#39;rdo-list@redhat.com\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/OpenStack/rdo\u0026#39;]) filter(lists,\u0026#39;@bugs.launchpad.net\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/OpenStack/launchpad\u0026#39;]) lists:contain_field(\u0026#39;X-Launchpad-Notification-Type\u0026#39;, \u0026#39;bug\u0026#39;):move_messages(MAILSERVER[\u0026#39;Tags/Lists/OpenStack/launchpad\u0026#39;]) filter(lists,\u0026#39;pgsql-hackers@lists.postgresql.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/Others/pgsql-hackers\u0026#39;]) filter(lists,\u0026#39;pgsql-hackers@postgresql.org\u0026#39;,MAILSERVER[\u0026#39;Tags/Lists/Others/pgsql-hackers\u0026#39;]) -- Filter messages not filtered back to INBOX pending:move_messages(MAILSERVER[\u0026#39;INBOX\u0026#39;]) -- Start processing of messages older than: maxage=365 -- Delete old messages from mailing lists maxage=180 deleteold(MAILSERVER[\u0026#39;Tags/Lists\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/Fedora\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/Fedora/Ambassador\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/Fedora/announce\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/Fedora/infra\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/Fedora/kickstart\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/OpenStack\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/OpenStack/es\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/Others/pgsql-hackers\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/WORK/SysEng/CNV/kubevirt-dev\u0026#39;],maxage) deleteold(MAILSERVER[\u0026#39;Tags/Lists/WORK/SysEng/CNV/metalkube-dev\u0026#39;],maxage) -- Delete old BZ tickets maxage=30 deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/ceilometer\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/cinder\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/designate\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/glance\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/heat\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/ironic\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/keystone\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/neutron\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhosp/nova\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/ceph\u0026#39;],maxage) maxage=30 deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/portal\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhel\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/rhn\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/fedora\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/gluster\u0026#39;],maxage) deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/_bugzilla/security\u0026#39;],maxage) -- delete old cases maxage=30 -- for each in $(cat .imapfilter/config.lua|grep -i cases|tr \u0026#34; ,()\u0026#34; \u0026#34;\\n\u0026#34;|grep cases|sort|uniq|grep -v \u0026#34;:\u0026#34; );do echo \u0026#34;deleteoldcases($each,maxage)\u0026#34;;done deleteoldcases(MAILSERVER[\u0026#39;Tags/WORK/Customers/cases\u0026#39;],maxage) -- Empty trash every 7 days maxage=7 old=MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]:is_older(maxage) old:move_messages(MAILSERVER[\u0026#39;[Gmail]/Papelera\u0026#39;]) I think that is more or less self-explanatory, but in short, first sorts mails into folders based on the sender, recipient, keywords and finally, applies expiration policy to the folders to remove old emails that might not be relevant anymore.\nIn this case, it also keeps messages in which I was directly involved (removing, via the function to delete messages that are in the variable \u0026lsquo;mine\u0026rsquo;)\nDuring the next days I\u0026rsquo;ll try to get back email in mutt, but this at least makes the usage in the meantime more bearable\u0026hellip;\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/07/07/imapfilter-for-gmail/","summary":"\u003cp\u003eUntil two weeks ago I was using an IMAP server (based on Zimbra) for my work email, but the date for migration to Gmail arrived with no choice to postpone\u0026hellip;\u003c/p\u003e\n\u003cp\u003eI was very tied to using my current setup, where:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eofflineimap\u003c/code\u003e was downloading all the email to a local \u003ccode\u003emaildir\u003c/code\u003e folder,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimapfilter\u003c/code\u003e classified the email into folders based on local options\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emutt\u003c/code\u003e accessed the \u003ccode\u003emaildir\u003c/code\u003e folder for working with the emails\u003c/li\u003e\n\u003cli\u003ea script to remove duplicate emails from disk \u003ccode\u003eimapdedup.py\u003c/code\u003e before next sync\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBut, with the change and peculiarities for Gmail, it was no longer working\u0026hellip; I was trying several times with different combinations of folder translations but each one took approximately one day to sync all emails, to find out the next issue with the folder translation.\u003c/p\u003e","title":"imapfilter for Gmail"},{"content":"Hi,\nIn case you\u0026rsquo;ve a dual boot machine, sometimes it might happen that grub menu is no longer appearing.\nFor systems using regular BIOS, a grub-install against the device it was installed might be required, but when using UEFI, it\u0026rsquo;s really easy to use a rescue media and execute efibootmgr to alter the boot order.\nWhen executing efibootmgr, it might output some information like this:\nBootCurrent: 0001 Timeout: 0 seconds BootOrder: 0001,0019,001D,001C,0017,0018,001A,001B,001E,001F,0020,0000 Boot0000* Windows Boot Manager Boot0001* Fedora Boot0010 Setup Boot0011 Boot Menu Boot0012 Diagnostic Splash Screen Boot0013 Lenovo Diagnostics Boot0014 Startup Interrupt Menu Boot0015 Rescue and Recovery Boot0016 MEBx Hot Key Boot0017* USB CD Boot0018* USB FDD Boot0019* NVMe0 Boot001A* NVMe1 Boot001B* ATA HDD2 Boot001C* ATA HDD3 Boot001D* ATA HDD0 Boot001E* ATA HDD1 Boot001F* USB HDD Boot0020* PCI LAN Boot0021* IDER BOOT CDROM Boot0022* IDER BOOT Floppy Boot0023* ATA HDD Boot0024* ATAPI CD Note there, the BootCurrent and the BootOrder, the numbers in the BootOrder correspond to the Boot#### that are listed below it.\nOnce we\u0026rsquo;re sure about the boot order we want (for example, to restore booting into grub), execute:\nefibootmgr -o 0001,0000,0010,0011 But, choosing the right order you want for your system.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/07/01/uefi-boot-order-change/","summary":"\u003cp\u003eHi,\u003c/p\u003e\n\u003cp\u003eIn case you\u0026rsquo;ve a dual boot machine, sometimes it might happen that \u003ccode\u003egrub\u003c/code\u003e menu is no longer appearing.\u003c/p\u003e\n\u003cp\u003eFor systems using regular BIOS, a \u003ccode\u003egrub-install\u003c/code\u003e against the device it was installed might be required, but when using UEFI, it\u0026rsquo;s really easy to use a rescue media and execute \u003ccode\u003eefibootmgr\u003c/code\u003e to alter the boot order.\u003c/p\u003e\n\u003cp\u003eWhen executing \u003ccode\u003eefibootmgr\u003c/code\u003e, it might output some information like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBootCurrent: \u003cspan style=\"color:#ae81ff\"\u003e0001\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eTimeout: \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e seconds\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBootOrder: 0001,0019,001D,001C,0017,0018,001A,001B,001E,001F,0020,0000\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0000* Windows Boot Manager\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0001* Fedora\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0010  Setup\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0011  Boot Menu\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0012  Diagnostic Splash Screen\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0013  Lenovo Diagnostics\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0014  Startup Interrupt Menu\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0015  Rescue and Recovery\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0016  MEBx Hot Key\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0017* USB CD\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0018* USB FDD\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0019* NVMe0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot001A* NVMe1\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot001B* ATA HDD2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot001C* ATA HDD3\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot001D* ATA HDD0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot001E* ATA HDD1\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot001F* USB HDD\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0020* PCI LAN\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0021* IDER BOOT CDROM\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0022* IDER BOOT Floppy\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0023* ATA HDD\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eBoot0024* ATAPI CD\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote there, the \u003ccode\u003eBootCurrent\u003c/code\u003e and the \u003ccode\u003eBootOrder\u003c/code\u003e, the numbers in the \u003ccode\u003eBootOrder\u003c/code\u003e correspond to the \u003ccode\u003eBoot####\u003c/code\u003e that are listed below it.\u003c/p\u003e","title":"UEFI boot order change"},{"content":"For some of the telegram groups I\u0026rsquo;m in, I\u0026rsquo;ve been detecting users that after some period of time, just publish spam messages of any topic.\nThere are many bots for controlling when a user joins, by showing a CAPTCHA that user must resolve (either clicking a button, answering a mathematical operation, inputting a CAPTCHA image text, etc).\nSome time ago, a colleague was using Machine Learning and I wanted to have a look at it and it would make a good feature to implement.\nFirst thing I wanted, was to get rid of the spammers, so the first approach was to include a new command on redken_bot to mark with /spam when replying to a message to take some actions.\nThe /spam command also required some protection from abuse, so it should only work for admins.\nThe admin detection In the beginning some of the commands added to redken_bot had admin access that required the user to define the list of administrators via the admin configuration variable\u0026hellip; but no one did.\nWith some changes in the telegram BOT API, the bot can get (when added as one of the admins in your group) the membership permission updates, so when a membership update arrives (new user added as admin or admin user becoming regular user), the bot will call the function to refresh the list of administrators and use it to update the admin variable automatically.\nThis required changing the way calls were made to get new telegram updates, but I enabled all the possible types of messages, as well as rewriting the function processing the data out of each message, but was a good improvement (even if invisible for outside users.)\nSpam actions Once the admin detection was solved and running (bot currently is member of 549 groups and has 28252 unique users, and only 20 groups have the admin variable set), the next step was to work on the spam actions.\nMany times I was manually doing the stuff:\nDeleting message Kicking user and reporting as spam Sometimes even noting user UID to add to a blocklist So I decided to create a new command /spam which automates part of the job:\nDeletes message Stores UID in the database as spammer Kicks user out of the chat Buttons for easier usage It would be great to have automatic detection, and easier reporting, so next step was playing a bit with buttons.\nTo be honest, never used them except for some attempts, but as I was playing with updated messages for the admin stuff, so was worth to use the feature that was there to add extra parameters to the call.\nyes = \u0026#34;Yes\u0026#34; no = \u0026#34;No\u0026#34; ignore = \u0026#34;Ignore\u0026#34; isthisspammsg = \u0026#34;Is this spam?\u0026#34; extra = ( \u0026#39;reply_markup={\u0026#34;inline_keyboard\u0026#34;:[[{\u0026#34;text\u0026#34;:\u0026#34;%s\u0026#34;,\u0026#34;callback_data\u0026#34;:\u0026#34;SPAM\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;%s\u0026#34;,\u0026#34;callback_data\u0026#34;:\u0026#34;HAM\u0026#34;},{\u0026#34;text\u0026#34;:\u0026#34;%s\u0026#34;,\u0026#34;callback_data\u0026#34;:\u0026#34;IGNORE\u0026#34;}]]}\u0026#39; % (yes, no, ignore) ) With above approach, the bot could reply to messages and attach an inline-keyboard with configurable buttons, returning as part of the callback data the message I wanted (HAM,SPAM or IGNORE).\nThis, also required to process the callback_data that we were now collecting since the changes added for the admin status change.\nThe good thing is that the answer provided when pressing the button, contains reference to the original message, so it was easier to later catch the text we were replying with the buttons.\nMachine learning Machine learning more or less, is showing data to an algorithm with results tagged in one or either way, divide the set of data between a training group and a test group and feed it to the algorithm to find how good it has been.\nFor doing so, it converts the input data into numbers and tries to find relationships between them. This also opens the pandora box as it has lot of different approaches, depending on the function being used for doing the conversion, for example, some of them use frequency of elements, removing the less frequent, etc\nFinally, using the model the program can classify new data, based on the model that was elaborated, and in order to improve it, the data should be refreshed with new patterns for both cases: ham and spam so that it can continue evolving.\nThere are lot of documents about this, the shortest example I can think of is:\nfrom langdetect import detect from nltk.corpus import stopwords from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer from sklearn.metrics import classification_report, confusion_matrix, accuracy_score from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline import logging import nltk import os import pandas as pd import pickle import dill import string import sys logger = logging.getLogger(__name__) logger.debug(\u0026#34;Downloading updated stopwords\u0026#34;) nltk.download(\u0026#34;stopwords\u0026#34;) logger.debug(\u0026#34;Starting training with on-disk databases\u0026#34;) message = pd.read_csv( \u0026#34;spam/spam.%s.csv\u0026#34; % language, sep=\u0026#34;,\u0026#34;, names=[\u0026#34;Category\u0026#34;, \u0026#34;Message\u0026#34;] ) # Drop duplicate messages message.drop_duplicates(inplace=True) # Split the messages between train and test msg_train, msg_test, label_train, label_test = train_test_split( message[\u0026#34;Message\u0026#34;], message[\u0026#34;Category\u0026#34;], test_size=0.2 ) # Define the function for our language pipeline = Pipeline( [ ( \u0026#34;bow\u0026#34;, CountVectorizer( analyzer=lambda text: process_text(text, language=languages[language]) ), ), (\u0026#34;tfidf\u0026#34;, TfidfTransformer()), (\u0026#34;classifier\u0026#34;, MultinomialNB()), ] ) logger.debug(\u0026#34;Training the algorithm for language: %s\u0026#34; % language) pipeline.fit(msg_train, label_train) # Evaluate the model on the training data set print(\u0026#34;Testing the training for language: %s\u0026#34; % language) pred = pipeline.predict(msg_train) logger.debug(\u0026#34;Accuracy with train data\u0026#34;) logger.debug(\u0026#34;%s\u0026#34; % classification_report(label_train, pred)) logger.debug(\u0026#34;Confusion Matrix:\\n%s\u0026#34; % confusion_matrix(label_train, pred)) logger.debug(\u0026#34;Accuracy: %s\u0026#34; % accuracy_score(label_train, pred)) pred = pipeline.predict(msg_test) logger.debug(\u0026#34;Accuracy with test data\u0026#34;) logger.debug(\u0026#34;%s\u0026#34; % classification_report(label_test, pred)) logger.debug(\u0026#34;Confusion Matrix:\\n%s\u0026#34; % confusion_matrix(label_test, pred)) accuracy = accuracy_score(label_test, pred) logger.debug(\u0026#34;Accuracy: %s\u0026#34; % accuracy) In short, this loads a know list of messages with spam and ham and divides it to train the model (via the pipeline) and later to test on the test data to check accuracy.\nFor doing so, it also downloads stopwords, which allows to remove junctions from phrases that are usually less meaningful of the message data itself. The process_text function defined in the pipeline is the one that we should write and takes care of removing punctuation, stopwords, etc.\nOf course, training takes a while so it\u0026rsquo;s not something you\u0026rsquo;ll be doing in realtime, but there is where the dill library (that worked better for my use case than pickle) helped me\u0026hellip; Bot trains every day for new language model, stores it on disk, and later is able to restore from disk and directly use it.\nFor saving and restoring we can use something like this:\n# Save trained pipeline with open(pkl_filename, \u0026#34;wb\u0026#34;) as file: dill.dump(pipeline, file) # Restore trained pipeline with open(pkl_filename, \u0026#34;rb\u0026#34;) as file: pipeline = dill.load(file) So, here we\u0026rsquo;ve already all the pieces\u0026hellip;\nWe can train a pipeline with our database of spam We can save and restore a pipeline so that we can do the hard work in easy moments and still have fast results when using it We can report messages as spam manually and perform actions The last piece of glue, was extending the /spam command to also store the message received and marked as spam into the database.\nWith this approach, the bot is able to work on the current existing database to generate a model, and still allow to grow the database with admin-reported spam messages. Those messages will then be used to train the pipeline periodically either as training or test, helping improving and enhancing the detection\nSo the actual behavior is that @redken_bot does:\nChecks new message if a model for that language exists (only saves it when accuracy is 85% or higher) If it\u0026rsquo;s spam, It will show a keyboard to either mark as spam (confirm) or ignore the message Admins can reply with /spam to messages, even if there\u0026rsquo;s no model for that language, helping in creating the database of messages If a message has been marked as spam, either by replying with /spam or by clicking on the button saying that it\u0026rsquo;s spam, the spam process begins: The button is always removed when replying (with the question about spam status) If the message was marked as spam, the original message is removed The user that sent the message, gets added to the database as spammer and then kicked out of the chat The message is stored on the database for future enhancement of the machine language detection. Additionally, I\u0026rsquo;m testing a \u0026lsquo;ham\u0026rsquo; training feature, being fed with regular messages to start building a positive set of messages to compare with.\nI will continue searching for spam databases in other languages to do an initial set, but in the meantime, it will continue only with English.\nNext steps are:\nPromote messages marked as spam on groups into the general list (right now, messages marked as spam will be only stored, but no other work done with them) Once the message is promoted, it would be extracted from the database and put in an external CSV file similar to the SMS collection for the relevant language for future training. Use the list of blocked UID with status global, to warn in groups where that user is in, showing the chance to kick the user. spamcheck set to auto, automatically deletes the spam messages and reports the user, together with previous item, it will also auto-kick users Enjoy and happy filtering!\n","permalink":"https://iranzo.io/blog/2021/06/24/redken-machine-learning-for-spam-detection/","summary":"\u003cp\u003eFor some of the telegram groups I\u0026rsquo;m in, I\u0026rsquo;ve been detecting users that after some period of time, just publish spam messages of any topic.\u003c/p\u003e\n\u003cp\u003eThere are many bots for controlling when a user joins, by showing a CAPTCHA that user must resolve (either clicking a button, answering a mathematical operation, inputting a CAPTCHA image text, etc).\u003c/p\u003e\n\u003cp\u003eSome time ago, a colleague was using Machine Learning and I wanted to have a look at it and it would make a good feature to implement.\u003c/p\u003e","title":"Redken machine learning for spam detection"},{"content":"Some years ago I started using geo replication to keep a copy of all the pictures, docs, etc\nAfter being using BitTorrent sync and later resilio sync (even if I didn\u0026rsquo;t fully liked the idea of it being not open source), I gave up. My NAS with 16 GB of ram, even if a bit older (HP N54L), seemed not to have enough memory to run it, and was constantly swapping.\nChecking the list of processes pointed to the rslsync process as the culprit, and apparently the cause is the way it handles the files it controls.\nThe problem is that even one file is deleted long ago, rslsync does keep it in the database\u0026hellip; and in memory. After checking with their support (as I had a family license), the workaround was to remove the folder and create a new one, which in parallel meant having to configure it again on all the systems that used for keeping a copy.\nI finally decide to give syncthing another try after years since last evaluation.\nSyncthing is now is covering some of the features I was using with rslsync:\nMulti-master replication Remote encrypted peers Read only peers Multiple folder support In addition, it includes systemd support and it\u0026rsquo;s packaged in the operating system, making it really easy to install and update (´rslsync´ was without updates for almost a year).\nOnly caveat, if using Debian, is to use the repository they provide as the package included in the distribution is really old, causing some issues with the remote encrypted peers.\nFor starting as user the command is very simple:\nsystemctl enable syncthing@user systemctl start syncthing@user Once the process is started, the browser can be pointed locally at http://127.0.0.1:8384 to start configuration:\nIt is recommended to define a GUI username and password for avoiding other users with access to the system from altering the configuration. Once done, we\u0026rsquo;re ready to start adding folders and systems. One difference is that in rslsync having the secret for the key is enough, in syncthing you need to add the hosts in both ways to accept them and be able to share data.\nOne easing feature here is that one host can be configured as presenter which allows other systems to inherit the know list of hosts from the host marked as presenter, making it easier to do the both-ways initial introduction.\nBest outcome, is that the use (or abuse) of RAM has been completely slashed what rslsync was using.\nCurrently, the only issue is that for some computers in the local network the sync was a bit slow (it even got some remote underpowered devices syncing faster than local ones), but some of the copies were fully in synced already.\nHowever, this issue was fixed when updating all the systems to v1.17.0 which included some improvements to the encrypted folders, making all my systems to be perfectly in sync! Bear also in mind that the provided version by the operating system, like the one in Raspbian is really old, and was causing issues with encrypted folders with the systems in fedora\u0026hellip; once all of them were updated, it worked fine (there were messages about encrypted folder being announced but not configured when the Raspbian version was 1.0.0).\nThe web interface is not bad, even if, for what I was used to, it\u0026rsquo;s not showing as much detail about the hosts status at glance, having to open each individual folder to see how it is going, as in the general view, it shows the percentage of completion and the amount of data still missing to be synced.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/06/12/geo-replication-with-syncthing/","summary":"\u003cp\u003eSome years ago I started using geo replication to keep a copy of all the pictures, docs, etc\u003c/p\u003e\n\u003cp\u003eAfter being using BitTorrent sync and later resilio sync (even if I didn\u0026rsquo;t fully liked the idea of it being not open source), I gave up. My NAS with 16 GB of ram, even if a bit older (HP N54L), seemed not to have enough memory to run it, and was constantly swapping.\u003c/p\u003e","title":"Geo replication with syncthing"},{"content":"I was improving a playbook in Ansible and wanted to find a way to find if a system was virtual or not to decide about some tunning like setting tuned-adm profile virtual-guest or disable the power off when the lid is closed.\nAfter some research and try-except situations I got to this one that seemed to work (I had to tune it as one desktop machine was missing the /sys entry I was using before):\n--- - hosts: all user: root tasks: - name: Check if platform is Virtual lineinfile: dest: /sys/devices/virtual/dmi/id/sys_vendor line: \u0026#34;QEMU\u0026#34; check_mode: yes register: virtual failed_when: (virtual is changed) or (virtual is failed) ignore_errors: true - name: Check if platform is Physical set_fact: physical: true virtual: false when: virtual is changed - name: Set fact for Virtual set_fact: physical: false virtual: true when: virtual - name: Report system is virtual debug: msg: this is virtual when: virtual - name: Report system is physical debug: msg: This is physical when: physical - name: Get system Chassis shell: hostnamectl status | grep Chassis | cut -f2 -d \u0026#34;:\u0026#34; | tr -d \u0026#39; \u0026#39; register: chassis This playbook tasks check the sys_vendor for QEMU which worked for both systems on real KVM and on some other like Oracle Cloud that provided other values. It uses th lineinfile module that is usually used with a regexp to find a proper value and replace with the one we\u0026rsquo;re interested in, like in this example I use for setting logrotate.conf settings:\n- name: Configure logrotate.conf lineinfile: dest: /etc/logrotate.conf create: true state: present regexp: \u0026#34;{{ item.regexp }}\u0026#34; line: \u0026#34;{{ item.line }}\u0026#34; with_items: - { regexp: \u0026#34;^compress\u0026#34;, line: \u0026#34;compress\u0026#34; } - { regexp: \u0026#34;^rotate.*\u0026#34;, line: \u0026#34;rotate 14\u0026#34; } - { regexp: \u0026#34;^daily\u0026#34;, line: \u0026#34;daily\u0026#34; } - { regexp: \u0026#34;^weekly.*\u0026#34;, line: \u0026#34;\u0026#34; } - { regexp: \u0026#34;^dateext.*\u0026#34;, line: \u0026#34;\u0026#34; } With the first example, we use it in check_mode and use it to setup virtual variable and later we use that to define facts for virtual and physical physical , so that we can decide to use it later in our playbooks like this:\n- name: Set tuned profile for VM\u0026#39;s shell: /usr/sbin/tuned-adm profile virtual-guest when: virtual - name: Configure systemd for ignoring closed lid on power ini_file: path: /etc/systemd/logind.conf section: Login option: HandleLidSwitchExternalPower value: ignore when: physical and chassis == \u0026#39;laptop\u0026#39; - name: Configure systemd for ignoring closed lid on Docked ini_file: path: /etc/systemd/logind.conf section: Login option: HandleLidSwitchDocked value: ignore when: physical and chassis == \u0026#39;laptop\u0026#39; Of course, this could also be extended to check if system is really a laptop or different kind of system to enable some other specific tunning, but for some initial tasks, it will do the trick.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/05/10/how-to-check-if-a-system-is-virtual/","summary":"\u003cp\u003eI was improving a playbook in Ansible and wanted to find a way to find if a system was virtual or not to decide about some tunning like setting \u003ccode\u003etuned-adm profile virtual-guest\u003c/code\u003e or disable the power off when the lid is closed.\u003c/p\u003e\n\u003cp\u003eAfter some research and try-except situations I got to this one that seemed to work (I had to tune it as one desktop machine was missing the /sys entry I was using before):\u003c/p\u003e","title":"How to check if a system is virtual"},{"content":"As said in the article about mixnodes, NYM is a technology aiming for providing privacy for the communications.\nApart of the mixnodes, other key piece in the infrastructure are the validators.\nAs said, the project uses Open Source technology to run, and they have a nice docs with details on how to run a node at https://nymtech.net/docs/, and the one relevant for mixnodes at https://nymtech.net/docs/run-nym-nodes/validators/.\nIn this case, we can follow the instructions for compiling, but I faced some issues (compiling went fine, but initial sync failed), so in this case, we will use the pre-compiled version provided with the 0.10.0 release.\nLet\u0026rsquo;s now clone the repository:\ngit clone https://github.com/nymtech/nym.git cd nym git checkout tags/v0.10.0 The binaries we\u0026rsquo;re interested are inside the validator folder, and two of them are important:\nlibwasmvm.so nymd The official guide, already provides enough information about creating a systemd unit file, setting the LD_LIBRARY_PATH environment variable in our .bashrc, etc. So we will use them after installing the required packages:\ndnf -y install certbot nginx systemctl enable nginx systemctl start nginx Those packages will enable our system to serve secure web pages using a domain name validated with let\u0026rsquo;s encrypt.\nPay special attention to the required steps:\nInitialize the validator as described using nymd init $SERVER --chain-id testnet-finney Run wget -O $HOME/.nymd/config/genesis.json https://nymtech.net/testnets/finney/genesis.json to overwrite the created file with the one for finney release. Edit the $HOME/.nymd/config/config.toml file as described (persistent_peers, cors_allowed_origins and create_empty_blocks) Edit the $HOME/.nymd/config/app.toml to set the proper values for minimum-gas-prices and enabling [API] Once this is performed, initialize an user, and remember the key that you typed and of course, store the mnemonic properly.\nFollow the steps on the guide for setting the systemd service so that the process starts automatically after each reboot:\nsystemctl enable nymd systemctl start nymd After a while, with the process started, you can create the validator using the command at the documentation by creating a transaction and staking (you\u0026rsquo;ll need tokens for that, and the program will ask your confirmation and password before signing and broadcasting the request).\nBefore it, remember to open the firewall ports:\nfor port in 1317/tcp 9090/tcp 26656/tcp; do firewall-cmd --add-port=${port} firewall-cmd --add-port=${port} --permanent done Once it\u0026rsquo;s finished, you\u0026rsquo;re ready to run the validator as instructed in the official guide.\nClaiming rewards Once the remaining steps for setting it up have been followed, and the validator has been running for a while, you can check the obtained rewards:\nnymd query distribution validator-outstanding-rewards halvaloper\u0026lt;...the address you get when \u0026#34;nymd keys show default --bech=val\u0026#34;...\u0026gt; Using the values obtained from previous command, you can withdraw all rewards with:\nnymd tx distribution withdraw-rewards halvaloper\u0026lt;...the address you get when \u0026#34;nymd keys show default --bech=val\u0026#34;...\u0026gt; --from nym-admin --keyring-backend=os --chain-id=\u0026#34;testnet-finney\u0026#34; --gas=\u0026#34;auto\u0026#34; --gas-adjustment=1.15 --commission --fees 5000uhal If you want to check your current balances, check them with:\n~/.nymd/nymd query bank balances hal\u0026lt;address\u0026gt; For example:\nbalances: - amount: \u0026#34;22976200\u0026#34; denom: stake - amount: \u0026#34;919376\u0026#34; denom: uhal pagination: next_key: null total: \u0026#34;0\u0026#34; You can, of course, stake back the available balance to your validator with the following command:\nnymd tx staking delegate halvaloper\u0026lt;...the address you get when \u0026#34;nymd keys show nym-admin --bech=val\u0026#34;...\u0026gt; \u0026lt;amount\u0026gt;stake --from nym-admin --keyring-backend=os --chain-id \u0026#34;testnet-finney\u0026#34; --gas=\u0026#34;auto\u0026#34; --gas-adjustment=1.15 --fees 5000uhal Note\nThe value to be used instead of the \u0026lt;amount\u0026gt;stake can be calculated from the available balance. For example, if you\u0026rsquo;ve 999989990556 in the balance, you can stake 999909990556, note that the 5th digit, has been changed from 8 to 0 to leave some room for fees (amounts are multiplied by 10^6).\nRemember to replace halvaloper with your validator address and nym-admin with the user you created during initialization.\nAdditionally you can also fix some of the data provided for your validator with:\nnymd tx staking edit-validator --chain-id=testnet-finney --moniker=\u0026lt;mymoniker\u0026gt; --details=\u0026#34;Nym validator\u0026#34; --security-contact=\u0026#34;YOUREMAIL\u0026#34; --identity=\u0026#34;XXXXXXX\u0026#34; --gas=\u0026#34;auto\u0026#34; --gas-adjustment=1.15 --from=nym-admin --fees 2000uhal With above command you can specify the gpg key last numbers (as used in keybase) as well as validator details and your email for security contact\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/05/09/how-to-run-a-nym-validator/","summary":"\u003cp\u003eAs said in the article about \u003ca href=\"/blog/2021/05/09/how-to-run-a-nym-mixnode/\"\u003emixnodes\u003c/a\u003e, NYM is a technology aiming for providing privacy for the communications.\u003c/p\u003e\n\u003cp\u003eApart of the mixnodes, other key piece in the infrastructure are the validators.\u003c/p\u003e\n\u003cp\u003eAs said, the project uses Open Source technology to run, and they have a nice docs with details on how to run a node at \u003ca href=\"https://nymtech.net/docs/\"\u003ehttps://nymtech.net/docs/\u003c/a\u003e, and the one relevant for mixnodes at \u003ca href=\"https://nymtech.net/docs/run-nym-nodes/validators/\"\u003ehttps://nymtech.net/docs/run-nym-nodes/validators/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn this case, we can follow the instructions for compiling, but I faced some issues (compiling went fine, but initial sync failed), so in this case, we will use the pre-compiled version provided with the \u003ccode\u003e0.10.0\u003c/code\u003e release.\u003c/p\u003e","title":"How to run a NYM Validator"},{"content":"Some time ago I\u0026rsquo;ve started running a NYM mixnode. NYM is a project that targets improving privacy by decomposing network packages from different hosts, so that origin and target cannot be traced.\nYou can check more about the NYM project at their site at https://nymtech.net/.\nThe project uses Open Source technology to run, and they have a nice docs with details on how to run a node at https://nymtech.net/docs/, and the one relevant for mixnodes at https://nymtech.net/docs/run-nym-nodes/mixnodes/.\nBut first, we need to compile it (as described in https://nymtech.net/docs/run-nym-nodes/build-nym/).\nThose instructions are mostly adapted to Debian hosts, but it\u0026rsquo;s not that different to build on RHEL, CentOS or Fedora, so let\u0026rsquo;s explore how in the next steps (Note: this is based on Fedora 34 Server installation, feel free to adapt to your distribution of choice and required prerequisites on repositories, etc.)\nWe will need some packages to be installed for developing and compiling:\ndnf -y install curl jq cargo git openssl-devel Let\u0026rsquo;s now clone the repository:\ngit clone https://github.com/nymtech/nym.git cd nym git checkout tags/v0.10.0 And let\u0026rsquo;s proceed to compile the code via:\ncargo build --release Once it\u0026rsquo;s finished, you\u0026rsquo;re ready to run the mixnode.\nNote\nThe compiled files will be now inside the ./target/release/ folder, so you\u0026rsquo;re ready to continue with the official guide at https://nymtech.net/docs/run-nym-nodes/mixnodes/, just remember to run cd target/release before, so that it will find the commands as described in the official guide.\nIf you want to see this guide in Asciinema check this:\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/05/09/how-to-run-a-nym-mixnode/","summary":"\u003cp\u003eSome time ago I\u0026rsquo;ve started running a NYM mixnode. \u003ccode\u003eNYM\u003c/code\u003e is a project that targets improving privacy by decomposing network packages from different hosts, so that origin and target cannot be traced.\u003c/p\u003e\n\u003cp\u003eYou can check more about the NYM project at their site at \u003ca href=\"https://nymtech.net/\"\u003ehttps://nymtech.net/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe project uses Open Source technology to run, and they have a nice docs with details on how to run a node at \u003ca href=\"https://nymtech.net/docs/\"\u003ehttps://nymtech.net/docs/\u003c/a\u003e, and the one relevant for mixnodes at \u003ca href=\"https://nymtech.net/docs/run-nym-nodes/mixnodes/\"\u003ehttps://nymtech.net/docs/run-nym-nodes/mixnodes/\u003c/a\u003e.\u003c/p\u003e","title":"How to run a NYM mixnode"},{"content":"Xmas came with a good gift that I was able to finish mounting today, the Lego Lamborghini Sian FKP47 🛒#ad.\nThe build experience was good, except for some issues related with not building in the best conditions of light, but more or less fixable\u0026hellip;\nI forgot to add a gear that required me to bend a bit the model to put it back Realized that the gear shift wasn\u0026rsquo;t working because I didn\u0026rsquo;t put the piece in the right direction, so it was being blocked between two options Pick the wrong piece at one step so later on it was missing for another The biggest issue was that for the 2nd issue, I got into the motor drive had to be resettled and included some more dismantling, but nothing impossible.\nSize is very similar to the Porsche and Bugatti) and had interesting build techniques for the frontal, the doors, etc.\nHere you have some pictures of it:\nAnd with it\u0026rsquo;s road friends!\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/01/15/lego-technic-42115-lamborghini-sian-fkp47/","summary":"\u003cp\u003eXmas came with a good gift that I was able to finish mounting today, the \u003ca href=\"https://www.amazon.es/dp/B0813RJRYC?tag=redken-21\"\u003eLego Lamborghini Sian FKP47 🛒#ad\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe build experience was good, except for some issues related with not building in the best conditions of light, but more or less fixable\u0026hellip;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI forgot to add a gear that required me to bend a bit the model to put it back\u003c/li\u003e\n\u003cli\u003eRealized that the gear shift wasn\u0026rsquo;t working because I didn\u0026rsquo;t put the piece in the right direction, so it was being blocked between two options\u003c/li\u003e\n\u003cli\u003ePick the wrong piece at one step so later on it was missing for another\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe biggest issue was that for the 2nd issue, I got into the motor drive had to be resettled and included some more dismantling, but nothing impossible.\u003c/p\u003e","title":"Lego Technic 42115 Lamborghini Sian FKP47"},{"content":"Be lazy, automate: GitHub Actions for static blogging /me: Pablo Iranzo Gómez ( https://iranzo.io )\nWhat is a blog? A place to share knowledge, interests, tips, etc.\nUsually features:\nimages comments from visitors, related articles, etc. What are the costs for a blog? Web costs money:\nHosting Domain Maintenance etc. What is static blogging? Generate a static webpage\nThink of it as rendering templates into HTML Has no requirements on the web server, any simple Webserver is enough: Look ma!, no database! Look ma!, no users! Look ma!, no security issues! What does it mean to us? We write an article Command for generating html from templates is used New files uploaded to Webserver Some Philosophy Empty your mind, be shapeless, formless, like water. Now you put water in a cup, it becomes the cup, you put water into a bottle, it becomes the bottle You put water in a teacup, it becomes the teapot Now water can flow or it can crash. Be water my friend\nNote: Automation: Be lazy, have someone else doing it for you.\nGit Hub / Gitlab Lot of source code is hosted at GitHub, Gitlab or other services, but it\u0026rsquo;s a code repository. BUT: We want a website!! Pages come to play Git Hub provides a service called GitHub Pages Git lab provides Gitlab pages Both provide a \u0026lsquo;static\u0026rsquo; Webserver to be used for your projects for free 😜\nG(H/L) serve from a branch in your repo (usually yourusername.github.io repo)\nYou can buy a domain and point it to your repo.\nStatic doesn\u0026rsquo;t mean end of fun There are many \u0026lsquo;static\u0026rsquo; content generators that provide rich features:\nstyles links image resizing even \u0026lsquo;search\u0026rsquo; Even more fun with external services comments mailing lists etc. Some static generators Importance of language is for developing \u0026lsquo;plugins\u0026rsquo;, not content.\nJekyll (Ruby) Pelican (Python) Hugo(Go) They \u0026lsquo;render\u0026rsquo; markdown into html\nThere\u0026rsquo;s even more fun Github provides Jekyll support out of the box. Github, Gitlab, etc allow to plug in third-party CI Github has Actions Think about endless possibilities!!!\nWhat are Github actions? Github is a repository for code\nAllows third-party integration: Travis, Jenkins, bots, etc Github added GitHub Actions\nFor all repositories For free Easy to define new actions \u0026lsquo;cloning\u0026rsquo; with just a yaml in the repo What can we find? CI Formatting Linting Publishing Anything can be combined!! A full Marketplace (https://github.com/marketplace?type=actions) Some food for thought Repositories have branches Repositories can have automation External automation like Travis CI can do things for you Note: We\u0026rsquo;ve all the pieces to push a new markdown file and have it triggering a website update and publish\nIs a static webpage ugly? There are lot of templates http://www.pelicanthemes.com Each theme has different feature set Choose wisely! (Small screens, html5, etc) If not, changing themes is quite easy: update, and \u0026lsquo;render\u0026rsquo; using a new one. Travis-ci.org Automation for projects:\nFree for Open Source projects Configured via .travis.yml Some other settings via Web interface (environment variables, etc) Why Actions? Configured within yaml files in the repo GitHub pre-creates a token that can be used to push new files, branches, etc Without too much hassle, we\u0026rsquo;ve all the pieces! Wrap up Ok, automation is ready, our project validates commits, PR\u0026rsquo;s, website generation\u0026hellip;\nWhat else?\nTest it yourself Try https://github.com/iranzo/blog-o-matic/\nFork to your repo and get:\nminimal setup steps Automated setup of Pelican + Elegant theme via Git Hub action that builds on each commit. Ready to be submitted to search engines via sitemap, and web claiming Questions? Pablo.Iranzo@gmail.com\nhttps://iranzo.io ","permalink":"https://iranzo.io/presentations/2021-01-14-be-lazy-automate-gha-for-static-blogging/2021-01-14-be-lazy-automate-gha-for-static-blogging/","summary":"\u003ch2 id=\"be-lazy-automate-github-actions-for-static-blogging\"\u003eBe lazy, automate: GitHub Actions for static blogging\u003c/h2\u003e\n\u003cp\u003e/me: Pablo Iranzo Gómez ( \u003ca href=\"https://iranzo.io\"\u003ehttps://iranzo.io\u003c/a\u003e )\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-is-a-blog\"\u003eWhat is a blog?\u003c/h2\u003e\n\u003cp\u003eA place to share knowledge, interests, tips, etc.\u003c/p\u003e\n\u003cp\u003eUsually features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eimages\u003c/li\u003e\n\u003cli\u003ecomments from visitors,\u003c/li\u003e\n\u003cli\u003erelated articles,\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-are-the-costs-for-a-blog\"\u003eWhat are the costs for a blog?\u003c/h2\u003e\n\u003cp\u003eWeb costs money:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHosting\u003c/li\u003e\n\u003cli\u003eDomain\u003c/li\u003e\n\u003cli\u003eMaintenance\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-is-static-blogging\"\u003eWhat is static blogging?\u003c/h2\u003e\n\u003cp\u003eGenerate a static webpage\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThink of it as rendering templates into HTML\u003c/li\u003e\n\u003cli\u003eHas no requirements on the web server, any simple Webserver is enough:\n\u003cul\u003e\n\u003cli\u003eLook ma!, no database!\u003c/li\u003e\n\u003cli\u003eLook ma!, no users!\u003c/li\u003e\n\u003cli\u003eLook ma!, no security issues!\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-does-it-mean-to-us\"\u003eWhat does it mean to us?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eWe write an article\u003c/li\u003e\n\u003cli\u003eCommand for generating html from templates is used\u003c/li\u003e\n\u003cli\u003eNew files uploaded to Webserver\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"some-philosophy\"\u003e\u003ca href=\"https://www.youtube.com/watch?v=cJMwBwFj5nQ\"\u003eSome Philosophy\u003c/a\u003e\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eEmpty your mind, be shapeless, formless, like water.\nNow you put water in a cup, it becomes the cup, you put water into a bottle, it becomes the bottle\nYou put water in a teacup, it becomes the teapot\nNow water can flow or it can crash.\nBe water my friend\u003c/p\u003e","title":"Be lazy, automate: GitHub actions for static blogging"},{"content":"Introduction During the pandemic I wanted to work a bit on the wireless system at home, the router provided by the ISP was having already issues that resulted in WiFi devices not connecting and only new devices were able to report \u0026lsquo;AP full\u0026rsquo; message when the connection failed.\nFirst I started testing the ASUS devices like the AX92U which had WiFi 6 support. My flat is not that big, but as I had some issues with 2.4Ghz devices like smart power plugs, I decided to go for a second unit of the same router and test the AI-Mesh that was announced by ASUS as the best way to extend coverage.\nUnfortunately, that meant several things:\nWiFi 6 (available only in one of the two 5Ghz channels) was used for linking both AX92 routers, so clients couldn\u0026rsquo;t use it (I only had one laptop that supported it and got an Intel AX200 WiFi card for an older laptop to test it) That still left one 2.4Ghz channel and a 5Ghz channel to provide service to clients, while the WiFi 6 was used for the connection between both APs to provide connectivity The outcome was not good for having spent 400 EUR in WiFi equipment for a 90 m² flat and still have more disconnection issues from devices\u0026hellip; so all came back to where it came from.\nFor some months I was still checking reviews and struggling on how to improve it, it was not like thousand of WiFi devices, but still wanted network to work fine, and the result was that even if the Router was configured to allow more wireless clients, it was not allowing all of them to connect, so when I tried to reconnect a WiFi plug or an old phone that was sitting in a drawer, there was no way to get it working.\nUbiquiti UniFi Later in the year, after speaking with a colleague, he mentioned that he went with UniFi AC-LR (Long Range)\nCheck the parameter values\nRead before you start!!!\nIt\u0026rsquo;s not for noobs, requires either using a very simple approach using \u0026lsquo;standalone mode\u0026rsquo; or buying dedicated hardware for running the \u0026lsquo;Controller\u0026rsquo; or running a container on a Raspberry Pi or on your NAS/Computer Configuration is not as easy as other routers In exchange you get:\nComplete monitoring via their tool Configuration is kept at controller, you can add/remove devices and they will get \u0026lsquo;provisioned\u0026rsquo; with the configuration automatically. APs can \u0026lsquo;join\u0026rsquo; over WiFi to generate a network more capable of dealing with failures. APs use a Power Injector to get both data and power, so ideal for placing in the ceiling and just driving a RJ45 wire there. In general my experience was really smooth:\nAC-LR solved my connectivity problems for all the WiFi devices (2.4 and 5Ghz) Devices roamed from one AP to another transparently (only one was wired to my ISP router and the other was \u0026rsquo;linking\u0026rsquo; via WiFi) Container installation via linuxserver/unifi-controller container was great: podman create --name=UniFi-controller -e PUID=1000 -e PGID=1000 -e MEM_LIMIT=1024M -p 3478:3478/udp -p 10001:10001/udp -p 8080:8080 -p 8443:8443 -p 1900:1900/udp -p 8843:8843 -p 8880:8880 -p 6789:6789 -p 5514:5514 -v /root/data/UniFi:/config:Z --restart unless-stopped linuxserver/UniFi-controller Upgrades for it are also easy: first remove the existing container, then pull a new image and create a new container pointing to the same data folders. All from my Fedora NAS The final setup After some time, still under the evaluation, I experienced sometimes some issues with the AC-LR AP, it was performing fine, but when streaming from the one near my living room from TV (that connected via wireless to the one connected to the router), sometimes Prime Video had some hiccups. With my experience with the ASUS in the past, I decided to return the devices before it was too late and go for the UniFi NanoHD.\nThis device, in comparison, had better throughput in 5Ghz mode (with a bit higher cost than AC-LR), but smaller antennas (LR stands for Long-Range).\nAs I was happy with the UniFi setup, I also got some other hardware:\nUS-Gateway to be directly connected to my router US-8 ports switch to replace a underused 24 ports one 2 NanoHD AP They were connected in the following way:\nDiagram The Controller software is running as a container in the NAS system and manages the full infrastructure.\nI\u0026rsquo;m attaching here some of the pictures of the User Interface. Some of the features I like:\nthe ability to setup a guest portal than can even use \u0026rsquo;tickets\u0026rsquo;, so that you can limit internet access by day you can add/remove devices and they get the configuration from the controller when using the USG, you get extra features like VLAN for isolating network traffic, DHCP server, etc devices can get an alias (instead of mac or given name) from the User Interface, reserve IP address, etc. Devices are running Linux and you can connect to them via SSH Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2021/01/01/ubiquiti-unifi-for-wifi-network/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eDuring the pandemic I wanted to work a bit on the wireless system at home, the router provided by the ISP was having already issues that resulted in WiFi devices not connecting and only new devices were able to report \u0026lsquo;AP full\u0026rsquo; message when the connection failed.\u003c/p\u003e\n\u003cp\u003eFirst I started testing the ASUS devices like the \u003ca href=\"https://www.amazon.es/dp/B07SCBMMS8?tag=redken-21\"\u003eAX92U\u003c/a\u003e which had WiFi 6 support. My flat is not that big, but as I had some issues with 2.4Ghz devices like smart power plugs, I decided to go for a second unit of the same router and test the \u003ccode\u003eAI-Mesh\u003c/code\u003e that was announced by ASUS as the best way to extend coverage.\u003c/p\u003e","title":"Ubiquiti UniFi for WiFi network"},{"content":"Introduction In a regular OpenShift environment, NTP server is more less like this: Diagram In a self-contained cluster with no connection to external networks NTP server is not reachable, but a reachable NTP server is required for proper cluster synchronization. Cluster does use SSL certificates that require validation and might fail if the dates between the systems are not in sync or at least pretty close in time.\nDiagram We\u0026rsquo;ve several components already available in our OpenShift cluster that are very useful:\nMCO allows to define configuration to be applied by role, etc to the nodes chrony is the client/server installed in Red Hat CoreOS images for connecting to an external NTP Server chrony is already being used and configured via MachineConfigs to point to the configured NTP servers. Via a MCO change with a higher number than the prior ones, we can override the chrony.conf file by role, so that masters can set up required steps to serve time to other machines in the network even with external access to upstream servers or local GPS devices.\nWorkers can point to the masters so that those can be in sync via another file or via setting the proper install-config.yaml settings at install time.\nThere are some risks without a proper time sync:\nSystems might be not synced with real clock at all because of no external NTP access and no local time generator attached (like a GPS device). Users might forget to define proper BIOS time on all the systems prior to installation. In regards to system supportability, as applying MCO changes to the cluster and configuring chrony.conf is documented already it should not have a heavy impact on the cluster supportability (check references).\nImplementation First we need an available cluster installed, and remember:\nDefine proper clock/date in each system\u0026rsquo;s BIOS settings or installation will fail. In order to configure the ntp server, we\u0026rsquo;ll make use of the master servers, but we also need to deal with:\nTo allow multiple servers in the network to use the same local configuration and to be synchronized to one another, without confusing clients that poll more than one server, use the orphan option of the local directive which enables the orphan mode. Each server needs to be configured to poll all other servers with local. This ensures that only the server with the smallest reference ID has the local reference active and other servers are synchronized to it. When the server fails, another one will take over.\nIn order to do so, define a sample chrony.conf file:\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (https://www.pool.ntp.org/join.html). # This file is managed by the machine config operator server master-0.cloud iburst server master-1.cloud iburst server master-2.cloud iburst stratumweight 0 driftfile /var/lib/chrony/drift rtcsync makestep 10 3 bindcmdaddress 127.0.0.1 bindcmdaddress ::1 keyfile /etc/chrony.keys commandkey 1 generatecommandkey noclientlog logchange 0.5 logdir /var/log/chrony # Serve as local NTP server for all clients even if we\u0026#39;re not in sync with upstream: # Allow NTP client access from local network. allow all # Serve time even if not synchronized to a time source. local stratum 3 orphan This file is similar to the standard one but with the added directives at the end to allow all clients to sync time against this and set the local stratum to level 3, so that others can sync from this server.\nAdditionally the orphan option does the following:\nThis option enables a special orphan mode, where sources with stratum equal to the local stratum are assumed to not serve real time. They are ignored unless no other source is selectable and their reference IDs are smaller than the local reference ID.\nThis allows multiple servers in the network to use the same local configuration and to be synchronised to one another, without confusing clients that poll more than one server. Each server needs to be configured to poll all other servers with the local directive. This ensures only the server with the smallest reference ID has the local reference active and others are synchronised to it. When that server fails, another will take over.\nIn the case of the worker nodes, we just point to the master servers in our cluster via a sample chrony.conf file:\n# This file is managed by the machine config operator server master-0.cloud iburst server master-1.cloud iburst server master-2.cloud iburst stratumweight 0 driftfile /var/lib/chrony/drift rtcsync makestep 10 3 bindcmdaddress 127.0.0.1 bindcmdaddress ::1 keyfile /etc/chrony.keys commandkey 1 generatecommandkey noclientlog logchange 0.5 logdir /var/log/chrony Applying the configuration changes We can later configure prior chrony.conf via a yaml applied to our cluster.\nFor the master nodes:\n# This example MachineConfig replaces /etc/chrony.conf apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: master name: 99-master-etc-chrony-conf-override-to-server spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,BASE64ENCODEDCONFIGFILE filesystem: root mode: 0644 path: /etc/chrony.conf And then apply it:\n[user@myhost ~]$ oc apply -f ntp-server.yaml machineconfig.machineconfiguration.openshift.io/99-master-etc-chrony-conf-override-for-server created And for the workers:\n# This example MachineConfig replaces /etc/chrony.conf apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker name: 99-master-etc-chrony-conf-override-for-worker spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,BASE64ENCODEDCONFIGFILE filesystem: root mode: 0644 path: /etc/chrony.conf And apply in a similar way to master:\n[user@myhost ~]$ oc apply -f ntp-client.yaml machineconfig.machineconfiguration.openshift.io/99-master-etc-chrony-conf-override-for-worker created Validating Once the above (or equivalent) file is applied for both master and workers, we can execute oc describe machineconfigpool to check the status of the applied overrides.\nAnd for final validation, checking:\ncat /etc/chrony.conf on the nodes to validate the override we applied. chronyc sources will list the defined clock sources for each system References Setup chrony on an isolated network: Red Hat Enterprise Linux Documentation Customizing special chrony configuration How to set up a Network Time Protocol (NTP) Server on a LAN with no Internet access? ","permalink":"https://iranzo.io/blog/2020/12/07/configuring-openshift-with-self-contained-ntp/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn a regular OpenShift environment, NTP server is more less like this:\n\n\n  \n\n\n\u003cfigure\u003e\n  \u003cimg\n    src='https://g.gravizo.com/svg?%0a%0a%20%20%20%20%20%20digraph%20connected%20%7b%0a%20%20%20%20%20%20%20%20%20%20%2f%2f%20title%0a%20%20%20%20%20%20%20%20%20%20labelloc%3d%22t%22%3b%0a%20%20%20%20%20%20%20%20%20%20label%3d%22Connected%20Cluster%22%3b%0a%20%20%20%20%20%20%20%20%20%20node%20%5bshape%20%3d%20circle%5d%3b%0a%20%20%20%20%20%20%20%20%20%20%7b%20rank%20%3d%20same%3b%20%22External%20NTP%20Server%22%3b%7d%0a%20%20%20%20%20%20%20%20%20%20%7b%20rank%20%3d%20same%3b%20%22Master%201%22%3b%20%22Master%202%22%3b%20%22Master%203%22%7d%0a%20%20%20%20%20%20%20%20%20%20%7b%20rank%20%3d%20same%3b%20%22Worker%201%22%3b%20%22Worker%202%22%3b%20%22Worker%203%22%7d%0a%20%20%20%20%20%20%20%20%20%20%22Master%201%22%20-%3e%20%22External%20NTP%20Server%22%20%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Master%202%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Master%203%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Worker%201%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Worker%202%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%20%20%20%20%20%20%22Worker%203%22%20-%3e%20%22External%20NTP%20Server%22%5bcolor%3dred%5d%0a%20%20%20%20%7d%0a%0a'\n    alt='Diagram'\n    /\u003e\n    \u003cfigcaption\u003eDiagram\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003eIn a self-contained cluster with no connection to external networks NTP server is not reachable, but a reachable NTP server is required for proper cluster synchronization.\nCluster does use SSL certificates that require validation and might fail if the dates between the systems are not in sync or at least pretty close in time.\u003c/p\u003e\n\n\n  \n\n\n\u003cfigure\u003e\n  \u003cimg\n    src='https://g.gravizo.com/svg?%0adigraph%20disconnected%20%7b%0a%2f%2f%20title%0alabelloc%3d%22t%22%3b%0alabel%3d%22Disconnected%20Cluster%22%3b%0anode%20%5bshape%20%3d%20circle%5d%3b%0a%7b%20rank%20%3d%20same%3b%20%22Master%201%22%3b%20%22Master%202%22%3b%20%22Master%203%22%7d%0a%7b%20rank%20%3d%20same%3b%20%22Worker%201%22%3b%20%22Worker%202%22%3b%20%22Worker%203%22%7d%0a%22Master%201%22%20-%3e%20%22Master%202%22%20%5bcolor%3d%22red%22%5d%0a%22Master%201%22%20-%3e%20%22Master%203%22%20%5bcolor%3d%22green%22%5d%0a%22Master%202%22%20-%3e%20%22Master%201%22%20%5bcolor%3d%22purple%22%5d%0a%22Master%202%22%20-%3e%20%22Master%203%22%5bcolor%3d%22green%22%5d%0a%22Master%203%22%20-%3e%20%22Master%201%22%5bcolor%3d%22purple%22%5d%0a%22Master%203%22%20-%3e%20%22Master%202%22%5bcolor%3d%22red%22%5d%0a%22Worker%201%22%20-%3e%20%22Master%201%22%20%5bcolor%3d%22purple%22%5d%0a%22Worker%201%22%20-%3e%20%22Master%202%22%20%5bcolor%3d%22red%22%5d%0a%22Worker%201%22%20-%3e%20%22Master%203%22%20%5bcolor%3d%22green%22%5d%0a%22Worker%202%22%20-%3e%20%22Master%201%22%20%5bcolor%3d%22purple%22%5d%0a%22Worker%202%22%20-%3e%20%22Master%202%22%20%5bcolor%3d%22red%22%5d%0a%22Worker%202%22%20-%3e%20%22Master%203%22%20%5bcolor%3d%22green%22%5d%0a%22Worker%203%22%20-%3e%20%22Master%201%22%20%5bcolor%3d%22purple%22%5d%0a%22Worker%203%22%20-%3e%20%22Master%202%22%20%5bcolor%3d%22red%22%5d%0a%22Worker%203%22%20-%3e%20%22Master%203%22%20%5bcolor%3d%22green%22%5d%0a%7d%0a'\n    alt='Diagram'\n    /\u003e\n    \u003cfigcaption\u003eDiagram\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWe\u0026rsquo;ve several components already available in our OpenShift cluster that are very useful:\u003c/p\u003e","title":"Configuring OpenShift with self-contained NTP"},{"content":"During last year I\u0026rsquo;ve worked with the https://github.com/openshift-kni/baremetal-deploy/ repository after being working in the KNI Community team that was in charge of \u0026lt;KubeVirt.io\u0026gt; and \u0026lt;Metal3.io\u0026gt; where some of the below things were applied.\nOne of the goals we had was to streamline the upstream \u0026lt;-\u0026gt; downstream process to keep changes done in the right way: get changes upstream and copy over downstream with minimal changes.\nWe ended up using AsciiDoctor for building the documentation in the same way it\u0026rsquo;s done downstream so it\u0026rsquo;s just a matter of copying over the modules.\nAsciiDoctor can have variables replaced via command line parameters Can reuse the modules combined in different order to build different docs based on conditionals, master files, etc Can output both HTML and PDF format so it\u0026rsquo;s easy to create downloadable guides as well as browse-able ones In addition, as we use GitHub as a backend, we added some GitHub Actions workflows together with Netlify.\nNetlify provides a \u0026lsquo;free\u0026rsquo; service for Open Source projects that allows to \u0026lsquo;render\u0026rsquo; PR\u0026rsquo;s based on the defined scripts, in our case, that means building the documentation website fully for each PR received, so that you can directly see how the changes in a module are rendered in the final book without requiring manual checkout and build of the changes.\nWe\u0026rsquo;ve added GitHub actions for:\nOn each commit on the main branch, an action that runs Jekyll and a shell script build.sh is executed, this creates: HTML and PDF versions for each release defined in build.sh (we currently build from the same set of modules, the documentation for OpenShift 4.3 up to 4.7 JSON is created based on the documents rendered and stored in a file. Jekyll renders an index.md that loops over the items in the JSON to show the list of available documents. All above are pushed to another branch that is served via HTTP as a webpage using GitHub Pages at https://openshift-kni.github.io/baremetal-deploy/ Additionally: New users are welcomed on their first contribution. Old issues/PR\u0026rsquo;s are tagged and later closed if not updated. Issues are labelled according to target regular expressions. Ansible linting is performed on the Ansible playbooks we use in the repository. Broken links are checked. This whole process has automated our workflow, reducing the time spent on checking changes and by having a live preview of new changes and automatically built PDF versions that can be downloaded and accessed offline for onsite customer visits.\nDiagram Best practices:\nReuse as much as possible and keep the process simple: both for contributors and for reviewers, with this, you\u0026rsquo;re fostering collaboration: This is the way. Use automation wherever is possible, it might make things more complicated (like failing a PR because of a typo), but the end result will be worth it\u0026hellip; if not, you can perform those checks periodically and have extra work to do: automated spelling check website building (with proper tags for SEO, that will help your project to get known) output format creation, etc. If Upstream/Downstream has differences, try to get to a common ground: the less differences between booth, means more effective use of the time on creating new things or improving instead of reconciling changes Be FAST on answering to issues and PR\u0026rsquo;s, people has invested time into raising them (sometimes a bit of research should have avoided it, but many times that means a problem in usability, lack of clarity on the text, etc and instead of just going away, they did they part to raise it). It feels really bad when someone opens an issue or even contributes and gets no response/feedback from project maintainers. And even worse, when other changes get in the way and PR needs to be re-based once and over again instead of being merged. Feel free to reach with questions if any!\n","permalink":"https://iranzo.io/blog/2020/12/01/upstream/downstream-documentation-workflow/","summary":"\u003cp\u003eDuring last year I\u0026rsquo;ve worked with the \u003ca href=\"https://github.com/openshift-kni/baremetal-deploy/\"\u003ehttps://github.com/openshift-kni/baremetal-deploy/\u003c/a\u003e repository after being working in the KNI Community team that was in charge of \u0026lt;KubeVirt.io\u0026gt; and \u0026lt;Metal3.io\u0026gt; where some of the below things were applied.\u003c/p\u003e\n\u003cp\u003eOne of the goals we had was to streamline the upstream \u0026lt;-\u0026gt; downstream process to keep changes done in the right way: get changes upstream and copy over downstream with minimal changes.\u003c/p\u003e\n\u003cp\u003eWe ended up using AsciiDoctor for building the documentation in the same way it\u0026rsquo;s done downstream so it\u0026rsquo;s just a matter of copying over the modules.\u003c/p\u003e","title":"Upstream/Downstream documentation workflow"},{"content":"As with other kits, I bought the Led kit for the Porsche 911 GT3 RS 🛒#ad (more pics at Porsche 911 GT3) and just received my LED Kit 🛒#ad (or AliExpress 🛒#ad) after my experience with the Bugatti Chiron one.\nInstructions were a photocopy which was hard to read, but seller provided electronic copy that was easier to understand.\nThe kit contains:\nLED for front lights (main lights) LED for rear lights (brake) LED for the ceiling (driver area) USB connector Start installing them from the rear part, it will be easier to drive the wires trough the vehicle, it just comes with some bagged cables + pieces for each area listed above.\nCheck the results:\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/10/12/lego-porsche-911-led-kit/","summary":"\u003cp\u003eAs with other kits, I bought the Led kit for the \u003ca href=\"https://www.amazon.es/dp/B01CCT2ZHC?tag=redken-21\"\u003ePorsche 911 GT3 RS 🛒#ad\u003c/a\u003e (more pics at \u003ca href=\"/blog/2020/02/21/lego-bugatti-chiron-and-porsche-911-gt3-rs/\"\u003ePorsche 911 GT3\u003c/a\u003e) and just received my \u003ca href=\"https://www.amazon.es/dp/B07C31ZFDM?tag=redken-21\"\u003eLED Kit 🛒#ad\u003c/a\u003e (or \u003ca href=\"https://s.click.aliexpress.com/e/_eKUAhL\"\u003eAliExpress 🛒#ad\u003c/a\u003e) after my experience with the \u003ca href=\"/blog/2020/09/25/lego-bugatti-chiron-led-kit/\"\u003eBugatti Chiron\u003c/a\u003e one.\u003c/p\u003e\n\u003cp\u003eInstructions were a photocopy which was hard to read, but seller provided electronic copy that was easier to understand.\u003c/p\u003e\n\u003cp\u003eThe kit contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLED for front lights (main lights)\u003c/li\u003e\n\u003cli\u003eLED for rear lights (brake)\u003c/li\u003e\n\u003cli\u003eLED for the ceiling (driver area)\u003c/li\u003e\n\u003cli\u003eUSB connector\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStart installing them from the rear part, it will be easier to drive the wires trough the vehicle, it just comes with some bagged cables + pieces for each area listed above.\u003c/p\u003e","title":"Lego Porsche 911 Led Kit"},{"content":"I\u0026rsquo;ve the Lego Volkswagen T1 Camper van 🛒#ad (which I made a MOC for at Lego T1 Trailer MOC) and just received my LED Lightning Kit 🛒#ad (or AliExpress 🛒#ad) after my experience with the Bugatti Chiron one\nIt came with a very simple instruction manual (in comparison to the Chiron kit), and was able to have them installed within 10 minutes with minimal disruption to the pieces and powered via a power bank.\nThe kit contains:\nLED for front lights (direction + main lights) LED for rear lights (brake + plate) LED for the ceiling (driver area and rear area) USB connector Just make sure to check the lights before starting putting them on the van\u0026hellip; if something doesn\u0026rsquo;t work, it will take you lot of time to remove them and get replaced.\nCheck the results:\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/10/02/lego-volkswagen-t1-camper-van-led-kit/","summary":"\u003cp\u003eI\u0026rsquo;ve the \u003ca href=\"https://www.amazon.es/dp/B00PGZP8HE?tag=redken-21\"\u003eLego Volkswagen T1 Camper van 🛒#ad\u003c/a\u003e (which I made a MOC for at \u003ca href=\"/blog/2020/09/17/lego-volkswagen-t1-trailer/\"\u003eLego T1 Trailer MOC\u003c/a\u003e) and just received my \u003ca href=\"https://www.amazon.es/dp/B079LHQFYM?tag=redken-21\"\u003eLED Lightning Kit 🛒#ad\u003c/a\u003e (or \u003ca href=\"https://s.click.aliexpress.com/e/_dY2hmRx\"\u003eAliExpress 🛒#ad\u003c/a\u003e) after my experience with the \u003ca href=\"/blog/2020/09/25/lego-bugatti-chiron-led-kit/\"\u003eBugatti Chiron\u003c/a\u003e one\u003c/p\u003e\n\u003cp\u003eIt came with a very simple instruction manual (in comparison to the Chiron kit), and was able to have them installed within 10 minutes with minimal disruption to the pieces and powered via a power bank.\u003c/p\u003e\n\u003cp\u003eThe kit contains:\u003c/p\u003e","title":"Lego Volkswagen T1 Camper Van Led Kit"},{"content":"I\u0026rsquo;ve the Lego Bugatti Chiron 🛒#ad (more pics at its review and just recently via a credit coupon I got a LED Lightning Kit from Lightailing 🛒#ad (or AliExpress 🛒#ad).\nIt came with an instruction manual with 118 steps to get them installed and a plastic box with all the components required. Power was not included as I was planning to use a power-bank.\nTo be honest, the installation is not hard, but neither easy, it requires being extremely careful to properly place the wires and replace the original pieces with the ones supplied (for brakes and front lights) and be careful about the laying of the wires to allow them to fit in the proper places and still have room to locate the battery box.\nMine came with two sets of LED stripes for the rear brake (the one inside the red tube), but one of them couldn\u0026rsquo;t fit the hole, so the one was used (space is so slim that was not room to pass it through it without making too much force that could break it).\nThe kit contains:\nBoard for connecting Two wires to pass inside the red tube for rear brakes Small connectors to put inside the spoiler and the rear lights Front lights together with other bricks USB connector Just make sure to check the lights before starting putting them on the car\u0026hellip; if something doesn\u0026rsquo;t work, it will take you lot of time to remove them and get replaced.\nCheck the results:\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/09/25/lego-bugatti-chiron-led-kit/","summary":"\u003cp\u003eI\u0026rsquo;ve the \u003ca href=\"https://www.amazon.es/dp/B0792RB3B6?tag=redken-21\"\u003eLego Bugatti Chiron 🛒#ad\u003c/a\u003e (more pics at its \u003ca href=\"/blog/2020/02/21/lego-bugatti-chiron-and-porsche-911-gt3-rs/\"\u003ereview\u003c/a\u003e and just recently via a credit coupon I got a \u003ca href=\"https://www.amazon.es/dp/B07KG3LV8F?tag=redken-21\"\u003eLED Lightning Kit from \u003ccode\u003eLightailing\u003c/code\u003e 🛒#ad\u003c/a\u003e (or \u003ca href=\"https://s.click.aliexpress.com/e/_bWTEpe2\"\u003eAliExpress 🛒#ad\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eIt came with an instruction manual with 118 steps to get them installed and a plastic box with all the components required. Power was not included as I was planning to use a power-bank.\u003c/p\u003e\n\u003cp\u003eTo be honest, the installation is not hard, but neither easy, it requires being extremely careful to properly place the wires and replace the original pieces with the ones supplied (for brakes and front lights) and be careful about the laying of the wires to allow them to fit in the proper places and still have room to locate the battery box.\u003c/p\u003e","title":"Lego Bugatti Chiron Led Kit"},{"content":"I got some pieces that were part of a Lego Volkswagen T1 🛒#ad but couldn\u0026rsquo;t make for the complete set (which I already owned), so I was checking what mods I could build based on it, and best one, based on the amount of pieces was this one found on Rebrickable.\nAs I found it to be a bit sad to leave the wheel, and some other elements outside I did some minor changes:\nAdd a replacement wheel Expand the area for windows so that I can add more curtains Add a kitchen clock and smoke extractor Lift a bit the sink so that I could put the fold-able table there Put a sliding roof that can help to cover the area over the door Adding a rear bumper. The pics bellow show the evolution from the original model in Rebrickable to the changes I\u0026rsquo;ve incorporated, hope you enjoy them!\nI didn\u0026rsquo;t have more pieces, but the sliding roof has a whole, so that I could replace the top area with the original \u0026rsquo;elevating\u0026rsquo; roof, that will be reachable from the interior trough that hole\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/09/17/lego-volkswagen-t1-trailer/","summary":"\u003cp\u003eI got some pieces that were part of a \u003ca href=\"https://www.amazon.es/dp/B00PGZP8HE?tag=redken-21\"\u003eLego Volkswagen T1 🛒#ad\u003c/a\u003e but couldn\u0026rsquo;t make for the complete set (which I already owned), so I was checking what mods I could build based on it, and best one, based on the amount of pieces was this one found on \u003ca href=\"https://rebrickable.com/mocs/MOC-46121/tobowski/caravan-camping-trailer-for-10220-t1-bus/\"\u003eRebrickable\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAs I found it to be a bit sad to leave the wheel, and some other elements outside I did some minor changes:\u003c/p\u003e","title":"Lego Volkswagen T1 Trailer"},{"content":"When I started with blog-o-matic I had to involve external \u0026lsquo;Travis-CI\u0026rsquo;, generating a token on GitHub, setting environment variables on Travis, etc\nGitHub started enabling actions which allows to automate workflows in a similar way than Travis or other external providers allowed, but with one extra feature: configuration is defined inside .github/ folder of your repository, which makes incredibly easy to copy the setup for one tool to another (except of optional required tokens that are configured per repo).\nOne of the \u0026lsquo;available\u0026rsquo; tokens without extra configuration is a token that can be used to push or perform actions on the repo itself without asking user to create the \u0026lsquo;Personal Access Token\u0026rsquo;.\nThis can be used for things like:\nWelcoming new contributors Setting labels on issues/PR\u0026rsquo;s Check on project dependencies (npm, pip, node, etc) Expire issues/pull requests that are not touched in a period of time Allow to push to the repo itself etc Using the push to the repo, allows easily to get whatever is pushed to branch, operate over it and then push results\u0026hellip; which results ideal for getting data from one branch and push to Git Hub Pages branch for publishing.\nI\u0026rsquo;ve been also testing with a Git Hub Action creation to run a custom build command (to get asciidoctor content built to html and PDF), which is now at https://github.com/iranzo/gh-pages-jekyll-action.\nThis enables to chain more complex workflows, like having one repository with content and setup the build process with an action that finally pushes the generated results.\nAs I wanted to use asciidoc, I adapted a build.sh script that gets executed if existing on the repository and some configurable parameters (like website folder, etc) (check details on latest version at https://github.com/marketplace/actions/github-jekyll-build-action).\nEach time a new push is made to the repository, the GitHub action is executed and output generated.\nAs a side benefit, as actions work with \u0026lsquo;releases\u0026rsquo;, when a new release is done because and improvement is published as a release for the Git Hub Action, dependabot will create a PR to update each dependant repository to latest GitHub Action release.\nI\u0026rsquo;m working on getting something similar done with Pelican and then, move the blog-o-matic to use that approach to reduce the adoption curve.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/08/10/github-actions-for-publishing/","summary":"\u003cp\u003eWhen I started with \u003ca href=\"/blog/2019/01/09/blog-o-matic-quickly-get-a-github-hosted-blog-with-pelican-elegant-with-little-setup-steps./\"\u003eblog-o-matic\u003c/a\u003e I had to involve external \u0026lsquo;Travis-CI\u0026rsquo;, generating a token on GitHub, setting environment variables on Travis, etc\u003c/p\u003e\n\u003cp\u003eGitHub started enabling \u003ca href=\"https://github.com/features/actions\"\u003e\u003ccode\u003eactions\u003c/code\u003e\u003c/a\u003e which allows to automate workflows in a similar way than Travis or other external providers allowed, but with one extra feature: configuration is defined inside \u003ccode\u003e.github/\u003c/code\u003e folder of your repository, which makes incredibly easy to copy the setup for one tool to another (except of optional required tokens that are configured per repo).\u003c/p\u003e","title":"GitHub Actions for publishing"},{"content":"I wanted to practice a bit Go programing, so I divided that task in two parts, one, adding a golang extension for Citellus and a sample, but working plugin using it.\nIf interested in the code it\u0026rsquo;s available at the review at https://review.gerrithub.io/c/citellusorg/citellus/+/495622.\nThe final sample code for it has been:\n// Author: Pablo Iranzo Gómez (Pablo.Iranzo@gmail.com) // Header for citellus metadata // long_name: Report detected number of CPU\u0026#39;s // description: List the processors detected in the system // priority: 200 package main import ( \u0026#34;bufio\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) func main() { var OKAY, _ = strconv.Atoi(os.Getenv(\u0026#34;RC_OKAY\u0026#34;)) var SKIP, _ = strconv.Atoi(os.Getenv(\u0026#34;RC_SKIPPED\u0026#34;)) var INFO, _ = strconv.Atoi(os.Getenv(\u0026#34;RC_INFO\u0026#34;)) var CITELLUS_ROOT = os.Getenv(\u0026#34;CITELLUS_ROOT\u0026#34;) var CITELLUS_LIVE, _ = strconv.Atoi(os.Getenv(\u0026#34;CITELLUS_LIVE\u0026#34;)) var FAILED, _ = strconv.Atoi(os.Getenv(\u0026#34;RC_FAILED\u0026#34;)) if CITELLUS_LIVE == 1 { // Report # of CPU\u0026#39;s var CPUS = runtime.NumCPU() os.Stderr.WriteString(strconv.Itoa(CPUS)) os.Exit(INFO) } else if CITELLUS_LIVE == 0 { file, err := os.Open(CITELLUS_ROOT + \u0026#34;/proc/cpuinfo\u0026#34;) if err != nil { os.Stderr.WriteString(\u0026#34;Failure to open required file \u0026#34; + CITELLUS_ROOT + \u0026#34;/proc/cpuinfo\u0026#34;) os.Exit(SKIP) } defer file.Close() counts := wordCount(file) os.Stderr.WriteString(strconv.Itoa(counts[\u0026#34;processor\u0026#34;])) os.Exit(INFO) } else { os.Stderr.WriteString(\u0026#34;Undefined CITELLUS_LIVE status\u0026#34;) os.Exit(FAILED) } // Failback case, exiting as OK os.Exit(OKAY) } // https://forgetcode.com/go/2348-count-the-number-of-word-occurrence-in-given-a-file func wordCount(rdr io.Reader) map[string]int { counts := map[string]int{} scanner := bufio.NewScanner(rdr) scanner.Split(bufio.ScanWords) for scanner.Scan() { word := scanner.Text() word = strings.ToLower(word) counts[word]++ } return counts } Of course, lot of googling helped to start building the pieces.\nThings to have in consideration:\nGo programs are compiled so it requires being compiled with go build XXX or executed with go run XXX Programs executed with go run XXX might face some issues, as return code is not honoured (you can find more information on this). As program needs to be compiled, it will take a while to compile before execution (but still is fast) Undefined variables will make the program compilation to fail. Variable types are checked, so expect to use conversion when reading values, etc to have it compiling and working. For the citellus specifics:\nIn above example, we use Citellus Return Codes as defined in the citellus guidelines, but when executed with go run XXX only standard error codes are returned As citellus metadata parser gets it from headers, an update to the default function was done so that the comment character can be defined, in case of Go programs it is: // citellus then takes it into consideration for the offsets to grab the data. Happy coding!\n","permalink":"https://iranzo.io/blog/2020/06/14/go-golang-plugin-in-citellus/","summary":"\u003cp\u003eI wanted to practice a bit Go programing, so I divided that task in two parts, one, adding a golang extension for Citellus and a sample, but working plugin using it.\u003c/p\u003e\n\u003cp\u003eIf interested in the code it\u0026rsquo;s available at the review at \u003ca href=\"https://review.gerrithub.io/c/citellusorg/citellus/\u0026#43;/495622\"\u003ehttps://review.gerrithub.io/c/citellusorg/citellus/+/495622\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe final sample code for it has been:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Author: Pablo Iranzo Gómez (Pablo.Iranzo@gmail.com)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Header for citellus metadata\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// long_name: Report detected number of CPU\u0026#39;s\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// description: List the processors detected in the system\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// priority: 200\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003epackage\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e (\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;bufio\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;io\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;os\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;runtime\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;strconv\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;strings\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eOKAY\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003estrconv\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eAtoi\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eGetenv\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;RC_OKAY\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eSKIP\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003estrconv\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eAtoi\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eGetenv\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;RC_SKIPPED\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eINFO\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003estrconv\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eAtoi\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eGetenv\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;RC_INFO\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCITELLUS_ROOT\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eGetenv\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;CITELLUS_ROOT\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCITELLUS_LIVE\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003estrconv\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eAtoi\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eGetenv\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;CITELLUS_LIVE\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eFAILED\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003estrconv\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eAtoi\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eGetenv\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;RC_FAILED\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCITELLUS_LIVE\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#75715e\"\u003e// Report # of CPU\u0026#39;s\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCPUS\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003eruntime\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eNumCPU\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eStderr\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eWriteString\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003estrconv\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eItoa\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eCPUS\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eExit\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eINFO\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCITELLUS_LIVE\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003efile\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eOpen\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eCITELLUS_ROOT\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/proc/cpuinfo\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eStderr\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eWriteString\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Failure to open required file \u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCITELLUS_ROOT\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/proc/cpuinfo\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eExit\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eSKIP\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003edefer\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003efile\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eClose\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003ecounts\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ewordCount\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003efile\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eStderr\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eWriteString\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003estrconv\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eItoa\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ecounts\u003c/span\u003e[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;processor\u0026#34;\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eExit\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eINFO\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eStderr\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eWriteString\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Undefined CITELLUS_LIVE status\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eExit\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eFAILED\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#75715e\"\u003e// Failback case, exiting as OK\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eos\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eExit\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eOKAY\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// https://forgetcode.com/go/2348-count-the-number-of-word-occurrence-in-given-a-file\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ewordCount\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003erdr\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eio\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eReader\u003c/span\u003e) \u003cspan style=\"color:#66d9ef\"\u003emap\u003c/span\u003e[\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003ecounts\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003emap\u003c/span\u003e[\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e{}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003escanner\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ebufio\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eNewScanner\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003erdr\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003escanner\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eSplit\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ebufio\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eScanWords\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003escanner\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eScan\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eword\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003escanner\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eText\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003eword\u003c/span\u003e = \u003cspan style=\"color:#a6e22e\"\u003estrings\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eToLower\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eword\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#a6e22e\"\u003ecounts\u003c/span\u003e[\u003cspan style=\"color:#a6e22e\"\u003eword\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e++\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecounts\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOf course, lot of googling helped to start building the pieces.\u003c/p\u003e","title":"Go (golang) plugin in Citellus"},{"content":"In order to test IPv6 deployment on Dell hardware I was in need to patch the servers to ensure that UEFI boot mode is in use.\nNormally I would had use the DSU that runs from within Linux, but as the servers are part of an OpenShift installation (using baremetal-deploy) and using CoreOS as the underlying system I wanted to load ISO from HTTP server on the deployhost (running RHEL).\nThe command is not that hard, let\u0026rsquo;s first define some variables:\nIDRACIP=1.1.1.1 IDRACUSER=root IDRACPASS=mysecurepass ISOURL=\u0026#34;http://10.10.10.10/my.iso\u0026#34; Now, let\u0026rsquo;s attach the ISO file:\nracadm -r ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} remoteimage -c -l ${ISOURL} Once done, we should check status with:\nracadm -r ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} remoteimage -s Once finished, let\u0026rsquo;s disconnect:\nracadm -r ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} remoteimage -d And then, verify the status again:\nracadm -r ${IDRACIP} -u ${IDRACUSER} -p ${IDRACPASS} remoteimage -s Hope It\u0026rsquo;s useful for you!\n","permalink":"https://iranzo.io/blog/2020/05/12/dell-racadm-remote-iso-load/","summary":"\u003cp\u003eIn order to test IPv6 deployment on Dell hardware I was in need to patch the servers to ensure that UEFI boot mode is in use.\u003c/p\u003e\n\u003cp\u003eNormally I would had use the DSU that runs from within Linux, but as the servers are part of an OpenShift installation (using \u003ca href=\"https://github.com/openshift-kni/baremetal-deploy\"\u003ebaremetal-deploy\u003c/a\u003e) and using CoreOS as the underlying system I wanted to load ISO from HTTP server on the \u003ccode\u003edeployhost\u003c/code\u003e (running RHEL).\u003c/p\u003e\n\u003cp\u003eThe command is not that hard, let\u0026rsquo;s first define some variables:\u003c/p\u003e","title":"Dell racadm remote ISO load"},{"content":"Today I built the Lamborghini Urus ST-X \u0026amp; Lamborghini Huracán Super Trofeo EVO 🛒#ad.\nI liked especially the Huracán and the Urus was also very well done, a lot of details!\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/04/30/lego-speed-champions-76899-lamborghini-urus-st-x-lamborghini-hurac%C3%A1n-super-trofeo-evo/","summary":"\u003cp\u003eToday I built the \u003ca href=\"https://www.amazon.es/dp/B07W6Q9G1Y?tag=redken-21\"\u003eLamborghini Urus ST-X \u0026amp; Lamborghini Huracán Super Trofeo EVO 🛒#ad\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI liked especially the Huracán and the Urus was also very well done, a lot of details!\u003c/p\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/dMor15ot.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/dMor15ot.jpg\" alt=\"Huracán frontal-side view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/dMor15o.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/WFZExx3t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/WFZExx3t.jpg\" alt=\"Huracán frontal view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/WFZExx3.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/wC3QWDbt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/wC3QWDbt.jpg\" alt=\"Huracán frontal view with rear spoiler\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/wC3QWDb.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/f0RYTaHt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/f0RYTaHt.jpg\" alt=\"Huracán back details\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/f0RYTaH.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/gveoUgut.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/gveoUgut.jpg\" alt=\"Urus frontal-side view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/gveoUgu.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/R3old3ot.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/R3old3ot.jpg\" alt=\"Urus side-back view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/R3old3o.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/91uRNPAt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/91uRNPAt.jpg\" alt=\"Urus front-side view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/91uRNPA.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/mCbBcsDt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/mCbBcsDt.jpg\" alt=\"Huracán and Urus side by side\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/mCbBcsD.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\u003c/div\u003e\n\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego Speed Champions 76899 Lamborghini Urus ST-X \u0026 Lamborghini Huracán Super Trofeo EVO"},{"content":"Today I built the Jaguar Racing GEN2 car \u0026amp; Jaguar I-PACE eTROPHY 🛒#ad.\nI also bought a Light box to take pictures, so here are my first attempts.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/04/28/lego-speed-champions-76898-jaguar-racing-gen2-car-jaguar-i-pace-etrophy/","summary":"\u003cp\u003eToday I built the \u003ca href=\"https://www.amazon.es/dp/B07W5PXDYZ?tag=redken-21\"\u003eJaguar Racing GEN2 car \u0026amp; Jaguar I-PACE eTROPHY 🛒#ad\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI also bought a \u003ca href=\"https://s.click.aliexpress.com/e/_bmC0MP\"\u003eLight box\u003c/a\u003e to take pictures, so here are my first attempts.\u003c/p\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/B50sbObt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/B50sbObt.jpg\" alt=\"I-Pace front-side view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/B50sbOb.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/fYx5Jtrt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/fYx5Jtrt.jpg\" alt=\"Both vehicles front view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/fYx5Jtr.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/KuB4tO6t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/KuB4tO6t.jpg\" alt=\"Racing Gen2 front view #1\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/KuB4tO6.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/vg9yZ9mt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/vg9yZ9mt.jpg\" alt=\"Racing Gen2 front view #2\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/vg9yZ9m.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego Speed Champions 76898 Jaguar Racing GEN2 car \u0026 Jaguar I-PACE eTROPHY"},{"content":"Today, as part of my confinement Lego catchup I was building the Lego NASA Lunar Lander Apollo 11 🛒#ad.\nIt covered the evening after work, and have some nice details. First bags are for the lunar surface and the remainder are for the module starting with the parts which are closer to the moon surface.\nIt includes lots of gold-colour plates and some stickers for the external side (flag, controls of the lunar module and some external surfaces).\nThe module has a dual door that opens close to the ladder and the module sides can be removed and placed again easily so that the astronauts can be put there or outside.\nAlso, the module has two opening closets, one for the camera used to record the astronaut going down the scale :)\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/04/08/lego-10266-nasa-lunar-lander/","summary":"\u003cp\u003eToday, as part of my confinement Lego catchup I was building the \u003ca href=\"https://www.amazon.es/dp/B07G3WS3KV?tag=redken-21\"\u003eLego NASA Lunar Lander Apollo 11\n🛒#ad\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIt covered the evening after work, and have some nice details. First bags are for the lunar surface and the remainder are for the module starting with the parts which are closer to the moon surface.\u003c/p\u003e\n\u003cp\u003eIt includes lots of gold-colour plates and some stickers for the external side (flag, controls of the lunar module and some external surfaces).\u003c/p\u003e","title":"Lego 10266 NASA Lunar Lander"},{"content":"Introduction Before the move to Python3, redken had per-group language configuration by using i18n, with the upgrade/rewrite of Python3 code there were some issues and I had to remove the support, defaulting everything to English (most of the outputs where already in English so not a great loss).\nOn another side, having to manually configure each channel could be problematic as most users just add the bot to their groups but don\u0026rsquo;t care about other settings that might be useful like welcome message, inactivity for kicking out inactive users, etc.\nTelegram\u0026rsquo;s approach Initial version was added by expanding regular function that processes the Telegram-Server message to also take consideration of the user-indicated language and use that to store a new field in the database with the language, but that came with problems:\nNot all users had the language configured, so most times it was \u0026quot;\u0026quot; When talking on a group, the id was the group one, so if a user had language set to something and then other user wrote on the chat, it was just storing the \u0026rsquo;last used\u0026rsquo; language interface. Python to the rescue As the bot is written in Python, I did a quick search for language detection and found langdetect which is a port of a Java library to Python that can find certain words, etc in the texts and give some hint about the language.\nSo, instead of using the almost-always-empty user defined language for the interface, it started using langdetect for getting the message language based on the text.\nThis of course, solved on of the problems (no-configured language by users), still left the one on the \u0026rsquo;last-language-used\u0026rsquo; and introduced a new one:\nlangdetect does a guess based on common words, accents, characters, etc but it\u0026rsquo;s just that: a guess. The approach then was to use it to calculate averages based on prior count and new value.\nIn this way, the language moved from being just a string to become a dictionary, storing count and lang: % values, for example:\n{ \u0026#34;count\u0026#34;: 272, \u0026#34;en\u0026#34;: 2.2, \u0026#34;es\u0026#34;: 75.36, \u0026#34;it\u0026#34;: 3.74, \u0026#34;ca\u0026#34;: 3.48, \u0026#34;fi\u0026#34;: 2.3, \u0026#34;fr\u0026#34;: 1.64, \u0026#34;pt\u0026#34;: 1.73, \u0026#34;et\u0026#34;: 0.97, \u0026#34;ro\u0026#34;: 0.68, \u0026#34;de\u0026#34;: 1.11, \u0026#34;hr\u0026#34;: 0.68, \u0026#34;sw\u0026#34;: 1.09, \u0026#34;tl\u0026#34;: 0.96, \u0026#34;lt\u0026#34;: 0.68, \u0026#34;sk\u0026#34;: 1.22, \u0026#34;so\u0026#34;: 1.16, \u0026#34;da\u0026#34;: 1.18, \u0026#34;sv\u0026#34;: 0.67, \u0026#34;tr\u0026#34;: 0.58, \u0026#34;hu\u0026#34;: 0.46, \u0026#34;vi\u0026#34;: 0.43, \u0026#34;sl\u0026#34;: 0.41, \u0026#34;no\u0026#34;: 0.37, } This is my status, from 272 messages, the library has detected Spanish 75% of times, plus some other messages. As you might infer, there are lot of languages listed there that I never used, so it\u0026rsquo;s really important to keep this values refreshing with higher message counts.\nFor groups, this becomes even more interesting, as the message languages get updates for each user that speaks in the channel, giving \u0026lsquo;faster\u0026rsquo;, good results on the language being used:\nEnglish group:\n{ \u0026#34;count\u0026#34;: 56, \u0026#34;tr\u0026#34;: 1.79, \u0026#34;en\u0026#34;: 80.36, \u0026#34;da\u0026#34;: 1.79, \u0026#34;af\u0026#34;: 1.79, \u0026#34;sl\u0026#34;: 1.79, \u0026#34;ca\u0026#34;: 1.79, \u0026#34;es\u0026#34;: 1.79, \u0026#34;fi\u0026#34;: 1.79, \u0026#34;nl\u0026#34;: 1.79, \u0026#34;it\u0026#34;: 1.79, \u0026#34;sq\u0026#34;: 1.79, \u0026#34;so\u0026#34;: 1.79, } Spanish group:\n{ \u0026#34;count\u0026#34;: 140, \u0026#34;es\u0026#34;: 61.42, \u0026#34;fi\u0026#34;: 0.68, \u0026#34;ca\u0026#34;: 1.46, \u0026#34;it\u0026#34;: 5.72, \u0026#34;tr\u0026#34;: 0.68, \u0026#34;sw\u0026#34;: 1.46, \u0026#34;pt\u0026#34;: 5.74, \u0026#34;so\u0026#34;: 2.12, \u0026#34;en\u0026#34;: 8.58, \u0026#34;pl\u0026#34;: 0.68, \u0026#34;sv\u0026#34;: 1.48, \u0026#34;hr\u0026#34;: 0.68, \u0026#34;sk\u0026#34;: 0.67, \u0026#34;cy\u0026#34;: 3.58, \u0026#34;tl\u0026#34;: 1.43, \u0026#34;sl\u0026#34;: 0.68, \u0026#34;no\u0026#34;: 1.44, \u0026#34;de\u0026#34;: 0.69, \u0026#34;da\u0026#34;: 0.7, } Of course, before integrating this code into @redken_bot, I did a small program to validate it:\nfrom langdetect import detect import json # Add some sentences to an array to test text = [] text.append( \u0026#34;It could be that your new system is not getting as much throughput to your hard disks as it should be\u0026#34; ) text.append( \u0026#34;Il mio machina e piu veloce\u0026#34;, ) text.append( \u0026#34;Je suis tres desolè\u0026#34;, ) text.append( \u0026#34;El caballo blando de santiago era blanco\u0026#34;, ) text.append( \u0026#34;My tailor is rich\u0026#34;, ) text.append( \u0026#34;En un lugar de la Mancha de cuyo nombre no quiero acordarme\u0026#34;, ) text.append(\u0026#34;Good morning, hello, good morning hello\u0026#34;) text.append( \u0026#34;No es cierto angel de amor que en esta apartada orilla no luce el sol sino brilla\u0026#34; ) text.append(\u0026#34;Tears will be falling under the heavy rain\u0026#34;) text.append(\u0026#34;Que\u0026#39;l heure est il?\u0026#34;) text.append(\u0026#34;Caracol, col col, saca los cuernos al sol\u0026#34;) # Create dictionary empty for this to work language = {} language[\u0026#34;count\u0026#34;] = 0 # Process each line in text for line in text: # As we\u0026#39;ll be iterating later over a dictionary, prepare updates in a different one updates = {} # Detec language in the line received language_code = detect(line) print(\u0026#34; \u0026#34;) print(\u0026#34;Processing Line with detected language \u0026#34;, language_code, \u0026#34;|\u0026#34;, line) updates[\u0026#34;count\u0026#34;] = language[\u0026#34;count\u0026#34;] + 1 # Check if we need to add key to language if language_code not in language: print(\u0026#34;New key in language, preparing updates\u0026#34;) # New language % is 100 over the total number of updates received before, so 100% for the first message in a group updates[language_code] = 100 / updates[\u0026#34;count\u0026#34;] # Process each key we already had in language for key in language: # If the new language matches the one detected, give it a 100%, else, 0% , so that we work on % for each language if key == language_code: value = 100 else: value = 0 # As we store message count in the same dictionary, we just skip it if key != \u0026#34;count\u0026#34;: print(\u0026#34;Processing key %s in language for average updates\u0026#34; % key) updates[key] = float( \u0026#34;{0:.2f}\u0026#34;.format( language[key] + ((value - language[key]) / updates[\u0026#34;count\u0026#34;]) ) ) print(\u0026#34;New average: %s for language %s\u0026#34; % (updates[key], key)) print(\u0026#34;Updates: %s\u0026#34; % updates) language.update(updates) print(language) # Validate that final sum of % is close to 100% (consider rounding problems) accum = 0 for key in language: if key != \u0026#34;count\u0026#34;: accum = accum + language[key] print(float(\u0026#34;{0:.2f}\u0026#34;.format(accum))) # Dump the json of the final detected languages print(json.dumps(language)) Which, when executed, gives as final results:\nProcessing Line with detected language fr | Que\u0026#39;l heure est il? Processing key en in language for average updates New average: 40.0 for language en Processing key it in language for average updates New average: 10.0 for language it Processing key fr in language for average updates New average: 20.0 for language fr Processing key es in language for average updates New average: 30.0 for language es Updates: {\u0026#39;count\u0026#39;: 10, \u0026#39;en\u0026#39;: 40.0, \u0026#39;it\u0026#39;: 10.0, \u0026#39;fr\u0026#39;: 20.0, \u0026#39;es\u0026#39;: 30.0} {\u0026#39;count\u0026#39;: 10, \u0026#39;en\u0026#39;: 40.0, \u0026#39;it\u0026#39;: 10.0, \u0026#39;fr\u0026#39;: 20.0, \u0026#39;es\u0026#39;: 30.0} 100.0 Processing Line with detected language es | Caracol, col col, saca los cuernos al sol Processing key en in language for average updates New average: 36.36 for language en Processing key it in language for average updates New average: 9.09 for language it Processing key fr in language for average updates New average: 18.18 for language fr Processing key es in language for average updates New average: 36.36 for language es Updates: {\u0026#39;count\u0026#39;: 11, \u0026#39;en\u0026#39;: 36.36, \u0026#39;it\u0026#39;: 9.09, \u0026#39;fr\u0026#39;: 18.18, \u0026#39;es\u0026#39;: 36.36} {\u0026#39;count\u0026#39;: 11, \u0026#39;en\u0026#39;: 36.36, \u0026#39;it\u0026#39;: 9.09, \u0026#39;fr\u0026#39;: 18.18, \u0026#39;es\u0026#39;: 36.36} 99.99 {\u0026#34;count\u0026#34;: 11, \u0026#34;en\u0026#34;: 36.36, \u0026#34;it\u0026#34;: 9.09, \u0026#34;fr\u0026#34;: 18.18, \u0026#34;es\u0026#34;: 36.36} Conclusion Above code was adapted to redken, so when a new user message was received, both the group when the user wrote a sentence and the user itself, got a new dictionary of languages detected.\nThis approach, of using the \u0026lsquo;moving %\u0026rsquo;, just requires prior value for language and items count to calculate the new one reducing both the information needed to be stored to a minimum.\nIn the future, when I\u0026rsquo;m adding back strings for languages, I can automate how redken reacts per channel (unless overridden) so that it provides messages in a more natural way for users.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/03/25/language-detection-in-@redken_bot/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBefore the move to Python3, \u003ca href=\"https://t.me/redken_bot\"\u003eredken\u003c/a\u003e had per-group language configuration by using i18n, with the upgrade/rewrite of Python3 code there were some issues and I had to remove the support, defaulting everything to English (most of the outputs where already in English so not a great loss).\u003c/p\u003e\n\u003cp\u003eOn another side, having to manually configure each channel could be problematic as most users just add the bot to their groups but don\u0026rsquo;t care about other settings that might be useful like \u003ccode\u003ewelcome\u003c/code\u003e message, \u003ccode\u003einactivity\u003c/code\u003e for kicking out inactive users, etc.\u003c/p\u003e","title":"Language detection in @redken_bot"},{"content":"Introduction Ian, our English teacher from Tailor-Made Communication has given to us this game.\nThere are 5 buildings with consecutive numbers and some data to fill.\nA set of clues is given in order for the reader to deduct the data and to introduce some vocabulary.\nFill-in table Category 12 14 16 18 20 Name Marital status Pet Book Drink Clues Miss Dudd owns a dog The woman at number 12 has two pets. A tortoise and a rabbit. The dog owner drinks beer. Mrs Evans is married. Mr Abraham is a widower, his neighbour is divorced. The married woman reads thriller The woman who likes coffee does not own a pet. There are five pets in Baker Street, a cat, a dog, a canary, a rabbit and a tortoise. The bachelor likes historical novels. Mr Abraham cannot read, he watches TV. The widower and the spinster like beer. Mrs Birt likes to read books by Charles Dickens. The whisky drinker owns a canary. The dog owner living next door to the bachelor is keen on love stories. Mr Charles lives between Miss Dudd and Mrs Birt The married woman drinks wine The pet at number 14 is a dog. Mr Abraham lives at number 20 The dog owner and the cat owner do not live next to each other Fold here\nResolution Category 12 14 16 18 20 Name Mrs Evans Dudd Charles Birt Abraham Marital status Married Spinster Bachelor Divorced Widower Pet Tortoise and Rabbit Dog Canary XXX Cat Book Thriller Love stories Historical novels Dickens Watches TV Drink Wine Beer Whisky Coffee Beer ","permalink":"https://iranzo.io/blog/2020/03/24/english-game-baker-street/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIan, our English teacher from \u003ca href=\"https://www.linkedin.com/in/ian-pinhey-85553116/\"\u003eTailor-Made Communication\u003c/a\u003e has given to us this game.\u003c/p\u003e\n\u003cp\u003eThere are 5 buildings with consecutive numbers and some data to fill.\u003c/p\u003e\n\u003cp\u003eA set of clues is given in order for the reader to deduct the data and to introduce some vocabulary.\u003c/p\u003e\n\u003ch2 id=\"fill-in-table\"\u003eFill-in table\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003eCategory\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e12\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e14\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e16\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e18\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e20\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003eName\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003eMarital status\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003ePet\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003eBook\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003eDrink\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"clues\"\u003eClues\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eMiss Dudd owns a dog\u003c/li\u003e\n\u003cli\u003eThe woman at number 12 has two pets. A tortoise and a rabbit.\u003c/li\u003e\n\u003cli\u003eThe dog owner drinks beer.\u003c/li\u003e\n\u003cli\u003eMrs Evans is married.\u003c/li\u003e\n\u003cli\u003eMr Abraham is a widower, his neighbour is divorced.\u003c/li\u003e\n\u003cli\u003eThe married woman reads thriller\u003c/li\u003e\n\u003cli\u003eThe woman who likes coffee does not own a pet.\u003c/li\u003e\n\u003cli\u003eThere are five pets in Baker Street, a cat, a dog, a canary, a rabbit and a tortoise.\u003c/li\u003e\n\u003cli\u003eThe bachelor likes historical novels.\u003c/li\u003e\n\u003cli\u003eMr Abraham cannot read, he watches TV.\u003c/li\u003e\n\u003cli\u003eThe widower and the spinster like beer.\u003c/li\u003e\n\u003cli\u003eMrs Birt likes to read books by Charles Dickens.\u003c/li\u003e\n\u003cli\u003eThe whisky drinker owns a canary.\u003c/li\u003e\n\u003cli\u003eThe dog owner living next door to the bachelor is keen on love stories.\u003c/li\u003e\n\u003cli\u003eMr Charles lives between Miss Dudd and Mrs Birt\u003c/li\u003e\n\u003cli\u003eThe married woman drinks wine\u003c/li\u003e\n\u003cli\u003eThe pet at number 14 is a dog.\u003c/li\u003e\n\u003cli\u003eMr Abraham lives at number 20\u003c/li\u003e\n\u003cli\u003eThe dog owner and the cat owner do not live next to each other\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFold here\u003c/p\u003e","title":"English Game - Baker Street"},{"content":"Today, as part of my isolation I was building the Lego Fiat 500 🛒#ad.\nI was not sure about it, but I liked a lot the building experience and the final result.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/03/22/lego-creator-fiat-500-10271/","summary":"\u003cp\u003eToday, as part of my isolation I was building the \u003ca href=\"https://www.amazon.es/dp/B084ZR2D5Y?tag=redken-21\"\u003eLego Fiat 500\n🛒#ad\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI was not sure about it, but I liked a lot the building experience and the\nfinal result.\u003c/p\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/pF5IWGBt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/pF5IWGBt.jpg\" alt=\"Front view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/pF5IWGB.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/cEGzdpJt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/cEGzdpJt.jpg\" alt=\"Front view with opened roof\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/cEGzdpJ.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/yXdREeBt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/yXdREeBt.jpg\" alt=\"Driver area with door open\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/yXdREeB.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/sucvwKot.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/sucvwKot.jpg\" alt=\"Wheel in the trunk\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/sucvwKo.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/HYNNoNHt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/HYNNoNHt.jpg\" alt=\"Access to rear seats\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/HYNNoNH.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/5nSxDKIt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/5nSxDKIt.jpg\" alt=\"Suitcase in the back\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/5nSxDKI.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/UiBizBxt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/UiBizBxt.jpg\" alt=\"Engine door\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/UiBizBx.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/AvNPLB0t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/AvNPLB0t.jpg\" alt=\"Suitcase contents\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/AvNPLB0.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/Td4v9Rbt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/Td4v9Rbt.jpg\" alt=\"Suitcase holder grid\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/Td4v9Rb.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/ZDWtvCYt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/ZDWtvCYt.jpg\" alt=\"Side view with painting inside\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/ZDWtvCY.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/G44w463t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/G44w463t.jpg\" alt=\"Front view alongside Beetle\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/G44w463.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego Creator Fiat 500 10271"},{"content":"This article was published originally at https://kubevirt.io/2020/Live-migration.html\nIntroduction This blog post will be explaining on KubeVirt\u0026rsquo;s ability to perform live migration of virtual machines.\nLive Migration is a process during which a running Virtual Machine Instance moves to another compute node while the guest workload continues to run and remain accessible.\nThe concept of live migration is already well-known among virtualization platforms and enables administrators to keep user workloads running while the servers can be moved to maintenance for any reason that you might think of like:\nHardware maintenance (physical, firmware upgrades, etc) Power management, by moving workloads to a lower number of hypervisors during off-peak hours etc KubeVirt also includes support for virtual machine migration within Kubernetes when enabled.\nKeep reading to learn how!\nEnabling Live Migration To enable live migration we need to enable the feature-gate for it by adding LiveMigration to the key:\napiVersion: v1 kind: ConfigMap metadata: name: kubevirt-config namespace: kubevirt labels: kubevirt.io: \u0026#34;\u0026#34; data: feature-gates: \u0026#34;LiveMigration\u0026#34; A current kubevirt-config can be edited to append \u0026ldquo;LiveMigration\u0026rdquo; to an existing configuration:\nkubectl edit configmap kubevirt-config -n kubevirt data: feature-gates: \u0026#34;DataVolumes,LiveMigration\u0026#34; Configuring Live Migration If we want to alter the defaults for Live-Migration, we can further edit the kubevirt-config like:\napiVersion: v1 kind: ConfigMap metadata: name: kubevirt-config namespace: kubevirt labels: kubevirt.io: \u0026#34;\u0026#34; data: feature-gates: \u0026#34;LiveMigration\u0026#34; migrations: |- parallelMigrationsPerCluster: 5 parallelOutboundMigrationsPerNode: 2 bandwidthPerMigration: 64Mi completionTimeoutPerGiB: 800 progressTimeout: 150 Parameters are explained in the below table (check the documentation for more details):\nParameter Default value Description parallelMigrationsPerCluster 5 How many migrations might happen at the same time parallelOutboundMigrationsPerNode 2 How many outbound migrations for a particular node bandwidthPerMigration 64Mi MiB/s to have the migration limited to, in order to not affect other systems completionTimeoutPerGiB 800 Time for a GiB of data to wait to be completed before aborting the migration. progressTimeout 150 Time to wait for Live Migration to progress in transferring data Performing the Live Migration Error\nVirtual Machines using PVC must have a RWX access mode to be Live-Migrated Additionally, pod network binding of bridge interface is not allowed Live migration is initiated by posting an object VirtualMachineInstanceMigration to the cluster, indicating the VM name to migrate, like in the following example:\napiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstanceMigration metadata: name: migration-job spec: vmiName: vmi-fedora This will trigger the process for the VM.\nNote\nWhen a VM is started, a calculation has been already performed indicating if the VM is live-migratable or not. This information is stored in the VMI.status.conditions. Currently, most of the calculation is based on the Access Mode for the VMI volumes but can be based on multiple parameters. For example:\nStatus: Conditions: Status: True Type: LiveMigratable Migration Method: BlockMigration If the VM is Live-Migratable, the request will submit successfully. The status change will be reported under VMI.status. Once live migration is complete, a status of Completed or Failed will be indicated.\nImportant\nThe Migration Method field can contain:\nBlockMigration : meaning that the disk data is being copied from source to destination LiveMigration: meaning that only the memory is copied from source to destination VMs with block devices located on shared storage backends like the ones provided by Rook that provide PVCs with ReadWriteMany access have the option to live-migrate only memory contents instead of having to also migrate the block devices.\nCancelling a Live Migration If we want to abort the Live Migration, \u0026lsquo;Kubernetes-Style\u0026rsquo;, we\u0026rsquo;ll just delete the object we created for triggering it.\nIn this case, the VM status for migration will report some additional information:\nMigration State: Abort Requested: true Abort Status: Succeeded Completed: true End Timestamp: 2019-03-29T04:02:49Z Failed: true Migration Config: Completion Timeout Per GiB: 800 Progress Timeout: 150 Migration UID: 57a693d6-51d7-11e9-b370-525500d15501 Source Node: node02 Start Timestamp: 2019-03-29T04:02:47Z Target Direct Migration Node Ports: 39445: 0 43345: 49152 44222: 49153 Target Node: node01 Target Node Address: 10.128.0.46 Target Node Domain Detected: true Target Pod: virt-launcher-testvmimcbjgw6zrzcmp8wpddvztvzm7x2k6cjbdgktwv8tkq Note that there are some additional fields that indicate that Abort Requested happened and in the above example that it has Succeded, in this case, the original fields for migration will report as Completed (because there\u0026rsquo;s no running migration) and Failed set to true.\nWhat can go wrong? Live-migration is a complex process that requires transferring data from one \u0026lsquo;VM\u0026rsquo; in one node to another \u0026lsquo;VM\u0026rsquo; into another one, this requires that the activity of the VM being live-migrated to be compatible with the network configuration and throughput so that all the data can be migrated faster than the data is changed at the original VM, this is usually referred to as converging.\nSome values can be adjusted (check the table for settings that can be tuned), to allow it to succeed but as a trade-off:\nIncreasing the number of VMs that can migrate at once, will reduce the available bandwidth. Increasing the bandwidth could affect applications running on that node (origin and target). Storage migration (check the Info note in the Performing the Live Migration section on the differences) might also consume bandwidth and resources. Node Eviction Sometimes, a node requires to be put on maintenance and it includes workloads on it, either containers or, in KubeVirt\u0026rsquo;s case, VM\u0026rsquo;s.\nIt is possible to use selectors, for example, move all the virtual machines to another node via kubectl drain \u0026lt;nodename\u0026gt;, for example, evicting all KubeVirt VM\u0026rsquo;s from a node can be done via:\nkubectl drain \u0026lt;node name\u0026gt; --delete-local-data --ignore-daemonsets=true --force --pod-selector=kubevirt.io=virt-launcher Reenabling node after eviction\nOnce the node has been tainted for eviction, we can use kubectl uncordon \u0026lt;nodename\u0026gt; to make it schedulable again.\nAccording to documentation, --delete-local-data, --ignore-daemonsets and --force are required because:\nPods using emptyDir can be deleted because the data is ephemeral. VMI will have DaemonSets via virt-handler so it\u0026rsquo;s safe to proceed. VMIs are not owned by a ReplicaSet or DaemonSet, so kubectl can\u0026rsquo;t guarantee that those are restarted. KubeVirt has its own controllers for it managing VMI, so kubectl shouldn\u0026rsquo;t bother about it. If we omit the --pod-selector, we\u0026rsquo;ll force eviction of all Pods and VM\u0026rsquo;s from a node.\nImportant\nIn order to have VMIs using LiveMigration for eviction, we have to add a specific spec in the VMI YAML, so that when the node is tainted with kubevirt.io/drain:NoSchedule is added to a node.\nspec: evictionStrategy: LiveMigrate From that point, when kubectl taint nodes \u0026lt;foo\u0026gt; kubevirt.io/drain=draining:NoSchedule is executed, the migrations will start.\nConclusion As a briefing on the above data:\nLiveMigrate needs to be enabled on KubeVirt as a feature gate. LiveMigrate will add status to the VMI object indicating if it\u0026rsquo;s a candidate or not and if so, which mode to use (Block or Live) Based on the storage backend and other conditions, it will enable LiveMigration or just BlockMigration. References Live Migration Node Drain/Eviction Rook Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/eko/2020-03-22-live-migration/","summary":"\u003cp\u003eThis article was published originally at \u003ca href=\"https://kubevirt.io/2020/Live-migration.html\"\u003ehttps://kubevirt.io/2020/Live-migration.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis blog post will be explaining on KubeVirt\u0026rsquo;s ability to perform live migration of virtual machines.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eLive Migration is a process during which a running Virtual Machine Instance moves to another compute node while the guest workload continues to run and remain accessible.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe concept of live migration is already well-known among virtualization platforms and enables administrators to keep user workloads running while the servers can be moved to maintenance for any reason that you might think of like:\u003c/p\u003e","title":"Live Migration in KubeVirt"},{"content":"I\u0026rsquo;ve made a PR that got merged into Yaspeller repository which adds support for pre-commit to spell check your files.\nIt requires simple configuration, just add this snippet to your .pre-commit-config.yaml:\n- repo: https://github.com/hcodes/yaspeller.git rev: master hooks: - id: yaspeller files: \u0026#34;\\\\.en\\\\.md\u0026#34; The plugin will then initialize and spell check your files via yaspeller. It will use the standard .yaspeller.json file for dictionary and settings and automate it for each new commit you work on.\nImportant\nIf you were checking this information, note that I\u0026rsquo;ve made and get merged a PR to yaspeller so that we now use directly their repository. The snippet above was updated to reflect my current settings on this site (and only check articles in English).\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/03/12/yaspeller-hook-for-pre-commit/","summary":"\u003cp\u003eI\u0026rsquo;ve made a PR that got merged into Yaspeller repository which adds support for \u003ccode\u003epre-commit\u003c/code\u003e to spell check your files.\u003c/p\u003e\n\u003cp\u003eIt requires simple configuration, just add this snippet to your \u003ccode\u003e.pre-commit-config.yaml\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e- \u003cspan style=\"color:#f92672\"\u003erepo\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ehttps://github.com/hcodes/yaspeller.git\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003erev\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003emaster\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003ehooks\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    - \u003cspan style=\"color:#f92672\"\u003eid\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eyaspeller\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003efiles\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\\\\.en\\\\.md\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe plugin will then initialize and spell check your files via \u003ccode\u003eyaspeller\u003c/code\u003e. It will use the standard \u003ccode\u003e.yaspeller.json\u003c/code\u003e file for dictionary and settings and automate it for each new commit you work on.\u003c/p\u003e","title":"Yaspeller hook for pre-commit"},{"content":"Some pics about the \u0026lsquo;almost\u0026rsquo; modular Lego Ninjago City 🛒#ad.\nNote that it\u0026rsquo;s HUGE compared to other buildings and has lot of details. The water on the lower floor is defined by a set of bricks plus another set of transparent ones on top.\nSome of the details:\nHas an elevator from the lower floor to the top restaurant hidden areas to store advertisements ATM for getting money hidden area for ninja suite etc. Overall: one of the best sets I built. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/02/24/lego-ninjago-city-70620/","summary":"\u003cp\u003eSome pics about the \u0026lsquo;almost\u0026rsquo; modular \u003ca href=\"https://www.amazon.es/dp/B074XCNQSY?tag=redken-21\"\u003eLego Ninjago City 🛒#ad\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNote that it\u0026rsquo;s HUGE compared to other buildings and has lot of details. The water on the lower floor is defined by a set of bricks plus another set of transparent ones on top.\u003c/p\u003e\n\u003cp\u003eSome of the details:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHas an elevator from the lower floor to the top restaurant\u003c/li\u003e\n\u003cli\u003ehidden areas to store advertisements\u003c/li\u003e\n\u003cli\u003eATM for getting money\u003c/li\u003e\n\u003cli\u003ehidden area for ninja suite\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOverall: one of the best sets I built.\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/v5ulbTAt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/v5ulbTAt.jpg\" alt=\"Ninjago city bridge\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/v5ulbTA.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/abpvOekt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/abpvOekt.jpg\" alt=\"Lower floor\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/abpvOek.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/nP9tgZMt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/nP9tgZMt.jpg\" alt=\"Dock area\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/nP9tgZM.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/DqvUYTAt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/DqvUYTAt.jpg\" alt=\"1st floor aerial view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/DqvUYTA.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/zA0iozMt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/zA0iozMt.jpg\" alt=\"lower floor aerial view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/zA0iozM.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/pjODgY9t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/pjODgY9t.jpg\" alt=\"1st floor details\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/pjODgY9.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/VIcgZNat.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/VIcgZNat.jpg\" alt=\"Minifigure with diamond\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/VIcgZNa.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/M4dTBYut.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/M4dTBYut.jpg\" alt=\"Docks view with robot\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/M4dTBYu.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\u003c/div\u003e\n\u003c/p\u003e","title":"Lego Ninjago City 70620"},{"content":"Some pics about the \u0026lsquo;almost\u0026rsquo; modular Fisherman Store 🛒#ad Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/02/23/lego-fisherman-store-21310/","summary":"\u003cp\u003eSome pics about the \u0026lsquo;almost\u0026rsquo; modular \u003ca href=\"https://www.amazon.es/dp/B06X9QM15K?tag=redken-21\"\u003eFisherman Store 🛒#ad\u003c/a\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/Y2sqxOqt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/Y2sqxOqt.jpg\" alt=\"Front view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/Y2sqxOq.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/JTF5Zaxt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/JTF5Zaxt.jpg\" alt=\"Minifigure closeup\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/JTF5Zax.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/n0HIC2ut.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/n0HIC2ut.jpg\" alt=\"Seagull\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/n0HIC2u.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/JPUXy2Mt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/JPUXy2Mt.jpg\" alt=\"Stairs\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/JPUXy2M.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego Fisherman Store 21310"},{"content":"Here are my findings from today\u0026rsquo;s \u0026lsquo;hunt\u0026rsquo; for Lego Minifigures DC Comics 🛒#ad:\nLove the three I got, was looking for superman and batman but no luck at this shop :/\n","permalink":"https://iranzo.io/blog/2020/02/22/lego-minifigures-dc-comics-series-flash-joker-wonder-woman/","summary":"\u003cp\u003eHere are my findings from today\u0026rsquo;s \u0026lsquo;hunt\u0026rsquo; for \u003ca href=\"https://www.amazon.es/dp/B07YG7QVPX?tag=redken-21\"\u003eLego Minifigures DC Comics 🛒#ad\u003c/a\u003e:\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003cp\u003eLove the three I got, was looking for superman and batman but no luck at this shop :/\u003c/p\u003e","title":"Lego Minifigures DC Comics series Flash, Joker, Wonder Woman"},{"content":"Just some pics I made during some sorting of minifigs from my childhood. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/02/22/some-lego-vintage-minifigures/","summary":"\u003cp\u003eJust some pics I made during some sorting of minifigs from my childhood.\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/CeV61xrt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/CeV61xrt.jpg\" alt=\"Space minifigures\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/CeV61xr.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/pfYEfGJt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/pfYEfGJt.jpg\" alt=\"Pirates, doctors and firefighters\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/pfYEfGJ.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/4X7sNU8t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/4X7sNU8t.jpg\" alt=\"Archer, ghost, soldiers and tribal\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/4X7sNU8.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\u003c/p\u003e\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Some Lego vintage minifigures"},{"content":"Here you can see a photo comparison between Porsche 911 GT3 RS 🛒#ad and Bugatti Chiron 🛒#ad.\nBoth models have nice details, include a suitcase under the hood, sequential shift, 4x4 traction, etc. have a look at them!\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/02/21/lego-bugatti-chiron-and-porsche-911-gt3-rs/","summary":"\u003cp\u003eHere you can see a photo comparison between \u003ca href=\"https://www.amazon.es/dp/B01CCT2ZHC?tag=redken-21\"\u003ePorsche 911 GT3 RS 🛒#ad\u003c/a\u003e and \u003ca href=\"https://www.amazon.es/dp/B0792RB3B6?tag=redken-21\"\u003eBugatti Chiron 🛒#ad\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBoth models have nice details, include a suitcase under the hood, sequential shift, 4x4 traction, etc. have a look at them!\u003c/p\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/f6Clxgft.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/f6Clxgft.jpg\" alt=\"Side view with Porsche first\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/f6Clxgf.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/bIQfCLit.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/bIQfCLit.jpg\" alt=\"Front side view with Chiron first\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/bIQfCLi.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/Rbb5PpQt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/Rbb5PpQt.jpg\" alt=\"Side view of both cars\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/Rbb5PpQ.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/nwHJ62Mt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/nwHJ62Mt.jpg\" alt=\"Spoiler view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/nwHJ62M.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/XJVMBKAt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/XJVMBKAt.jpg\" alt=\"Rear view of Chiron\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/XJVMBKA.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/oW7sPn4t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/oW7sPn4t.jpg\" alt=\"Rear view of Porsche\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/oW7sPn4.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/cGVSsMYt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/cGVSsMYt.jpg\" alt=\"Chiron rear view with lowered spoiler\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/cGVSsMY.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/7bPMnuFt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/7bPMnuFt.jpg\" alt=\"Chiron wheel detail\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/7bPMnuF.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/qdbWfKqt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/qdbWfKqt.jpg\" alt=\"Chiron driving seat\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/qdbWfKq.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego Bugatti Chiron and Porsche 911 GT3 RS"},{"content":"I bought Lego 70821 Emmet Garage and Benny Spaceship 🛒#ad because I loved the astronaut minifigures from my childhood, and I even loved the broken helmet (as I used to have them broken in the same way).\nIn this case, the spacecraft is an adapted version of the typical spaceship at that time, the 497-Galaxy Explorer.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/02/20/lego-benny-spaceship-70821/","summary":"\u003cp\u003eI bought \u003ca href=\"https://www.amazon.es/dp/B07FP2KS4F?tag=redken-21\"\u003eLego 70821 Emmet Garage and Benny Spaceship 🛒#ad\u003c/a\u003e because I loved the astronaut minifigures from my childhood, and I even loved the \u003ccode\u003ebroken\u003c/code\u003e helmet (as I used to have them broken in the same way).\u003c/p\u003e\n\u003cp\u003eIn this case, the spacecraft is an \u003ccode\u003eadapted\u003c/code\u003e version of the typical spaceship at that time, the \u003ccode\u003e497-Galaxy Explorer\u003c/code\u003e.\u003c/p\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/69ePXLBt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/69ePXLBt.jpg\" alt=\"Front side view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/69ePXLB.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/3iMki6zt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/3iMki6zt.jpg\" alt=\"Rear view with bay area for rover\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/3iMki6z.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/nov158st.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/nov158st.jpg\" alt=\"Cockpit view rear\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/nov158s.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/gXfNh1It.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/gXfNh1It.jpg\" alt=\"Cockpit view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/gXfNh1I.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/9juBiAVt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/9juBiAVt.jpg\" alt=\"Cockpit view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/9juBiAV.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego Benny Spaceship 70821"},{"content":"Hi,\nUsing the following code from the Browser console:\nconsole.log(\u0026#34;\u0026#34;); var images = $$(\u0026#34;img\u0026#34;); for (each in images) { console.log(` \u0026lt;a href=\u0026#34;${images[each].src}\u0026#34; data-size=\u0026#34;4032x3024\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;${images[each].src}\u0026#34; width=\u0026#34;403\u0026#34; height=\u0026#34;302\u0026#34; alt=\u0026#34;Image description\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; `); } console.log(\u0026#34;\u0026lt;/div\u0026gt;\u0026#34;); It will output a copy-paste ready code for integrating in your blog post and leverage the picture gallery.\nWarning\nReview the data-size to make it match the image size as PhotoSwipe requires it to match image and adjust the figcaption entry.\n","permalink":"https://iranzo.io/blog/2020/02/19/javascript-for-imgur-gallery-generation-for-photoswipe-and-pelican-elegant/","summary":"\u003cp\u003eHi,\u003c/p\u003e\n\u003cp\u003eUsing the following code from the Browser console:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-js\" data-lang=\"js\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003econsole\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003elog\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eimages\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e$$\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;img\u0026#34;\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eeach\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eimages\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003econsole\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003elog\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e`\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e        \u0026lt;a href=\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eimages\u003c/span\u003e[\u003cspan style=\"color:#a6e22e\"\u003eeach\u003c/span\u003e].\u003cspan style=\"color:#a6e22e\"\u003esrc\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;  data-size=\u0026#34;4032x3024\u0026#34;\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e            \u0026lt;img src=\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eimages\u003c/span\u003e[\u003cspan style=\"color:#a6e22e\"\u003eeach\u003c/span\u003e].\u003cspan style=\"color:#a6e22e\"\u003esrc\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34; width=\u0026#34;403\u0026#34; height=\u0026#34;302\u0026#34;  alt=\u0026#34;Image description\u0026#34; /\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e        \u0026lt;/a\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    `\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003econsole\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003elog\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026lt;/div\u0026gt;\u0026#34;\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIt will output a copy-paste ready code for integrating in your blog post and leverage the picture gallery.\u003c/p\u003e\n\u003cdiv class=\"admonition warning\"\u003e\n    \u003cp class=\"admonition-title\"\u003eWarning\u003c/p\u003e\n    \u003cp class=\"admonition\"\u003eReview the \u003ccode\u003edata-size\u003c/code\u003e to make it match the image size as PhotoSwipe requires it to match image and adjust the \u003ccode\u003efigcaption\u003c/code\u003e entry.\u003c/p\u003e","title":"JavaScript for imgur gallery generation for PhotoSwipe and Pelican-Elegant"},{"content":"Disclaimer Simple talk oriented to 4 y-o children Vídeos, etc have been reduced from the originals (credits in bibliography) Magnetic Field, Aurora, Moon Phases and Rockets Pablo Iranzo Gómezhttps://iranzo.io\nThursday 13th, February 2020\nIntroduction Magnetic field Auroras Moon-Earth-Sun movement Moon phases Eclipses Rockets End Magnetic Field Magnetic field surrounds us, but it\u0026rsquo;s invisible although we can detect and measure.\nEasiest way to check it is to use a compass.\nNote: Use magnets and compass\n\u0026lsquo;See\u0026rsquo; the magnetic field We can see it by using iron powder and magnets.\nNote: Use magnets and iron powder\n\u0026lsquo;See\u0026rsquo; the magnetic field 2 Origin Earth has metal inside that moves and generates the magnetic field (pole) Earth is also very big and generates gravity field. Note: Volcanoes spit that inner melted materials to the surface of Earth\nSolar storms, solar wind and flares The Sun, sometimes has storms and particles (debris) is launched towards earth and others with the solar wind.\nNote: Analogy with water and storm\nEarth\u0026rsquo;s shield Magnetic field is Earth\u0026rsquo;s shield against solar wind.\nNote: Analogy with umbrella\nAuroras Auroras are created when the solar wind arrives to earth\nAuroras Video Video\nMoon-Earth-Sun movement Earth turns around the Sun and Moon around Earth.\nNote: Use 3 kids for explaining the movement and \u0026rsquo;eclipse\u0026rsquo; between them\nMoon phases Depending on Moon position relative to Earth and Sun, it illuminates different fractions of it causing the phases we see:\nNew Moon Crescent Moon Full Moon Waning moon Note: When the moon is \u0026lsquo;C\u0026rsquo;-shaped, it\u0026rsquo;s in Waning phase.\nMoon Phases 2 Eclipses Sometimes, the Moon is between Earth and Sun\nEclipses can be total or partial Eclipse 2 Note: Make a point on Moon round shape while it puts between Earth and Sun\nEclipse 3 Eclipse 4 Rockets Rockets fight against the other Earth: \u0026lsquo;gravity\u0026rsquo;\nMoon\u0026rsquo;s gravity causes tides on Earth\nEscaping Earth In order to escape Earth, speed must be very high (40.280 km/h).\nRockets are made in \u0026lsquo;sections\u0026rsquo; that can be dropped to be lighter and use less energy to lift off.\nSaturn V Take off Shuttle Falcon Heavy Trip to Moon Going back to Earth Going back to Earth 2 Shuttle\nBibliography Solar storms\nAuroras: Magic in the sky\nHow things started\nVideos Edited/reduced from originals:\nAuroras Shuttle Launch Falcon Heavy Shuttle Landing ¡End! ¡Thanks!\nNote: Give away compass and stickers\nMaterials Compass Stickers Eclipse model Iron powder Magnets Balloon Pegs Wool Straws Activities Magnets + compass Magnets + iron powder Eclipse model Balloons with wool for rocket Plastic parachute ","permalink":"https://iranzo.io/presentations/campo-magnetico-auroras-fases-lunares-eclipses-y-cohetes/","summary":"\u003ch2 id=\"disclaimer\"\u003eDisclaimer\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSimple talk oriented to 4 y-o children\u003c/li\u003e\n\u003cli\u003eVídeos, etc have been reduced from the originals (credits in bibliography)\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"magnetic-field-aurora-moon-phases-and-rockets\"\u003eMagnetic Field, Aurora, Moon Phases and Rockets\u003c/h2\u003e\n\u003cp\u003ePablo Iranzo Gómez\u003c!-- raw HTML omitted --\u003e\u003ca href=\"https://iranzo.io\"\u003ehttps://iranzo.io\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003eThursday 13th, February 2020\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eMagnetic field\n\u003cul\u003e\n\u003cli\u003eAuroras\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMoon-Earth-Sun movement\n\u003cul\u003e\n\u003cli\u003eMoon phases\u003c/li\u003e\n\u003cli\u003eEclipses\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRockets\u003c/li\u003e\n\u003cli\u003eEnd\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"magnetic-field\"\u003eMagnetic Field\u003c/h2\u003e\n\u003cp\u003eMagnetic field surrounds us, but it\u0026rsquo;s invisible although we can detect and\nmeasure.\u003c/p\u003e","title":"Magnetic field, aurora, moon phases, eclipse and rockets"},{"content":"While I was working for a presentation for kid\u0026rsquo;s school at Magnetic field, Aurora, Lunar Phases and Rockets, I added 4 big videos to the presentation (as I was going to use them offline while presenting).\nI know what git is not the place for big binary files, and even Github proposed to use the LFS backend for that, but as it was just temporary, I went ahead.\nAfter that commit, I also wrote two more articles, the one on Lego Speed Champions and the one on Galleria.io and PhotoSwipe, so it became a problem to have big files in between, when my plan was to remove them in the end.\nGit allows you to create branches at any point and play around with the commits, cherry-picking them into your branch, etc so for continue working I did create a new branch:\ngit checkout -b branchwithoutpresentation Note\nUntil this point, we\u0026rsquo;ve not performed any \u0026lsquo;damage\u0026rsquo; to the repository (and we still could revert back), make sure you\u0026rsquo;re testing on a repository suitable before doing this on valuable data.\nThen, I wanted to remove the \u0026lsquo;problematic commit\u0026rsquo; by running:\ngit rebase -i HEAD~20 In that way, git offers you an editor with the latest 20 commits in the branch so that you can elect to \u0026lsquo;drop\u0026rsquo; the ones that are problematic, in this case the one from the presentation.\nTo do so, go to the line describing the commit of the presentation and change pick to d and when the editor saves the changes and exits, the git history will be rewritten and the files dropped.\nWe\u0026rsquo;ve done that only in a new branch, so the original branch with the code (source in my case), still contains the presentation.\nTo rewrite the history and have the presentation in the end, we need to:\ngit checkout source git rebase -f branchwithoutpresentation Above command will rewrite \u0026lsquo;source\u0026rsquo; commits to be \u0026lsquo;on top\u0026rsquo; of the branchwithoutpresentation branch (the one without the presentation), leaving us with all the commits ordered, and in the last one, the presentation itself.\nThis allowed me to continue editing the last commit (git commit --amend --no-edit) adding or removing files always on the same commit, so once the big files where uploaded to YouTube, I could just drop them from the repository leaving it clean again.\nHowever, this means that the commits where altered in order, being the latest one, a commit \u0026lsquo;dated\u0026rsquo; earlier, of course, the end results didn\u0026rsquo;t changed, but wanted my git history to look \u0026rsquo;linear\u0026rsquo;, so I did the following procedure to \u0026lsquo;insert\u0026rsquo; the commit back where it belonged:\ngit log # to get list of commits (write down the commit number for the presentation) # also, write down the commit \u0026#39;after\u0026#39; the presentation should be inserted git checkout -b sortedbranch commitafterthepresentationshouldbeinserted git cherry-pick commitnumberofthepresentation git checkout source # to get back to the regular branch git rebase -f sortedbranch At this point, the remaining commits of the source branch were added on top of sortedbranch, leaving us in a ordered git log and in this case, without the big files in the repository.\nDanger\nAt this point, a simple git push will not allow you to update your upstream repository as the history is not linear (from their point of view), so a git push --force is needed to overwrite remote repository with local changes. This is destructive if others have made changes on the repository, so be really sure about what you do before doing it.\nHappy git playing!\n","permalink":"https://iranzo.io/blog/2020/02/13/git-commit-reordering/","summary":"\u003cp\u003eWhile I was working for a presentation for kid\u0026rsquo;s school at \u003ca href=\"%22campo-magnetico-auroras-fases-lunares-cohetes.md%22\"\u003eMagnetic field, Aurora, Lunar Phases and Rockets\u003c/a\u003e, I added 4 big videos to the presentation (as I was going to use them offline while presenting).\u003c/p\u003e\n\u003cp\u003eI know what git is not the place for big binary files, and even Github proposed to use the LFS backend for that, but as it was just temporary, I went ahead.\u003c/p\u003e\n\u003cp\u003eAfter that commit, I also wrote two more articles, the one on \u003ca href=\"/blog/2020/02/08/lego-speed-champions-2020-review/\"\u003eLego Speed Champions\u003c/a\u003e and the one on \u003ca href=\"/blog/2020/02/12/galleria.io-and-photoswipe/\"\u003eGalleria.io and PhotoSwipe\u003c/a\u003e, so it became a problem to have big files in between, when my plan was to remove them in the end.\u003c/p\u003e","title":"Git commit reordering"},{"content":"Introduction I was looking for an alternative for my (this) blog and hold pictures. I\u0026rsquo;m a lego fan so I wanted to get some pictures uploaded but without bloating the site.\nIn the article Lego Mini Cooper MOC I did add lot of pictures, same for Lego Chinese dinner and Lego Dragon Dance.\nI was checking and how telegram does handle some links and found that Instagram \u0026rsquo;links\u0026rsquo; get expanded to list the images inside directly to see if that could help in a task I was helping at Pelican-Elegant theme used at this site for creating a gallery.\nInstagram and multiple pictures While doing some research, I found that apparently, to an Instagram URL you can append ?__a=1 and it will \u0026lsquo;dump\u0026rsquo; a `xml\u0026rsquo; of the picture or gallery, like the following one for this picture:\nIn this case, the generated JSON, available at https://www.instagram.com/p/B7yh4IdItNd/?__a=1 contains the information we required for getting image size, thumbnail, etc\nThe important there is that from a gallery ID, we can get a JSON without requiring any API key (which is required for Flickr for example).\nI\u0026rsquo;m not an expert in JavaScript, but I did something which was useful enough to get the json, then process the internal code and separate in two cases, \u0026lsquo;gallery ID\u0026rsquo; with one or multiple pictures. Using it, I was able to then consider using some Gallery Software to get them rendered.\nGalleria.io https://galleria.io offers a nice gallery, which uses jQuery and also accepts a json as input, so it was more or less straightforward to convert from the json from Instagram to what Galleria.io required.\nI used that to get an initial draft to be added to Pelican-Elegant, but as the goal in the project is to remove dependency on jQuery it was discarded.\nHowever, my personal research came part of another website that even not directly from Instagram, takes a good advantage of defining a json of images for showcasing pictures.\nPhotoSwipe While discussing about Galleria, Talha from Pelican-Elegant found two other projects and it was decided that PhotoSwipe provided nice features, mobile usage like pinch-in and pinch-out.\nPhotoSwipe required extra steps like creating a div and using HTML tags for putting pics (check the official documentation for Pelican-Elegant Galleries)\nOne of the main problems was getting image size required (Galleria.io didn\u0026rsquo;t required it), but when using Instagram pictures we can grab the information from the JSON.\nFinally with the help from Talha, we got it also working with Instagram Gallery Embed Instagram Post\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2020/02/12/galleria.io-and-photoswipe/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI was looking for an alternative for my (this) blog and hold pictures. I\u0026rsquo;m a lego fan so I wanted to get some pictures uploaded but without bloating the site.\u003c/p\u003e\n\u003cp\u003eIn the article \u003ca href=\"/blog/2019/06/09/lego-75894-1967-mini-cooper-s-rally-and-buggy-moc-adaptation-as-mini-transporter/\"\u003eLego Mini Cooper MOC\u003c/a\u003e I did add lot of pictures, same for \u003ca href=\"/blog/2019/06/28/lego-80101-chinese-new-years-eve/\"\u003eLego Chinese dinner\u003c/a\u003e and \u003ca href=\"/blog/2019/06/28/lego-80102-dragon-dance/\"\u003eLego Dragon Dance\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI was checking and how telegram does handle some links and found that\nInstagram \u0026rsquo;links\u0026rsquo; get expanded to list the images inside directly to see if that could help in a task I was helping at \u003ca href=\"https://github.com/Pelican-Elegant/elegant\"\u003ePelican-Elegant theme\u003c/a\u003e used at this site for creating a gallery.\u003c/p\u003e","title":"Galleria.io and PhotoSwipe"},{"content":"Introduction on the new models I recently got the new Lego 2020 sets for the Speed Champions collection, which feature the new \u0026lsquo;8 studs\u0026rsquo; wide.\nOverall impression is that it allows more details on the vehicles (and thus, it gets more pieces, now separated in the instructions in bags 1 and 2).\nThe new baseplate allows enough space to hold two minifigs, but with a caveat, the side closer to the external place of the vehicle requires the minifig to have the arm lowered (not completely because it wouldn\u0026rsquo;t fit).\nIt also gains space in the middle for more details like the gear shift, hand brake.\nPrice seems like 25% higher than the older models, so it\u0026rsquo;s also something to get into consideration.\nBuilding experience is good, similar to other models, except the more detailed finishing that might require some more pieces for building.\nMy main concern about the proportions are the wheels, which are the same size as the older Speed Champions in 6 studs, so they look a bit \u0026rsquo;too small\u0026rsquo; compared to the car. Changing the wheels might require also to perform other changes like the piece that covers the wheel which is also inherited from the older models.\nFerrari F8 Tributo The Ferrari F8 Tributo 🛒#ad is for me, the \u0026rsquo;nicest\u0026rsquo; one, very nice on the building, low amount of stickers which is always good\u0026hellip; If you\u0026rsquo;re going to buy it, one thing to be careful of is the 1x1 stud pieces that contain the Ferrari logo and must be placed in the sides of the car, if you don\u0026rsquo;t pay attention you might end up having to dismantle for finding them (similar in other Speed Champions Ferrari models).\nMy only concern with this one is that I love the hair for the minifigure but can\u0026rsquo;t be put inside the car with it as the windshield will not close, so helmet must be put in place.\nNissan Nismo GT-R This Nissan Nismo GT-R 🛒#ad was at the beginning the car I liked the less and was not initially considering to buy it, but to be honest, I liked a lot building and the details (like rear brake lights) and it\u0026rsquo;s my kid\u0026rsquo;s preferred one.\nAudi Quattro S1 The third model, Audi Quattro S1 🛒#ad is also a very iconical car from the rallies, nice details and\u0026hellip; lot of stickers and you get a very detailed car (with alternative build with extra front lights as the ones depicted here)\nWrap up I liked the new models (except for the wheels and the price increase), and will probably consider the Jaguar and Lamborghini (I especially like the Lamborghini Huracán Super Trofeo).\nRegards and enjoy your builds!\n","permalink":"https://iranzo.io/blog/2020/02/08/lego-speed-champions-2020-review/","summary":"\u003ch2 id=\"introduction-on-the-new-models\"\u003eIntroduction on the new models\u003c/h2\u003e\n\u003cp\u003eI recently got the new Lego 2020 sets for the Speed Champions collection,\nwhich feature the new \u0026lsquo;8 studs\u0026rsquo; wide.\u003c/p\u003e\n\u003cp\u003eOverall impression is that it allows more details on the vehicles (and thus,\nit gets more pieces, now separated in the instructions in bags 1 and 2).\u003c/p\u003e\n\u003cp\u003eThe new baseplate allows enough space to hold two minifigs, but with a\ncaveat, the side closer to the external place of the vehicle requires the\nminifig to have the arm lowered (not completely because it wouldn\u0026rsquo;t fit).\u003c/p\u003e","title":"Lego Speed Champions 2020 review"},{"content":"I\u0026rsquo;ve been involved with a number of projects, some of which are listed below.\n@Redken_bot in Telegram Linux System Administration 9 and Linux System Administration 8 RISU Issue identification on systems/sosreports Other pets at Github ","permalink":"https://iranzo.io/projects/","summary":"\u003cp\u003eI\u0026rsquo;ve been involved with a number of projects, some of which are listed below.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/redken_bot/\"\u003e@Redken_bot\u003c/a\u003e in Telegram\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/blog/2022/10/12/book-red-hat-enterprise-linux-9-administration/\"\u003eLinux System Administration 9\u003c/a\u003e and \u003ca href=\"/blog/2021/09/11/book-red-hat-enterprise-linux-8-administration/\"\u003eLinux System Administration 8\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://risuorg.github.io\"\u003eRISU\u003c/a\u003e Issue identification on systems/sosreports\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/iranzo\"\u003eOther pets at Github\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Projects"},{"content":"Introduction During last days I\u0026rsquo;ve been playing around with python and API access to Google Sheets.\nSince some time ago, I already experimented with ICS parsing from python because added https://t.me/redken_bot access to calendar files (.ics) so that it can provide daily reminders on chats about the events happening for the specific date, and had the chance to propose it\u0026rsquo;s usage to cover a specific use case: Accessing a spreadsheet in Google Sheets and parse its contents and output an ICS file so that the events listed and the dates specific for each one are available in an easier-to-consume approach.\nAccessing Google Sheets Doing some research, it seemed that gspread python library was the easiest one to get the access to the spread sheet and then be able to process it.\nAccess requires setting up a credentials.json that is created using the Google developers console (we should create a new project, create new credentials, etc). Please, read gspread documentation on requisites and steps for obtaining this file.\nOnce the file is there, the code can be as easy as:\nimport os import gspread from oauth2client.service_account import ServiceAccountCredentials # use creds to create a client to interact with the Google Drive API scope = [ \u0026#34;https://spreadsheets.google.com/feeds\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive\u0026#34;, ] creds = ServiceAccountCredentials.from_json_keyfile_name( os.environ[\u0026#34;GACREDENTIALS\u0026#34;], scope ) client = gspread.authorize(creds) # Find a workbook by name and open the first sheet # Make sure you use the right name here. sheet = client.open(\u0026#34;My spreadsheet\u0026#34;).sheet1 # Extract and print all of the values records = sheet.get_all_records() for record in records: if record[\u0026#34;name\u0026#34;] != \u0026#34;\u0026#34;: dosomething() In this case, we access the file via the file pointed in GACREDENTIALS environment variable and use the scope for defining the access level we need for this file.\nIn this case, the spreadsheet was shared with \u0026lsquo;viewer\u0026rsquo; permissions to the system account created for the credentials and stored in the file on disk.\nThis enabled us to iterate over the elements and parse the rows in the sheet for doing or \u0026lsquo;ICS\u0026rsquo; dump to file, using specific column for each item (start date, end date, description, location, name, etc)\nThe generated ICS file could be stored in an accessible Webserver for subscribing to it, or we could manually import the events into our calendar.\nThis solution, however, created duplicates of entries on each import, as the walk process just recreated all the entries on each execution.\nAs a next-step, it was considered to directly write them to Google calendar so that the file itself could be created and stored but the actual event check happening via regular calendaring app.\nAccessing Google Calendar Access to Google calendar required to setup another method for accessing via using the standard Google API for Python. This required to setup credentials for OAUTH app, with a \u0026lsquo;portal\u0026rsquo; to grant access to a user, which writes a token to disk that can later be used for accessing the service.\nAs we\u0026rsquo;re reading always from the same Google Sheet, we wanted to first cleanup the older entries, so we used the following code (taken from: https://karenapp.io/articles/2019/07/how-to-automate-google-calendar-with-python-using-the-calendar-api/):\nimport os import os.path import pickle import sys import dateutil.parser import gspread from google.auth.transport.requests import Request from googleapiclient.discovery import build from oauth2client.service_account import ServiceAccountCredentials # If modifying these scopes, delete the file token.pickle. SCOPES = [\u0026#34;https://www.googleapis.com/auth/calendar\u0026#34;] credpickle = os.environ[\u0026#34;GApickle\u0026#34;] # Calendar to write to mycalendarid = \u0026#34;mycalendarID received when listing available calendars\u0026#34; def get_calendar_service(): \u0026#34;\u0026#34;\u0026#34; Gets calendar service handler :return: services handler \u0026#34;\u0026#34;\u0026#34; creds = None # The file token.pickle stores the user\u0026#39;s access and refresh tokens, and is # created automatically when the authorization flow completes for the first # time. if os.path.exists(credpickle): with open(credpickle, \u0026#34;rb\u0026#34;) as token: creds = pickle.load(token) # If there are no (valid) credentials available, FAIL! if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) else: print(\u0026#34;Error, create a new pickle file for continuing\u0026#34;) sys.exit(1) # Save the credentials for the next run with open(\u0026#34;token.pickle\u0026#34;, \u0026#34;wb\u0026#34;) as token: pickle.dump(creds, token) service = build(\u0026#34;calendar\u0026#34;, \u0026#34;v3\u0026#34;, credentials=creds) return service We also wrote a function for writing new events to the calendar:\ndef writeevent(service, summary, location, start, end, description): \u0026#34;\u0026#34;\u0026#34; Writes event to Gcal :param service: service handler :param summary: summary of event :param location: location of event :param start: start date :param end: end date :param description: Description of event \u0026#34;\u0026#34;\u0026#34; event_result = ( service.events() .insert( calendarId=mycalendarid, body={ \u0026#34;summary\u0026#34;: summary, \u0026#34;location\u0026#34;: location, \u0026#34;start\u0026#34;: { \u0026#34;date\u0026#34;: start.strftime(\u0026#34;%Y-%m-%d\u0026#34;), \u0026#34;timeZone\u0026#34;: \u0026#34;Europe/Madrid\u0026#34;, }, \u0026#34;end\u0026#34;: { \u0026#34;date\u0026#34;: end.strftime(\u0026#34;%Y-%m-%d\u0026#34;), \u0026#34;timeZone\u0026#34;: \u0026#34;Europe/Madrid\u0026#34;, }, \u0026#34;description\u0026#34;: description, }, ) .execute() ) print(\u0026#34;Event: %s written\u0026#34; % summary) And the main code:\ndef main(): \u0026#34;\u0026#34;\u0026#34; Main code for the program \u0026#34;\u0026#34;\u0026#34; # Cleanup Gcalendar service = get_calendar_service() # Call the Calendar API events_result = service.events().list(calendarId=mycalendarid).execute() events = events_result.get(\u0026#34;items\u0026#34;, []) # Delete all the events for event in events: try: print(\u0026#34;Deleting event: %s\u0026#34; % event[\u0026#34;summary\u0026#34;]) service.events().delete( calendarId=mycalendarid, eventId=event[\u0026#34;id\u0026#34;] ).execute() except googleapiclient.errors.HttpError: print(\u0026#34;Failed to delete event\u0026#34;) # Create events # Connect to Gspreadsheet to get events # use creds to create a client to interact with the Google Drive API scope = [ \u0026#34;https://spreadsheets.google.com/feeds\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive\u0026#34;, ] creds = ServiceAccountCredentials.from_json_keyfile_name( os.environ[\u0026#34;GACREDENTIALS\u0026#34;], scope ) client = gspread.authorize(creds) # Find a workbook by name and open the first sheet # Make sure you use the right name here. sheet = client.open(\u0026#34;KNI conferences pipeline\u0026#34;).sheet1 # Extract and print all of the values records = sheet.get_all_records() # Iterate records for valid events and create them for record in records: if record[\u0026#34;Name\u0026#34;] != \u0026#34;\u0026#34;: # Main event start = record[\u0026#34;Starts\u0026#34;] end = record[\u0026#34;Ends\u0026#34;] startdate = False enddate = False if start != \u0026#34;\u0026#34;: startdate = dateutil.parser.parse(start) if end != \u0026#34;\u0026#34;: enddate = dateutil.parser.parse(end) if startdate and enddate: writeevent( service, summary=record[\u0026#34;Conference\u0026#34;], location=record[\u0026#34;Location\u0026#34;], start=startdate, end=enddate, description=record[\u0026#34;Event URL\u0026#34;], ) if __name__ == \u0026#34;__main__\u0026#34;: main() Wrap up The final code, is able to use both API\u0026rsquo;s to get access to the spreadsheet, read the rows, and in parallel use the API to nuke the calendar, by removing all events and then, recreating them with the data obtained from the spreadsheet.\nThere\u0026rsquo;s of course room for optimization, like just updating events if it\u0026rsquo;s changing or not, but that would make code more complex and not adding much more functionality (as still calls would be needed for querying event, comparing, and then updating as needed).\nNote\nIf you wonder about why using environment variables for the credential files used\u0026hellip; this jobs runs inside Jenkins instance that has the defined secrets exported in the Jenkinsfile as environment variables, allowing to \u0026lsquo;secure\u0026rsquo; a bit more the access to the credentials itself (even if those have been scope-limited to only allow not harmful usage.)\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/11/28/api-access-for-google-calendar-and-google-sheet-access/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eDuring last days I\u0026rsquo;ve been playing around with python and API access to Google Sheets.\u003c/p\u003e\n\u003cp\u003eSince some time ago, I already experimented with ICS parsing from python because added \u003ca href=\"https://t.me/redken_bot\"\u003ehttps://t.me/redken_bot\u003c/a\u003e access to calendar files (\u003ccode\u003e.ics\u003c/code\u003e) so that it can provide daily reminders on chats about the events happening for the specific date, and had the chance to propose it\u0026rsquo;s usage to cover a specific use case: Accessing a spreadsheet in Google Sheets and parse its contents and output an ICS file so that the events listed and the dates specific for each one are available in an easier-to-consume approach.\u003c/p\u003e","title":"API access for Google Calendar and Google Sheet access"},{"content":"Introduction In the article Lego Dimensions, I covered all the available packs for playing with the console.\nUnfortunately, not all of them are available for purchase anymore except from collectors, \u0026lt;brickset.com\u0026gt; or some resellers.\nAfter some research, several pages provide instructions about the tags themselves and how to \u0026lsquo;fix\u0026rsquo; or \u0026lsquo;clone\u0026rsquo; them.\nMaterials Each base is a NFC tag NTAG 213 which is either read only (characters) or rewritable (vehicles).\nWith an NFC writer/reader on your mobile phone, the tags can be cloned/copied so that you can \u0026lsquo;repair\u0026rsquo; the broken ones, without buying again the pack (base can be opened with a precision screwdriver by pulling it out using the small marks (opposite in the base with + shape).\nIn order to \u0026lsquo;create\u0026rsquo; your own, the following bill of materials is useful:\nItem AliExpress Amazon Tags http://s.click.aliexpress.com/e/dMeJ0gug Hole Punch http://s.click.aliexpress.com/e/5U3WF4fq Holder http://s.click.aliexpress.com/e/_ruEsf3 Insights Each tag can be read/written with an app on your mobile (I\u0026rsquo;ve tested it with Nexus 5 🛒#ad, Sony Xperia Z5 🛒#ad, Samsung Galaxy S8 🛒#ad and Samsung Galaxy Note 9 🛒#ad), and the \u0026rsquo;trick\u0026rsquo; is to always \u0026lsquo;read\u0026rsquo; first the tag and then write it with the modified parameters.\nEach tag has in at the beginning the serial number, so that it can\u0026rsquo;t be changed and then, there\u0026rsquo;s a \u0026lsquo;password\u0026rsquo; field used to validate.\nThis article Manual para clonar figuras de Lego Dimensions includes two methods, one using an App that worked quite well called ldtageditor, which can be located at Google via https://www.google.com/search?client=firefox-b-d\u0026q=ldtageditor+apk and other using an app in the Android Play Store called MIFARE++ Ultralight.\nThe 2nd App is \u0026lsquo;harder\u0026rsquo; to use as it includes manually calculating ID\u0026rsquo;s via a website or app (search for ldcharcrpyto or read this forum http://www.proxmark.org/forum/viewtopic.php?id=2657\u0026p=2).\nTrick here is:\nto use the tags and read before selecting char and writing it back Don\u0026rsquo;t put characters that are not available in the tag, as those will not be recognized and tag will be unusable until you \u0026lsquo;format\u0026rsquo; them (use NFC Tools app for that) Doing this, you\u0026rsquo;ll be able to clone or write the tags that were damaged and put new ones in their place to allow you continue playing as you did before!\nThere was also a \u0026lsquo;ready-to-print\u0026rsquo; set of pics for the tags in one of the forum posts at http://www.mediafire.com/file/b2d5a9dwj7ksry9/LD1inchimages.zip which contains the images for each tag and possible vehicle.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/09/29/cloning-lego-dimensions-tags/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the article \u003ca href=\"/blog/2019/08/19/lego-dimensions-review/\"\u003eLego Dimensions\u003c/a\u003e, I covered all the available packs for playing with the console.\u003c/p\u003e\n\u003cp\u003eUnfortunately, not all of them are available for purchase anymore except from collectors, \u0026lt;brickset.com\u0026gt; or some resellers.\u003c/p\u003e\n\u003cp\u003eAfter some research, several pages provide instructions about the tags themselves and how to \u0026lsquo;fix\u0026rsquo; or \u0026lsquo;clone\u0026rsquo; them.\u003c/p\u003e\n\u003ch2 id=\"materials\"\u003eMaterials\u003c/h2\u003e\n\u003cp\u003eEach base is a NFC tag \u003ccode\u003eNTAG 213\u003c/code\u003e which is either read only (characters) or rewritable (vehicles).\u003c/p\u003e\n\u003cp\u003eWith an NFC writer/reader on your mobile phone, the tags can be cloned/copied so that you can \u0026lsquo;repair\u0026rsquo; the broken ones, without buying again the pack (base can be opened with a precision screwdriver by pulling it out using the small marks (opposite in the base with \u003ccode\u003e+\u003c/code\u003e shape).\u003c/p\u003e","title":"Cloning Lego Dimensions tags"},{"content":"Introduction iCalendar (.ics or webcal) is a standard for providing \u0026lsquo;calendar\u0026rsquo; information over text file, it allows to define events, etc and it\u0026rsquo;s ideal to \u0026lsquo;subscribe\u0026rsquo; over the internet to remote calendars, which is commonly used to show free/busy slots for scheduling meetings, etc.\nIf you\u0026rsquo;re familiar with Google Calendar or others, it uses \u0026lsquo;ics\u0026rsquo; under the hood, and you can get full url for it so that it can be shared.\nI wanted to add ICS functionality to @redken_bot in Telegram, in a similar way to what I had for RSS feeds, comic strips, etc, so I started to investigate.\nThe ICS file The ICS file is a text-based file which contains some definitions for the calendar itself:\nBEGIN:VCALENDAR PRODID:-//Google Inc//Google Calendar 70.9054//EN VERSION:2.0 CALSCALE:GREGORIAN METHOD:PUBLISH X-WR-CALNAME:Pablo X-WR-TIMEZONE:Europe/Madrid BEGIN:VTIMEZONE TZID:America/Los_Angeles X-LIC-LOCATION:America/Los_Angeles BEGIN:DAYLIGHT TZOFFSETFROM:-0800 TZOFFSETTO:-0700 TZNAME:PDT DTSTART:19700308T020000 RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=2SU END:DAYLIGHT BEGIN:STANDARD TZOFFSETFROM:-0700 TZOFFSETTO:-0800 TZNAME:PST DTSTART:19701101T020000 RRULE:FREQ=YEARLY;BYMONTH=11;BYDAY=1SU END:STANDARD END:VTIMEZONE BEGIN:VTIMEZONE TZID:Etc/UTC X-LIC-LOCATION:Etc/UTC BEGIN:STANDARD TZOFFSETFROM:+0000 TZOFFSETTO:+0000 TZNAME:GMT DTSTART:19700101T000000 END:STANDARD END:VTIMEZONE BEGIN:VTIMEZONE TZID:Europe/Madrid X-LIC-LOCATION:Europe/Madrid BEGIN:DAYLIGHT TZOFFSETFROM:+0100 TZOFFSETTO:+0200 TZNAME:CEST DTSTART:19700329T020000 RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU END:DAYLIGHT BEGIN:STANDARD TZOFFSETFROM:+0200 TZOFFSETTO:+0100 TZNAME:CET DTSTART:19701025T030000 RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU END:STANDARD END:VTIMEZONE BEGIN:VTIMEZONE TZID:Europe/Paris X-LIC-LOCATION:Europe/Paris BEGIN:DAYLIGHT TZOFFSETFROM:+0100 TZOFFSETTO:+0200 TZNAME:CEST DTSTART:19700329T020000 RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU END:DAYLIGHT BEGIN:STANDARD TZOFFSETFROM:+0200 TZOFFSETTO:+0100 TZNAME:CET DTSTART:19701025T030000 RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU END:STANDARD END:VTIMEZONE BEGIN:VTIMEZONE TZID:Africa/Ceuta X-LIC-LOCATION:Africa/Ceuta BEGIN:DAYLIGHT TZOFFSETFROM:+0100 TZOFFSETTO:+0200 TZNAME:CEST DTSTART:19700329T020000 RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU END:DAYLIGHT BEGIN:STANDARD TZOFFSETFROM:+0200 TZOFFSETTO:+0100 TZNAME:CET DTSTART:19701025T030000 RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU END:STANDARD END:VTIMEZONE And then, continues with the entries for the events:\nBEGIN:VEVENT DTSTART:20190928T100000Z DTEND:20190928T150000Z DTSTAMP:20190917T013736Z ORGANIZER;CN=YOURCOMMONNAME:mailto:YOUREMAIL UID:47chc1nab6lih8jtjs3o38mtio@google.com ATTENDEE;CUTYPE=INDIVIDUAL;ROLE=REQ-PARTICIPANT;PARTSTAT=NEEDS-ACTION;CN=CONTACTNAME;X-NUM-GUESTS=0:mailto:CONTACTEMAIL CREATED:20190914T204615Z DESCRIPTION: LAST-MODIFIED:20190915T211230Z LOCATION:LOCATIONFOREVENT SEQUENCE:0 STATUS:CONFIRMED SUMMARY:EVENTSUMMARY TRANSP:OPAQUE END:VEVENT In this case, we can see that there\u0026rsquo;s an event starting on 28th September 2019, which lasts 5 hours in \u0026lsquo;GMT\u0026rsquo; which also defines attendees (to get confirmations of assistance, etc), event location (so that you can click and use your maps app to \u0026rsquo;navigate\u0026rsquo; to it, etc)\nPython processing After some research (and trial-error), I found that icalendar library allowed to process the entries in a simple way:\n# coding: utf-8 from datetime import datetime from datetime import timedelta import requests from icalendar import Calendar, Event, vDatetime from pytz import timezone tz = timezone(\u0026#34;Europe/Madrid\u0026#34;) calendar = \u0026#34;http://www.webcal.fi/cal.php?id=191\u0026amp;format=ics\u0026amp;wrn=1\u0026amp;wp=4\u0026amp;wf=53\u0026amp;color=%23FF3100\u0026amp;cntr=es\u0026amp;lang=es\u0026amp;rid=wc\u0026#34; # Just in case we want to get calendar for another day date = datetime.now() + timedelta(days=0) datefor = \u0026#34;%s\u0026#34; % date.strftime(\u0026#34;%Y-%m-%d\u0026#34;) r = requests.get(calendar) # We use the library to \u0026#39;read\u0026#39; the url contents (text output) gcal = Calendar.from_ical(r.text) # walk the \u0026#39;VEVENTS\u0026#39; for event in gcal.walk(\u0026#34;VEVENT\u0026#34;): # Get \u0026#39;start\u0026#39; date if \u0026#34;DTSTART\u0026#34; in event: try: dtstart = event[\u0026#34;DTSTART\u0026#34;].dt.astimezone(timezone(\u0026#34;Europe/Madrid\u0026#34;)) except: dtstart = False # Get \u0026#39;stop\u0026#39; date if \u0026#34;DTEND\u0026#34; in event: try: dtend = event[\u0026#34;DTEND\u0026#34;].dt.astimezone(timezone(\u0026#34;Europe/Madrid\u0026#34;)) except: dtend = False # If we\u0026#39;ve dtstart or tend, print event information if dtstart or dtend: # This find current date (year-month-day) in the dtstart or dtend in the event for printing it if datefor in \u0026#34;%s\u0026#34; % dtstart or datefor in \u0026#34;%s\u0026#34; % dtend: print(\u0026#34;\\n📅\u0026#34;, event[\u0026#34;summary\u0026#34;]) if dtstart and dtend: lenght = (dtend - dtstart).total_seconds() / 60 else: lenght = False if lenght: print(\u0026#34;🕑 start: %s for %s minutes\u0026#34; % (dtstart, lenght)) This code was working fine, but my calendar have several events that are recurring, hence, start date or end date is not \u0026rsquo;today\u0026rsquo;, which was causing a problem, as I was getting the events for today (started/stopped), but not the recurring ones that were having \u0026lsquo;an iteration\u0026rsquo; today.\nAfter some more research, I found that dateutil library contains rrule which allows to process another field in the events:\nBEGIN:VEVENT DTSTART;TZID=Europe/Madrid:20190903T170000 DTEND;TZID=Europe/Madrid:20190903T174500 RRULE:FREQ=WEEKLY;WKST=MO;BYDAY=TH,TU DTSTAMP:20190917T013736Z ORGANIZER;CN=YOURNAME:mailto:GROUPEMAIL UID:3rhualb4j1kl2jdpemsd39o668@google.com ATTENDEE;CUTYPE=INDIVIDUAL;ROLE=REQ-PARTICIPANT;PARTSTAT=ACCEPTED;CN=ATTENDEENAME;X-NUM-GUESTS=0:mailto:ATTENDEEMAIL CREATED:20190617T073502Z DESCRIPTION: LAST-MODIFIED:20190902T105136Z LOCATION:EVENTLOCATION SEQUENCE:0 STATUS:CONFIRMED SUMMARY:EVENT SUMMARY TRANSP:OPAQUE END:VEVENT This event, starts and ends on 3rd September, so in the above code it was not showing, however, we can see that it has a RRULE field which says that happens weekly, with repeating days on Thursdays and Tuesdays.\nRrule allows to process it:\n# \u0026lt;snip\u0026gt; import dateutil.rrule as rrule if \u0026#34;RRULE\u0026#34; in event: # This gets the rule as \u0026#39;string\u0026#39; so that it can be processed ruletext = event[\u0026#34;RRULE\u0026#34;].to_ical().decode() # This defines the rule object, with the rule text and using the initial date for the event rule = rrule.rrulestr(ruletext, dtstart=event[\u0026#34;DTSTART\u0026#34;].dt) # This is the interesting part, where, based on actual date and rule for repetition, calculates \u0026#39;next\u0026#39; occurrence for the event nextrule = rule.after(datetime.now().astimezone(timezone(\u0026#34;Europe/Madrid\u0026#34;))) # \u0026lt;snip\u0026gt; Wrapping-it-up So, with the above process \u0026lsquo;in place\u0026rsquo;, the final code is similar to this:\n# coding: utf-8 from datetime import datetime from datetime import timedelta import requests from icalendar import Calendar, Event, vDatetime from pytz import timezone import dateutil.rrule as rrule format = \u0026#34;%Y-%m-%d %H:%M:%S %Z%z\u0026#34; tz = timezone(\u0026#34;Europe/Madrid\u0026#34;) calendars = [] calendars.append( \u0026#34;http://www.webcal.fi/cal.php?id=191\u0026amp;format=ics\u0026amp;wrn=1\u0026amp;wp=4\u0026amp;wf=53\u0026amp;color=%23FF3100\u0026amp;cntr=es\u0026amp;lang=es\u0026amp;rid=wc\u0026#34; ) date = datetime.now() + timedelta(days=0) datefor = \u0026#34;%s\u0026#34; % date.strftime(\u0026#34;%Y-%m-%d\u0026#34;) for calendar in calendars: r = requests.get(calendar) gcal = Calendar.from_ical(r.text) for event in gcal.walk(\u0026#34;VEVENT\u0026#34;): if \u0026#34;DTSTART\u0026#34; in event: try: dtstart = event[\u0026#34;DTSTART\u0026#34;].dt.astimezone(timezone(\u0026#34;Europe/Madrid\u0026#34;)) except: dtstart = False if \u0026#34;DTEND\u0026#34; in event: try: dtend = event[\u0026#34;DTEND\u0026#34;].dt.astimezone(timezone(\u0026#34;Europe/Madrid\u0026#34;)) except: dtend = False if \u0026#34;RRULE\u0026#34; in event: try: ruletext = event[\u0026#34;RRULE\u0026#34;].to_ical().decode() rule = rrule.rrulestr(ruletext, dtstart=event[\u0026#34;DTSTART\u0026#34;].dt) nextrule = rule.after(date.astimezone(timezone(\u0026#34;Europe/Madrid\u0026#34;))) except: nextrule = False else: nextrule = False if dtstart or dtend or nextrule: if ( datefor in \u0026#34;%s\u0026#34; % dtstart or datefor in \u0026#34;%s\u0026#34; % dtend or datefor in \u0026#34;%s\u0026#34; % nextrule ): print(\u0026#34;\\n📅\u0026#34;, event[\u0026#34;summary\u0026#34;]) if dtstart and dtend: lenght = (dtend - dtstart).total_seconds() / 60 else: lenght = False if not nextrule: if lenght: print(\u0026#34;🕑 start: %s for %s minutes\u0026#34; % (dtstart, lenght)) else: print(\u0026#34;🕑 start: %s for %s minutes\u0026#34; % (nextrule, lenght)) This code is still not complete, as it for sure, lists \u0026lsquo;single\u0026rsquo; events happening today, or \u0026lsquo;recurring\u0026rsquo; events happening also today, but it doesn\u0026rsquo;t take into consideration \u0026lsquo;Excludes\u0026rsquo; to those rules, like for example, when a recurring event is cancelled for a specific date, etc.\nHowever, for a first iteration, code works and now the @redken_bot can also remind you on your day events on multiple calendars :) Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/09/17/python-and-icalendar-ics-processing/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eiCalendar (\u003ccode\u003e.ics\u003c/code\u003e or \u003ccode\u003ewebcal\u003c/code\u003e) is a standard for providing \u0026lsquo;calendar\u0026rsquo; information over text file, it allows to define events, etc and it\u0026rsquo;s ideal to \u0026lsquo;subscribe\u0026rsquo; over the internet to remote calendars, which is commonly used to show free/busy slots for scheduling meetings, etc.\u003c/p\u003e\n\u003cp\u003eIf you\u0026rsquo;re familiar with \u003ccode\u003eGoogle Calendar\u003c/code\u003e or others, it uses \u0026lsquo;ics\u0026rsquo; under the hood, and you can get full url for it so that it can be shared.\u003c/p\u003e","title":"Python and iCalendar ICS processing"},{"content":"This article was published originally at https://metal3.io/blog/2019/09/11/Baremetal-operator.html\nIntroduction The baremetal operator, documented at https://github.com/metal3-io/baremetal-operator/blob/master/docs/api.md, it\u0026rsquo;s the Operator in charge of definitions of physical hosts, containing information about how to reach the Out of Band management controller, URL with the desired image to provision, plus other properties related with hosts being used for provisioning instances.\nQuoting from the project:\nThe Bare Metal Operator implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available hosts as instances of the BareMetalHost Custom Resource Definition. The Bare Metal Operator knows how to: Inspect the host\u0026rsquo;s hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more. Provision hosts with a desired image Clean a host\u0026rsquo;s disk contents before or after provisioning.\nA bit more in deep approach The Baremetal Operator (BMO) keeps a mapping of each host and its management interfaces (vendor based like iLO, iDrac, iRMC, etc) and controlled via IPMI.\nAll of this is defined in a CRD, for example:\napiVersion: v1 kind: Secret metadata: name: metal3-node01-credentials namespace: metal3 type: Opaque data: username: YWRtaW4= password: YWRtaW4= --- apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata: name: metal3-node01 namespace: metal3 spec: bmc: address: ipmi://172.22.0.2:6230 credentialsName: metal3-node01-credentials bootMACAddress: 00:c2:fc:3b:e1:01 description: \u0026#34;\u0026#34; hardwareProfile: \u0026#34;libvirt\u0026#34; online: false With the above values (described in API), we\u0026rsquo;re telling the operator:\nMAC: Defines the MAC address of the NIC connected to the network that will be used for provision the host bmc: defines the management controller address and the secret used credentialsName: Defines the name of the secret containing username/password for accessing the IPMI service Once the server is \u0026lsquo;defined\u0026rsquo; via the CRD, the underlying service (provided by ironic1 as of this writing) is inspected:\n[root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3 NAME STATUS PROVISIONING STATUS CONSUMER BMC HARDWARE PROFILE ONLINE ERROR metal3-node01 OK inspecting ipmi://172.22.0.1:6230 false Once the inspection has finished, the status will change to ready and made available for provisioning.\nWhen we define a machine, we refer to the images that will be used for the actual provisioning in the CRD (image):\napiVersion: v1 data: userData: DATA kind: Secret metadata: name: metal3-node01-user-data namespace: metal3 type: Opaque --- apiVersion: \u0026#34;cluster.k8s.io/v1alpha1\u0026#34; kind: Machine metadata: name: metal3-node01 namespace: metal3 generateName: baremetal-machine- spec: providerSpec: value: apiVersion: \u0026#34;baremetal.cluster.k8s.io/v1alpha1\u0026#34; kind: \u0026#34;BareMetalMachineProviderSpec\u0026#34; image: url: http://172.22.0.2/images/CentOS-7-x86_64-GenericCloud-1901.qcow2 checksum: http://172.22.0.2/images/CentOS-7-x86_64-GenericCloud-1901.qcow2.md5sum userData: name: metal3-node01-user-data namespace: metal3 [root@metal3-kubernetes ~]# kubectl create -f metal3-node01-machine.yml secret/metal3-node01-user-data created machine.cluster.k8s.io/metal3-node01 created Let\u0026rsquo;s examine the annotation created when provisioning (metal3.io/BareMetalHost):\n[root@metal3-kubernetes ~]# kubectl get machine -n metal3 metal3-node01 -o yaml apiVersion: cluster.k8s.io/v1alpha1 kind: Machine metadata: annotations: metal3.io/BareMetalHost: metal3/metal3-node01 creationTimestamp: \u0026#34;2019-07-08T15:30:44Z\u0026#34; finalizers: - machine.cluster.k8s.io generateName: baremetal-machine- generation: 2 name: metal3-node01 namespace: metal3 resourceVersion: \u0026#34;6222\u0026#34; selfLink: /apis/cluster.k8s.io/v1alpha1/namespaces/metal3/machines/metal3-node01 uid: 1bfd384a-5467-43b7-98aa-e80e1ace5ce7 spec: metadata: creationTimestamp: null providerSpec: value: apiVersion: baremetal.cluster.k8s.io/v1alpha1 image: checksum: http://172.22.0.1/images/CentOS-7-x86_64-GenericCloud-1901.qcow2.md5sum url: http://172.22.0.1/images/CentOS-7-x86_64-GenericCloud-1901.qcow2 kind: BareMetalMachineProviderSpec userData: name: metal3-node01-user-data namespace: metal3 versions: kubelet: \u0026#34;\u0026#34; status: addresses: - address: 192.168.122.79 type: InternalIP - address: 172.22.0.39 type: InternalIP - address: localhost.localdomain type: Hostname lastUpdated: \u0026#34;2019-07-08T15:30:44Z\u0026#34; Note\nIn the output above, the host assigned was the one we\u0026rsquo;ve defined earlier as well as the other parameters like IP\u0026rsquo;s, etc generated.\nNow, if we check the baremetal hosts, we can see how it\u0026rsquo;s getting provisioned:\n[root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3 NAME STATUS PROVISIONING STATUS CONSUMER BMC HARDWARE PROFILE ONLINE ERROR metal3-node01 OK provisioned ipmi://172.22.0.1:6230 true And also, check it via the ironic command:\n[root@metal3-kubernetes ~]# export OS_TOKEN=fake-token ; export OS_URL=http://localhost:6385 ; openstack baremetal node list +--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+ | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance | +--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+ | 7551cfb4-d758-4ad8-9188-859ee53cf298 | metal3-node01 | 7551cfb4-d758-4ad8-9188-859ee53cf298 | power on | active | False | +--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+ Wrap-up We\u0026rsquo;ve seen how via a CRD we\u0026rsquo;ve defined credentials for a baremetal host to make it available to get provisioned and how we\u0026rsquo;ve also defined a machine that was provisioned on top of that baremetal host. Enjoy! (and if you do, you can Buy Me a Coffee ) Ironic was chosen as the initial provider for baremetal provisioning, check Ironic documentation for more details about Ironic usage in Metal³\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iranzo.io/eko/2019-09-11-baremetal-operator/","summary":"\u003cp\u003eThis article was published originally at \u003ca href=\"https://metal3.io/blog/2019/09/11/Baremetal-operator.html\"\u003ehttps://metal3.io/blog/2019/09/11/Baremetal-operator.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href=\"https://github.com/metal3-io/baremetal-operator/\"\u003ebaremetal operator\u003c/a\u003e, documented at \u003ca href=\"https://github.com/metal3-io/baremetal-operator/blob/master/docs/api.md\"\u003ehttps://github.com/metal3-io/baremetal-operator/blob/master/docs/api.md\u003c/a\u003e, it\u0026rsquo;s the Operator in charge of definitions of physical hosts, containing information about how to reach the Out of Band management controller, URL with the desired image to provision, plus other properties related with hosts being used for provisioning instances.\u003c/p\u003e\n\u003cp\u003eQuoting from the project:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe Bare Metal Operator implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available hosts as instances of the BareMetalHost Custom Resource Definition. The Bare Metal Operator knows how to:\nInspect the host\u0026rsquo;s hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more.\nProvision hosts with a desired image\nClean a host\u0026rsquo;s disk contents before or after provisioning.\u003c/p\u003e","title":"Baremetal Operator"},{"content":"Introduction Lego Dimensions is a game for some platforms (Xbox, Wii U, PlayStation) which adds a base with NFC reader and colour LED that allows to interact with the video game from the real world.\nThe starter pack \u0026lsquo;starter pack\u0026rsquo; has minifigs for Supercool, Batman and Gandalf plus additional blocks to \u0026lsquo;decorate\u0026rsquo; the base by building a portal.\nThe main story is based on how those characters (Gandalf, Batman and Supercool) meet and attempt to find and rescue their friends while visiting several worlds.\nThe game itself, mixing both on screen play plus \u0026lsquo;real world\u0026rsquo; play works via NFC interaction with tags provided for each minifigure, vehicle or gadget, allowing console to recognize which one is on the base and in which position.\nNot only the base is used to add characters or vehicles to the game, it\u0026rsquo;s also used for playing, for example, freeing characters when moving them in the base, or transporting to different areas on screen, or even succeed at a puzzle moving the same character in a sequence through the base positions.\nFor enhancing the game, there are additional packs classified in story pack, promotional pack, team pack, fun pack or level pack which add new worlds, levels or characters depending of the different types.\nPacks with vehicles or gadgets come with additional pieces that allows improving it as we get through the story and getting new instructions on screen on how to build them, being finally the console via the NFC base the one writing to the NFC Tag the improved transformation.\nStarter pack Those are the starter packs depending on the platform (as it includes the game itself):\nMicrosoft Xbox 🛒#ad Microsoft Xbox 360 🛒#ad Sony PlayStation 4 (ps4) 🛒#ad Sony PlayStation 3 (ps3) 🛒#ad Nintendo Wii U 🛒#ad Promotional pack The Promotional pack adds more characters and vehicles and came with some promotions like console for a specific platform, etc:\nLego ID Type Name and link Image 71340 Promotional pack Supergirl 🛒#ad 71342 Promotional pack Green Arrow 🛒#ad Story pack Story pack (for all platforms) add a new portal and new play levels:\nLego ID Type Name and link Image 71242 Story pack Ghostbusters 🛒#ad 71253 Story pack Fantastic beasts 🛒#ad 71264 Story pack Batman Movie 🛒#ad Level pack The Level Pack pack add new level and a new story:\nLego ID Type Name and link Image 71201 Level Pack Back to the Future 🛒#ad 71202 Level Pack The Simpsons 🛒#ad 71203 Level Pack Portal 2 🛒#ad 71204 Level Pack Dr. Who 🛒#ad 71228 Level Pack GhostBusters 🛒#ad 71235 Level Pack Midway 🛒#ad 71244 Level Pack Sonic The Hedgehog 🛒#ad 71245 Level Pack Adventure Time 🛒#ad 71248 Level Pack Mission Impossible 🛒#ad 71267 Level Pack The Goonies 🛒#ad Team pack The Team pack add more characters and vehicles:\nLego ID Type Name and link Image 71205 Team pack Jurassic World 🛒#ad 71206 Team pack Scooby-Doo 🛒#ad 71207 Team pack Ninjago 🛒#ad 71229 Team pack DC Comics 🛒#ad 71246 Team pack Adventure Time 🛒#ad 71247 Team pack Harry Potter 🛒#ad 71255 Team pack Teen Titans Go! 🛒#ad 71256 Team pack Gremlins 🛒#ad 71346 Team pack Powerpuff girls 🛒#ad Fun pack Fun Pack add character and a gadget or vehicle:\nLego ID Type Name and link Image 71209 Fun pack Wonder Woman 🛒#ad 71210 Fun pack Cyborg 🛒#ad 71211 Fun pack Bart Simpson 🛒#ad 71212 Fun pack Emmet 🛒#ad 71213 Fun pack Bad Cop 🛒#ad 71214 Fun pack Benny 🛒#ad 71215 Fun pack Ninjago - Jay 🛒#ad 71216 Fun pack Ninjago - Nya 🛒#ad 71217 Fun pack Ninjago - Zane 🛒#ad 71218 Fun pack Gollum 🛒#ad 71219 Fun pack Legolas 🛒#ad 71220 Fun pack Gimli 🛒#ad 71221 Fun pack Wicked witch 🛒#ad 71222 Fun pack Chima - Laval 🛒#ad 71223 Fun pack Chima Cragger 🛒#ad 71227 Fun pack Krusty 🛒#ad 71230 Fun pack Doc Brown 🛒#ad 71231 Fun pack Unikitty 🛒#ad 71232 Fun pack Chima - Eris 🛒#ad 71233 Fun pack Stay Puft 🛒#ad 71234 Fun pack Ninjago - Sensei Wu 🛒#ad 71236 Fun pack Superman 🛒#ad 71237 Fun pack Aquaman 🛒#ad 71238 Fun pack Cyberman 🛒#ad 71239 Fun pack Ninjago - Lloyd 🛒#ad 71240 Fun pack DC Bane 🛒#ad 71241 Fun pack Slimer 🛒#ad 71251 Fun pack A-Team 🛒#ad 71257 Fun pack Fantastic Beasts - Tina 🛒#ad 71258 Fun pack E.T. 71266 Fun pack Lego City - Chase McCain 🛒#ad 71285 Fun pack Adventure Time - Marceline the Vampire Queen 🛒#ad 71286 Fun pack Knight Rider 🛒#ad 71287 Fun pack Teen Titans GO! 🛒#ad 71343 Fun pack Powerpuff Girls - Buttercup 🛒#ad 71344 Fun pack Excalibur Batman 🛒#ad 71348 Fun pack Harry Potter - Hermione Granger 🛒#ad 71349 Fun pack Beetlejuice 🛒#ad The video game Game shares the Lego methodology for other games: pickup coins and break objects to find more coins or things to build for making progress in the game, but also combined with special abilities like some objects being only breakable by specific characters, etc.\nIf a character is required but not owned, it\u0026rsquo;s possible to rent by using some of the earned pieces that character for some time interval, allowing us to complete certain tasks and keep progressing in the stories. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/08/19/lego-dimensions-review/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLego Dimensions is a game for some platforms (Xbox, Wii U, PlayStation)\nwhich adds a base with NFC reader and colour LED that allows to interact\nwith the video game from the real world.\u003c/p\u003e\n\u003cp\u003eThe starter pack \u0026lsquo;starter pack\u0026rsquo; has minifigs for Supercool, Batman and\nGandalf plus additional blocks to \u0026lsquo;decorate\u0026rsquo; the base by building a portal.\u003c/p\u003e\n\u003cp\u003eThe main story is based on how those characters (Gandalf, Batman and\nSupercool) meet and attempt to find and rescue their friends while visiting\nseveral \u003ccode\u003eworlds\u003c/code\u003e.\u003c/p\u003e","title":"Lego Dimensions Review"},{"content":"After my post about killercoda, I did split my initial scenarios into \u0026lsquo;organizations\u0026rsquo;.\nOne of them, is in progress to get contributed upstream to KubeVirt project killercoda (still getting some reviews to land on final repo), and the other is under Citellus organization.\nAs the goal was not to lose visits using the prior links, I contacted the team behind killercoda Support (thanks a lot Ben!!) and the requirements to get a \u0026lsquo;redirect\u0026rsquo; in place is to:\nRemove the old folders in the killercoda-scenarios repo for your account Place in the root folder a file redirect.json containing: [ { \u0026#34;scenario\u0026#34;: \u0026#34;kubevirt\u0026#34;, \u0026#34;targetScenario\u0026#34;: \u0026#34;kubevirt\u0026#34;, \u0026#34;targetUsername\u0026#34;: \u0026#34;kubevirt\u0026#34; }, { \u0026#34;scenario\u0026#34;: \u0026#34;citellus\u0026#34;, \u0026#34;targetScenario\u0026#34;: \u0026#34;citellus\u0026#34;, \u0026#34;targetUsername\u0026#34;: \u0026#34;citellus\u0026#34; } ] This file instructs to redirect \u0026lsquo;scenario\u0026rsquo; kubevirt to another scenario (same name here), but on another username (kubevirt organization) and likewise for citellus.\nThe point here, is that old visitors from my prior post, going to either:\nHow to use Citellus KubeVirt will get instead to the updated URL for both scenarios, losing no visitors and ensuring that no \u0026lsquo;old\u0026rsquo; copies are around.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/07/31/killercoda-scenario-redirection/","summary":"\u003cp\u003eAfter my post about \u003ca href=\"/blog/2019/06/11/killercoda-scenario-creation/\"\u003ekillercoda\u003c/a\u003e, I did split my initial scenarios into \u0026lsquo;organizations\u0026rsquo;.\u003c/p\u003e\n\u003cp\u003eOne of them, is in progress to get contributed upstream to \u003ca href=\"https://killercoda.com/kubevirt\"\u003eKubeVirt project killercoda\u003c/a\u003e (still getting some reviews to land on final repo), and the other is under \u003ca href=\"https://killercoda.com/citellus\"\u003eCitellus organization\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAs the goal was not to lose visits using the prior links, I contacted the team behind killercoda Support (thanks a lot Ben!!) and the requirements to get a \u0026lsquo;redirect\u0026rsquo; in place is to:\u003c/p\u003e","title":"Killercoda scenario redirection"},{"content":"I\u0026rsquo;ve uploaded those pics from the model, as it seems that people likes it.\nThe model is Lego 10242 Mini Cooper 🛒#ad and the pics are for the standard instructions build.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/07/27/lego-10242-mini-cooper-lego-creator/","summary":"\u003cp\u003eI\u0026rsquo;ve uploaded those pics from the model, as it seems that people likes it.\u003c/p\u003e\n\u003cp\u003eThe model is \u003ca href=\"https://www.amazon.es/dp/B00M0ETSWU?tag=redken-21\"\u003eLego 10242 Mini Cooper 🛒#ad\u003c/a\u003e\nand the pics are for the standard instructions build.\u003c/p\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/ewbaE5bt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/ewbaE5bt.jpg\" alt=\"Side view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/ewbaE5b.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/ndvDhi4t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/ndvDhi4t.jpg\" alt=\"Aerial view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/ndvDhi4.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/ocHIcX3t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/ocHIcX3t.jpg\" alt=\"Motor view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/ocHIcX3.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/nkJ37xkt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/nkJ37xkt.jpg\" alt=\"Trunk with replacement wheel area open\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/nkJ37xk.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/RN3zp86t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/RN3zp86t.jpg\" alt=\"Trunk with picnic kit\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/RN3zp86.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/WZ5VIk8t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/WZ5VIk8t.jpg\" alt=\"Picnic kit\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/WZ5VIk8.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/s0t0PPbt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/s0t0PPbt.jpg\" alt=\"Steering wheel\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/s0t0PPb.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/XS8MGtvt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/XS8MGtvt.jpg\" alt=\"Front seat moving to access rear seats\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/XS8MGtv.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego 10242 - Mini Cooper Lego Creator"},{"content":"This is the pictures of the set that my colleague Raúl brought back from his trip to APAC.\nThe set, includes a mechanism that makes the dancers to move up and down and the lantern hold by the pig to rotate.\nModel is 80102 Dragon Dance 🛒#ad\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/06/28/lego-80102-dragon-dance/","summary":"\u003cp\u003eThis is the pictures of the set that my colleague Raúl brought back from his trip to APAC.\u003c/p\u003e\n\u003cp\u003eThe set, includes a mechanism that makes the dancers to move up and down and the lantern hold by the pig to rotate.\u003c/p\u003e\n\u003cp\u003eModel is \u003ca href=\"https://www.amazon.es/dp/B07KRJJFY8?tag=redken-21\"\u003e80102 Dragon Dance 🛒#ad\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/8j1Drm8t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/8j1Drm8t.jpg\" alt=\"Side view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/8j1Drm8.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/y72cWUSt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/y72cWUSt.jpg\" alt=\"Rear view details\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/y72cWUS.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/EOD2WXWt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/EOD2WXWt.jpg\" alt=\"Minifigures detail\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/EOD2WXW.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/XCjG4TRt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/XCjG4TRt.jpg\" alt=\"Front minifigure\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/XCjG4TR.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/sY9a1Qdt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/sY9a1Qdt.jpg\" alt=\"Year of the pig figure\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/sY9a1Qd.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\n\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego 80102 - Dragon dance"},{"content":"This is the pictures of the set that my colleague Raúl brought back from his trip to APAC, hope you enjoy it!\nThere are lot of printed pieces that look very nice, even the faces have been suited for the region.\nModel is 80101 Chinese New\u0026rsquo;s Year Eve Dinner 🛒#ad.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/06/28/lego-80101-chinese-new-years-eve/","summary":"\u003cp\u003eThis is the pictures of the set that my colleague Raúl brought back from his trip to APAC, hope you enjoy it!\u003c/p\u003e\n\u003cp\u003eThere are lot of printed pieces that look very nice, even the faces have been suited for the region.\u003c/p\u003e\n\u003cp\u003eModel is \u003ca href=\"https://www.amazon.es/dp/B07KRFLDLN?tag=redken-21\"\u003e80101 Chinese New\u0026rsquo;s Year Eve Dinner 🛒#ad\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/xIIP1ERt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/xIIP1ERt.jpg\" alt=\"Front view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/xIIP1ER.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/BjgRRlCt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/BjgRRlCt.jpg\" alt=\"Grandma and kid\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/BjgRRlC.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/Gmx6LsMt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/Gmx6LsMt.jpg\" alt=\"Table food\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/Gmx6LsM.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/zn9Y7tOt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/zn9Y7tOt.jpg\" alt=\"Grandpa and mother\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/zn9Y7tO.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/XkDeqSUt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/XkDeqSUt.jpg\" alt=\"Family pictures and bookshelf\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/XkDeqSU.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/wLlgJmjt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/wLlgJmjt.jpg\" alt=\"Little girl smiling\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/wLlgJmj.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/kFHfF5st.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/kFHfF5st.jpg\" alt=\"Couch and corner with the window blinds and curtains\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/kFHfF5s.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/mgpsRrTt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/mgpsRrTt.jpg\" alt=\"Entrance and bookshelf\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/mgpsRrT.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/BSoWyVVt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/BSoWyVVt.jpg\" alt=\"Entry door details\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/BSoWyVV.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/0GDUcbWt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/0GDUcbWt.jpg\" alt=\"Aerial view\"/\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/0GDUcbW.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\n\u003cp\u003e\n\n\n\n  \n\n\n\u003cscript src=\"https://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=/js/load-photoswipe.js\u003e\u003c/script\u003e\n\n\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css\" integrity=\"sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=\" crossorigin=\"anonymous\" /\u003e\n\u003clink rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css\" integrity=\"sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=\" crossorigin=\"anonymous\" /\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js\" integrity=\"sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js\" integrity=\"sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n\n\n\u003cdiv class=\"pswp\" tabindex=\"-1\" role=\"dialog\" aria-hidden=\"true\"\u003e\n\n\u003cdiv class=\"pswp__bg\"\u003e\u003c/div\u003e\n\n\u003cdiv class=\"pswp__scroll-wrap\"\u003e\n    \n    \u003cdiv class=\"pswp__container\"\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n      \u003cdiv class=\"pswp__item\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \n    \u003cdiv class=\"pswp__ui pswp__ui--hidden\"\u003e\n    \u003cdiv class=\"pswp__top-bar\"\u003e\n      \n      \u003cdiv class=\"pswp__counter\"\u003e\u003c/div\u003e\n      \u003cbutton class=\"pswp__button pswp__button--close\" title=\"Close (Esc)\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--share\" title=\"Share\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--fs\" title=\"Toggle fullscreen\"\u003e\u003c/button\u003e\n      \u003cbutton class=\"pswp__button pswp__button--zoom\" title=\"Zoom in/out\"\u003e\u003c/button\u003e\n      \n      \n      \u003cdiv class=\"pswp__preloader\"\u003e\n        \u003cdiv class=\"pswp__preloader__icn\"\u003e\n          \u003cdiv class=\"pswp__preloader__cut\"\u003e\n            \u003cdiv class=\"pswp__preloader__donut\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"pswp__share-modal pswp__share-modal--hidden pswp__single-tap\"\u003e\n      \u003cdiv class=\"pswp__share-tooltip\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--left\" title=\"Previous (arrow left)\"\u003e\n    \u003c/button\u003e\n    \u003cbutton class=\"pswp__button pswp__button--arrow--right\" title=\"Next (arrow right)\"\u003e\n    \u003c/button\u003e\n    \u003cdiv class=\"pswp__caption\"\u003e\n      \u003cdiv class=\"pswp__caption__center\"\u003e\u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Lego 80101 - Chinese New Year's Eve"},{"content":"I was using from time ago this extension for Gnome named Argos which allows to create useful data views using scripts written in whatever language of choice available, bash included.\nSo far I had just a simple extension writing a UNICODE character, but with the suggestion of my colleague Javi Ramírez, I decided to use the motivational idea to write a simple script to calculate how much you\u0026rsquo;ve earned so far in the day and show that information at a glance on your menu bar.\nFile is available here, and must be named like showmethemoney.1s.sh inside your argos folder.\nContent is very simple:\n#!/usr/bin/env bash # Author: Pablo Iranzo Gómez (Pablo.Iranzo@gmail.com) # Description: Script to calculate earned money so far in the day # Adjust DAILY to the daily income, adjust DAYSTART hour and DAYEND hour to your working schedule # Customize to suit your details DAYSTART=\u0026#34;8:00\u0026#34; DAYEND=\u0026#34;17:00\u0026#34; DAILY=1000 SYMBOL=\u0026#34;€\u0026#34; URL=\u0026#34;github.com/p-e-w/argos\u0026#34; DIR=$(dirname \u0026#34;$0\u0026#34;) EARNED=\u0026#34;\u0026#34; DAILYTOTAL=$(echo \u0026#34;scale=4; $DAILY/(9*60*60)\u0026#34;| bc) UNIXSTART=$(date --date=\u0026#34;${DAYSTART}\u0026#34; +\u0026#34;%s\u0026#34;) UNIXNOW=$(date +\u0026#34;%s\u0026#34;) UNIXEND=$(date --date=\u0026#34;${DAYEND}\u0026#34; +\u0026#34;%s\u0026#34;) if [[ ${UNIXNOW} -gt ${UNIXEND} ]]; then EARNED=${DAILY} elif [[ ${UNIXNOW} -gt ${UNIXSTART} ]]; then EARNED=$(echo \u0026#34;scale=2; $DAILYTOTAL * (${UNIXNOW}-${UNIXSTART})\u0026#34; | bc) else EARNED=\u0026#34;0\u0026#34; fi STRING=\u0026#34;💰 ${EARNED}${SYMBOL} 💰\u0026#34; echo \u0026#34;$STRING | refresh=true\u0026#34; echo \u0026#34;---\u0026#34; echo \u0026#34;$URL | iconName=help-faq-symbolic href=\u0026#39;https://$URL\u0026#39;\u0026#34; echo \u0026#34;$DIR | iconName=folder-symbolic href=\u0026#39;file://$DIR\u0026#39;\u0026#34; It checks if day has started or it\u0026rsquo;s over and if in between, it calculates the amount based on Unix epoch.\nFor more details about developing them, check argos link above!\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/06/27/argos-extension-writing-for-gnome3/","summary":"\u003cp\u003eI was using from time ago this extension for Gnome named \u003ca href=\"https://github.com/p-e-w/argos\"\u003eArgos\u003c/a\u003e which allows to create useful data views using scripts written in whatever language of choice available, bash included.\u003c/p\u003e\n\u003cp\u003eSo far I had just a simple extension writing a UNICODE character, but with the suggestion of my colleague \u003ca href=\"http://www.sombrerorojo.com/\"\u003eJavi Ramírez\u003c/a\u003e, I decided to use the motivational idea to write a simple script to calculate how much you\u0026rsquo;ve earned so far in the day and show that information at a glance on your menu bar.\u003c/p\u003e","title":"Argos extension writing for Gnome3"},{"content":"After some time checking the scenarios at https://learn.openshift.com, I decided to give it a try.\nWith the help of Mario Vázquez, author of Getting Started with Kubefed, I did create two scenarios:\nHow to use Citellus on Citellus: Troubleshooting automation KubeVirt on a \u0026lsquo;browser-based\u0026rsquo; approach for MiniKube setup for validating KubeVirt: Kubernetes with VM Virtualization (versus the regular containers). You can check how them can be created by looking at their code at: killercoda Scenarios or the \u0026lsquo;playable\u0026rsquo; version at https://killercoda.com/iranzo/.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/06/11/killercoda-scenario-creation/","summary":"\u003cp\u003eAfter some time checking the scenarios at \u003ca href=\"https://learn.openshift.com\"\u003ehttps://learn.openshift.com\u003c/a\u003e, I decided to give it a try.\u003c/p\u003e\n\u003cp\u003eWith the help of \u003ca href=\"https://linuxera.org\"\u003eMario Vázquez\u003c/a\u003e, author of \u003ca href=\"https://learn.openshift.com/introduction/federated-clusters/\"\u003eGetting Started with Kubefed\u003c/a\u003e, I did create two scenarios:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://killercoda.com/citellus/citellus\"\u003eHow to use Citellus\u003c/a\u003e on \u003ca href=\"https://risuorg.github.io\"\u003eCitellus: Troubleshooting automation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://killercoda.com/iranzo/kubevirt\"\u003eKubeVirt\u003c/a\u003e on a \u0026lsquo;browser-based\u0026rsquo; approach for \u003ca href=\"https://kubevirt.io/quickstart_minikube/\"\u003eMiniKube\u003c/a\u003e setup for validating KubeVirt: Kubernetes with VM Virtualization (versus the regular containers).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can check how them can be created by looking at their code at: \u003ca href=\"https://github.com/iranzo/katacoda-scenarios\"\u003ekillercoda Scenarios\u003c/a\u003e or the \u0026lsquo;playable\u0026rsquo; version at \u003ca href=\"https://killercoda.com/iranzo/\"\u003ehttps://killercoda.com/iranzo/\u003c/a\u003e.\u003c/p\u003e","title":"Killercoda scenario creation"},{"content":"After seeing this MOC 75894-mini-transporter, I decided to:\nBuy the model Lego 75894 - 1967 Mini Cooper S Rally and 2018 Mini John Cooper Works Buggy 🛒#ad Buy the instructions at the MOC URL After several times trying to pay for the instructions (which I was unable to get because PayPal complained on the web site), I decided to make my own based on what I could see, and the result, are the images you can see, the initial model which had a small driving area and no minifig could sit there and the final one which added some more options like sizing cabin for allowing a minifigure to drive, plus some other alternatives for the front facing engine area and better holding of the rear wheels area.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/06/09/lego-75894-1967-mini-cooper-s-rally-and-buggy-moc-adaptation-as-mini-transporter/","summary":"\u003cp\u003eAfter seeing this MOC \u003ca href=\"https://rebrickable.com/mocs/MOC-24636/Keep%20On%20Bricking/75894-mini-transporter/\"\u003e75894-mini-transporter\u003c/a\u003e, I decided to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuy the model \u003ca href=\"https://www.amazon.es/dp/B07FNTMWMT?tag=redken-21\"\u003eLego 75894 - 1967 Mini Cooper S Rally and 2018 Mini John Cooper Works Buggy 🛒#ad\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eBuy the instructions at the MOC URL\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter several times trying to pay for the instructions (which I was unable to get because\nPayPal complained on the web site), I decided to make my own based on what I could see, and the result, are the images you can see, the initial model which had a small driving area and no minifig could sit there and the final one which added some more options like sizing cabin for allowing a minifigure to \u003ccode\u003edrive\u003c/code\u003e, plus some other alternatives for the front facing engine area and better holding of the rear wheels area.\u003c/p\u003e","title":"Lego 75894 - 1967 Mini Cooper S Rally and Buggy MOC adaptation as Mini Transporter"},{"content":"As a recipe, if you want to enable a custom domain name on blog-o-matic a special file needs to be created on the \u0026lsquo;GitHub Pages\u0026rsquo; served \u0026lsquo;master\u0026rsquo; branch.\nIn order to do so, edit pelicanconf.py and add the following differences:\ndiff --git a/pelicanconf.py b/pelicanconf.py index 680abcb..fc3dd8f 100644 --- a/pelicanconf.py +++ b/pelicanconf.py @@ -46,13 +46,16 @@ AMAZON_ONELINK = \u0026#34;b63a2115-85f7-43a9-b169-5f4c8c275655\u0026#34; # Extra files customization -EXTRA_PATH_METADATA = {} +EXTRA_PATH_METADATA = { + \u0026#39;extra/CNAME\u0026#39;: {\u0026#39;path\u0026#39;: \u0026#39;CNAME\u0026#39;}, +} + EXTRA_TEMPLATES_PATHS = [ \u0026#34;plugins/revealmd/templates\u0026#34;, # eg: \u0026#34;plugins/revealmd/templates\u0026#34; ] -STATIC_PATHS = [ \u0026#39;images\u0026#39; ] +STATIC_PATHS = [ \u0026#39;images\u0026#39; , \u0026#39;extra\u0026#39;] ## ONLY TOUCH IF YOU KNOW WHAT YOU\u0026#39;RE DOING! This will copy the CNAME file created in content/extra/CNAME to the resulting \u0026lsquo;master\u0026rsquo; branch as /CNAME.\nThis file is interpreted by Github pages server as the domain name to listen for, so your website will start to be available from it (supposing that you followed usual requirements):\nyourcustomdomain.es.\t1\tIN\tA\t185.199.108.153 yourcustomdomain.es.\t1\tIN\tA\t185.199.109.153 yourcustomdomain.es.\t1\tIN\tA\t185.199.110.153 yourcustomdomain.es.\t1\tIN\tA\t185.199.111.153 Please, do review if the serving servers have been updated on github pages! Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/05/16/use-custom-domain-name-with-blog-o-matic/","summary":"\u003cp\u003eAs a recipe, if you want to enable a custom domain name on \u003ca href=\"/blog/2019/01/09/blog-o-matic-quickly-get-a-github-hosted-blog-with-pelican-elegant-with-little-setup-steps./\"\u003eblog-o-matic\u003c/a\u003e a special file needs to be created on the \u0026lsquo;GitHub Pages\u0026rsquo; served \u0026lsquo;master\u0026rsquo; branch.\u003c/p\u003e\n\u003cp\u003eIn order to do so, edit \u003ccode\u003epelicanconf.py\u003c/code\u003e and add the following differences:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-diff\" data-lang=\"diff\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ediff --git a/pelicanconf.py b/pelicanconf.py\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eindex 680abcb..fc3dd8f 100644\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e--- a/pelicanconf.py\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003e+++ b/pelicanconf.py\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e@@ -46,13 +46,16 @@ AMAZON_ONELINK = \u0026#34;b63a2115-85f7-43a9-b169-5f4c8c275655\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e # Extra files customization\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e-EXTRA_PATH_METADATA = {}\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003e+EXTRA_PATH_METADATA = {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003e+    \u0026#39;extra/CNAME\u0026#39;: {\u0026#39;path\u0026#39;: \u0026#39;CNAME\u0026#39;},\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003e+}\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003e+\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e EXTRA_TEMPLATES_PATHS = [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e     \u0026#34;plugins/revealmd/templates\u0026#34;,  # eg: \u0026#34;plugins/revealmd/templates\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e-STATIC_PATHS = [ \u0026#39;images\u0026#39; ]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003e+STATIC_PATHS = [ \u0026#39;images\u0026#39; , \u0026#39;extra\u0026#39;]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e ## ONLY TOUCH IF YOU KNOW WHAT YOU\u0026#39;RE DOING!\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis will copy the \u003ccode\u003eCNAME\u003c/code\u003e file created in \u003ccode\u003econtent/extra/CNAME\u003c/code\u003e to the resulting \u0026lsquo;master\u0026rsquo; branch as \u003ccode\u003e/CNAME\u003c/code\u003e.\u003c/p\u003e","title":"Use custom domain name with Blog-O-Matic"},{"content":"Redken Manual This document contains some information about regular Redken https://t.me/redken_bot usage and some advanced settings.\nIntroduction By default, new groups where the bot is added are just ready to start being used.\nGeneral usage and features:\nword++ to add karma word-- to remove karma reply to message with \u0026lsquo;++\u0026rsquo;, \u0026lsquo;\u0026ndash;\u0026rsquo; or \u0026lsquo;==\u0026rsquo; to add or remove karma to the user of the replied message or to the same words that were used /quote add username text to add a quote for a given username with the following text as the message /quote username to retrieve a random quote for that username. @all to ping all usernames for users in a channel @all++ to give karma to all usernames in a channel stock \u0026lt;ticker\u0026gt; to get the trading quote for ticker in stock market /hilight \u0026lt;add|delete|list\u0026gt; \u0026lt;word\u0026gt; Adds/deletes word or lists words that will cause a forward to notify you /feed \u0026lt;add|delete|list\u0026gt; name url Adds/deletes/lists a new feed form URL on channel /remind \u0026lt;add|delete|list\u0026gt; name interval text Adds/deletes/lists a new reminder for interval in channel, interval can be specified as \u0026lsquo;1y2m3w5d\u0026rsquo; /ical \u0026lt;add|delete|list\u0026gt; name url Adds/deletes/lists a new ical url to print events happening during the day /cn \u0026lt;word\u0026gt; To get a random Chuck Norris fact related with the provided word (or random) /excuse To get a random excuse Also, while nothing is set against, you could use /gconfig to configure several aspects of it like:\nmodulo (to just report karma every modulo points) stock (to define the stock tickers to query when invoking stock) Once you started chatting with the bot, you can also use /hilight word so messages containing that word will be forwarded to you as a private message.\nBy default karma in channels is private to that group, but also, groups can be linked.\nOn the channel to become master execute: /admin link master and it will generate a code (token) to link against just once.\nOn the channel to be linked against master, a.k.a. slave, execute: /admin link slave \u0026lt;token\u0026gt; where token is the code received as reply to the command in master channel.\nUIDEnforcer Adds UID (as reported via /info) to the list of safe members of a chat, anyone else, will be kicked\n/uidenforcer add \u0026lt;UID\u0026gt; to add a new UID to safe list /uidenforcer remove \u0026lt;UID\u0026gt; to remove a UID from safe list /uidenforcer list to show the UIDs in safe list Once the list of UID\u0026rsquo;s is ready, and the bot is set as Group Administrator, enable the check (every hour) by running /gconfig set safelist=True on your group to have the bot start checking on next scheduled execution.\nBEWARE: If the list is created and setting enabled, bot will start kicking all other users in the chat, ensure that you\u0026rsquo;re the creator or your user has been whitelisted or you might be kicked out too.\nRed Hat Jobs Lists Red Hat Jobs published at https://t.me/rhjobs that have word in it:\n/rhjobs add \u0026lt;words\u0026gt; to add a new word to the rhjobs watch /rhjobs remove \u0026lt;words\u0026gt; to remove a new word to the rhjobs watch /rhjobs list to show the words being watched User/chat configuration The bot, once token has been used and admin has been set, will store that information in the database, so you can control it from a chat window\n/[g|l]config show will list actual defined settings (/gconfig, /lconfig or /config) /[g|l]config set var=value will set one of those settings with a new value As of this writing (verbosity, url for api, token, sleep timeout, owner, database, run in daemon mode) /[g|l]config delete var will delete that variable from configuration. The available list of configuration options that can be used depending on private or chats is listed below:\ncommon cleankey: Regexp to replace, for example tag= cleanlink: True if we want links to be expanded and removed currency: EUR futureevents: Enables sending a reminder the specified number of days in advance lang: set to language of choice to get some strings translated into supported languages https://crowdin.com/project/stampython and override autodetected language. modulo: 1 (to just show karma every X/modulo points, 0 to disable) privacy: Enables privacy for forwarded messages, if a message is forwarded and the config is set, redken will remove original message and resend text to the chat so that the original sender is removed but forwarder is credited. If value is set to silent it will just clean the message forwarder. quiet: Enables sending of messages without enabling notification: message arrives, but makes no noise. splitkarmaword: Set to \u0026lsquo;False\u0026rsquo; to make that johndoe.linux.expert++ stops reporting karma to the word and to johndoe stock: stock tickers to check chat admin: List of admins of channels, default empty: everyone alladmins: Set this to True to make all users in chat being able to use administrative commands, by default (False) means that only chat admins can run the commands. enableall: Set this to admin or karma or false to allow being used only by admins, allow regular users to just give karma but no pinging or to disable it in your chat. grace: Set this to the initial grace period in days for user to say something when added to a channel before being kicked out of inactivity (fakes the join date as grace days before being kicked out of max inactivity) hiordie: Set this to the initial number of minutes that a user has to say something on the chat, similar to grace but for shorter periods inactivity: Set this to the number of days without user activity before kicking it out. isolated: False, if true, allow link, all karma, etc is tied to GID link: empty, if defined, channel is slave to a mater maxage: chats older than this will be removed removejoinparts: Set this to automatically remove \u0026lsquo;User XXX has joined\u0026rsquo; or \u0026lsquo;User XXX has left\u0026rsquo; messages from the groups. silent: makes stampy not to output messages to that chat usernamereminder: Set this False to stop reminding new users to get a username to get the most out of karma commands. welcome: outputs the text when a new user joins the chat, replacing \u0026ldquo;$username\u0026rdquo; by user name Extra commands Only for admin user in groups or for individuals against the bot\n/reload_admins: Uses telegram API to find admins and populate the admin variable for commands that require admin access. /spoiler: Reploy to a message with this to delete it and send it back with hidden text /thanks [add|list|delete]: Manage the list of words that, when replied, give karma to original sender Karma skarma word=value will set specified word to the karma value provided. Auto-karma triggers Bot allows to trigger auto-karma events, so when keyword is given, it will trigger an event to increase karma value for other words\n/autok key=value Will create a new auto-karma trigger, so each time key is used, it will trigger value++ event /autok list [word] Will show current defined autokarma triggers and in case a word is provided will search based on that word /autok delete key=value will delete a previously defined auto-karma so no more auto-karma events will be triggered for that pair Auto-gif triggers Bot allows to trigger gif sending when a keyword is given.\n/autog key=value Will create a new auto-gif trigger, so each time key is used, it will trigger gif send event /autog list [word] Will show current defined autogif triggers and in case a word is provided will search based on that word /autog delete key=value will delete a previously defined auto-gif so no more gifs will be sent for that keyword Alias Bot allows to setup aliases, so when karma is given to a word, it will instead add it to a different one (and report that one)\n/alias key=value Will create a new alias, so each time key++ is used, it will instead do value++ This operation, sums the previous karma of key and value and stores it in value so no karma is lost Recursive aliases can be defined, so doing: /alias lettuce=vegetable /alias vegetable=food lettuce++ will give karma to food. Alias can be defined to groups of words so, it can be defined to have: /alias friday=tgif tfsmif friday++ will increase karma on tgif and tfsmif. /alias list Will show current defined aliases /alias delete key will delete a previously defined alias so each word gets karma on its own Alias team Bot allows to setup team aliases, so it can ping several users in a chat\n/ateam team=members Will create a new team, so when @team is used, it will ping each member\u0026rsquo;s username (prepending @ to each word defined as members) Teams can be defined to groups of words so, it can be defined to have: /ateam ateam=face murdock hannibal mr-t /ateam list Will show current defined teams /ateam delete team will delete a previously defined team quote /quote del id to remove a specific quote id from database Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/redken_bot/","summary":"\u003ch1 id=\"redken-manual\"\u003eRedken Manual\u003c/h1\u003e\n\u003cp\u003eThis document contains some information about regular Redken \u003ca href=\"https://t.me/redken_bot\"\u003ehttps://t.me/redken_bot\u003c/a\u003e usage and some advanced settings.\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBy default, new groups where the bot is added are just ready to start being used.\u003c/p\u003e\n\u003cp\u003eGeneral usage and features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eword++\u003c/code\u003e to add karma\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eword--\u003c/code\u003e to remove karma\u003c/li\u003e\n\u003cli\u003ereply to message with \u0026lsquo;++\u0026rsquo;, \u0026lsquo;\u0026ndash;\u0026rsquo; or \u0026lsquo;==\u0026rsquo; to add or remove karma to the user of the replied message or to the same words that were used\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/quote add username text\u003c/code\u003e to add a quote for a given username with the following text as the message\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/quote username\u003c/code\u003e to retrieve a random quote for that username.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e@all\u003c/code\u003e to ping all usernames for users in a channel\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e@all++\u003c/code\u003e to give karma to all usernames in a channel\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003estock \u0026lt;ticker\u0026gt;\u003c/code\u003e to get the trading quote for ticker in stock market\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/hilight \u0026lt;add|delete|list\u0026gt; \u0026lt;word\u0026gt;\u003c/code\u003e Adds/deletes word or lists words that will cause a forward to notify you\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/feed \u0026lt;add|delete|list\u0026gt; name url\u003c/code\u003e Adds/deletes/lists a new feed form URL on channel\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/remind \u0026lt;add|delete|list\u0026gt; name interval text\u003c/code\u003e Adds/deletes/lists a new reminder for interval in channel, interval can be specified as \u0026lsquo;1y2m3w5d\u0026rsquo;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/ical \u0026lt;add|delete|list\u0026gt; name url\u003c/code\u003e Adds/deletes/lists a new ical url to print events happening during the day\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/cn \u0026lt;word\u0026gt;\u003c/code\u003e To get a random Chuck Norris fact related with the provided word (or random)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/excuse\u003c/code\u003e To get a random excuse\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAlso, while nothing is set against, you could use \u003ccode\u003e/gconfig\u003c/code\u003e to configure several aspects of it like:\u003c/p\u003e","title":"@redken_bot"},{"content":"The new document is at Redken Documentation.\n","permalink":"https://iranzo.io/redkendoc/telegram-redken_bot-documentation/","summary":"\u003cp\u003eThe new document is at \u003ca href=\"/redkendoc/telegram-redken_bot-documentation/\"\u003eRedken Documentation\u003c/a\u003e.\u003c/p\u003e","title":"Telegram Redken bot documentation"},{"content":"Introduction For some time now, I wanted to put the presentations I did in the past to be available, and since I\u0026rsquo;ve added support to my blog to render revealjs slides, I wanted to also put other presentations that I did in the past, probably (or for sure) outdated, but that were sitting in my computer drive.\nThe presentations already got several transformations, but in the actual status they are stored as LibreOffice ODP files, that made it a bit difficult.\nSome software for the conversion, did generate them as \u0026lsquo;screenshoots\u0026rsquo; for each slide, this however had some cons/pros:\nPros: Format was kept almost 100% Easy to showcase with any \u0026lsquo;gallery\u0026rsquo; plugin Cons: Text was lost, so no links, no indexation, etc Alternatively, I could add and attach via a link to the odp file for end users to download and reproduce, but that will increase blog size (no constrains, but sounded like nonsense to me), so I continued my research for a solution or workaround to use.\nThe approach Thanks to https://github.com/cliffe/AwesomeSlides, which uses perl and a set of common available libraries on any distribution to do the job, I was able to convert my presentations from odp to html quite easily:\nfor file in *.odp; do perl convert-to-awesome.pl $file done This resulted in \u0026lsquo;master\u0026rsquo; html files in the slides_out folder, plus a folder containing the images and other media used by the presentation.\nAwesomeSlides does the conversion to \u0026lsquo;revealjs\u0026rsquo; format, plus adds extra features, transitions, etc to make them fancier, but in my case I was interested in plain markdown, so the next one to the rescue has been pandoc\nfor file in *.html; do pandoc -t markdown $file -o $file.md done The end result of course is not clean at all and not directly usable by the pelican plugin to render the images, etc.\nThe post-processing One of the things needed (and that I used for other slides) is to move the resulting md file to the same folder as the images and move them into the content/presentations folder of my website source.\nOnce there, a set of find/replacements was required:\nfind replacement description $folder `` Define images included as `` for pelican to pick them up \u0026mdash;\u0026mdash;\u0026mdash; Remove underlining after titles stangechars normalchar Some characters were lost (accents, etc), replaced by another one, to later spell check \\n\\n\\n \\n\\n Remove extra new lines -⠀⠀⠀⠀⠀ -⠀ Remove extra spaces before paragraph Other manual steps involved\nPut ## in front of each title Adjust empty slides (\u0026mdash; followed by another \u0026mdash;) And a lot more :-) The good thing, in the end, is that with some additional work, I was able to bring back \u0026lsquo;online\u0026rsquo; my older presentations, now listed in the presentations category. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/01/22/awesomeslides-for-converting-libreoffice-odp-into-revealjs/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eFor some time now, I wanted to put the presentations I did in the past to be available, and since I\u0026rsquo;ve added support to \u003ca href=\"/blog/2019/01/20/fixing-pelican-revealjs-plugin/\"\u003emy blog to render revealjs slides\u003c/a\u003e, I wanted to also put other presentations that I did in the past, probably (or for sure) outdated, but that were sitting in my computer drive.\u003c/p\u003e\n\u003cp\u003eThe presentations already got several transformations, but in the actual status they are stored as LibreOffice ODP files, that made it a bit difficult.\u003c/p\u003e","title":"AwesomeSlides for converting LibreOffice ODP into revealjs"},{"content":"Introduction After my recent talk about blog-o-matic, I was trying to upload somewhere the slides I used.\nSince some time ago I started using Reveal-MD, so I could use Markdown to create and show slides, but wanted also a way to upload them for consumption.\nPelican-revealmd plugin seemed to be the answer.\nIt does use pypandoc library and \u0026lsquo;pandoc\u0026rsquo; for doing the conversion.\nThe problems found After some test, it had 3 issues:\nImages, specified with whatever pelican formatting, where rendered not alongside the html Code blocks where not properly shown Title was shown as \u0026lsquo;Untitled\u0026rsquo; in the generated html For the 1st one, after reaching on #pelican on FreeNode, it was clear that images should be placed alongside the html, and that required using \u0026lsquo;{static}\u0026rsquo;, but pandoc was escaping it.\nThe fixes The first patch was to use a replace function on the text to move back { and } to the character that pelican will recognize and interpret.\nSecond patch was for the Title, initially, I did use python Beautiful Soup to replace the title tag on the HTML, it took a while, but it worked.\nFor 3rd one, there was no clear approach, as the html rendered was working, but not shown after using pelican. When also tried to use a newer pandoc version, the results were even worse.\nFinally, I dissected what I wanted from pypandoc module, and instead of using it, went to directly use the same shell command I was using.\nAs the templates for the rendered the pages should be similar, I moved to use the \u0026rsquo;non-standalone\u0026rsquo; version of pandoc conversion, hence, instead of generating full html, I could reuse headers, css loading, etc and just put the content relevant for the slides, and at the same time, reuse article metadata like article.title, author, date to fill some values in the rendered html.\nThis also, rendered in two outcomes, 2nd patch was no longer needed in that form, and some other dependencies were removed (no more pypandoc, no more BeautifulSoup, etc)\nThe new version of the plugin has been contributed via PR, but while it is being accepted by original author, you can find the relevant version in the \u0026lsquo;master\u0026rsquo; branch of https://github.com/iranzo/pelican-revealmd.\nOutcomes This of course, has brought some outcomes:\nMy blog can now render my reveal-md slides stored as filename.revealjs I\u0026rsquo;ve learned a bit on how Pelican plugins work Blog-o-Matic has been updated to include that support too, out of the box Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/01/20/fixing-pelican-revealjs-plugin/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAfter my recent talk about \u003ca href=\"/blog/2019/01/09/blog-o-matic-quickly-get-a-github-hosted-blog-with-pelican-elegant-with-little-setup-steps./\"\u003eblog-o-matic\u003c/a\u003e, I was trying to upload somewhere the slides I used.\u003c/p\u003e\n\u003cp\u003eSince some time ago I started using Reveal-MD, so I could use Markdown to create and show slides, but wanted also a way to upload them for consumption.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/brookskindle/pelican-revealmd/\"\u003ePelican-revealmd plugin\u003c/a\u003e seemed to be the answer.\u003c/p\u003e\n\u003cp\u003eIt does use pypandoc library and \u003ca href=\"https://pandoc.org\"\u003e\u0026lsquo;pandoc\u0026rsquo;\u003c/a\u003e for doing the conversion.\u003c/p\u003e\n\u003ch2 id=\"the-problems-found\"\u003eThe problems found\u003c/h2\u003e\n\u003cp\u003eAfter some test, it had 3 issues:\u003c/p\u003e","title":"Fixing pelican revealjs plugin"},{"content":"Project hosting and automation /me: Pablo Iranzo Gómez ( https://iranzo.io )\nYou got a shinny new project? What now? Code usually also requires a webpage\nDocumentation, General Information, Developer information, etc Web costs money\nHosting, Domain, Maintenance, etc Some Philosophy Empty your mind, be shapeless, formless, like water. Now you put water in a cup, it becomes the cup, you put water into a bottle, it becomes the bottle You put water in a teacup, it becomes the teapot Now water can flow or it can crash. Be water my friend\nNote: Automation: Be lazy, have someone else doing it for you.\nGitHub / Gitlab Lot of source code is hosted at Github, Gitlab or other services, but it\u0026rsquo;s a code repository. We want a website!! Pages come to play GitHub provides a service called GitHub Pages Gitlab provides Gitlab pages Both provide a \u0026lsquo;static\u0026rsquo; Webserver to be used for your projects for free.\nG(H/L) serve from a branch in your repo (usually yourusername.github.io repo)\nYou can buy a domain and point it to your website.\nStatic doesn\u0026rsquo;t mean end of fun There are many \u0026lsquo;static\u0026rsquo; content generators that provide rich features:\nstyles links image resizing even \u0026lsquo;search\u0026rsquo; Some static generators Importance of language is for developing \u0026lsquo;plugins\u0026rsquo;, not content.\nJekyll (Ruby) Pelican (Python) They \u0026lsquo;render\u0026rsquo; markdown into html\nThere\u0026rsquo;s even more fun Github provides Jekyll support Github, Gitlab, etc allow to plug in third-party CI Think about endless possibilities!!!\nSome food for thought Repositories have branches Repositories can have automation External automation like Travis CI can do things for you Note: We\u0026rsquo;ve all the pieces to push a new markdown file and have it triggering a website update and publish\nIs a static webpage ugly? There are lot of templates http://www.pelicanthemes.com Each theme have different feature set Choose wisely! (Small screens, html5, etc) If not, changing themes is quite easy: update, and \u0026lsquo;render\u0026rsquo; using new one. ** DEMO on Pelican + Theme **\nTravis-ci.org Automation for projects:\nFree for Open Source projects Configured via .travis.yml Some other settings via Web Interface (environment variables, etc) Example (setup environment)\nlanguage: python dist: trusty sudo: required python: - \u0026#34;3.5\u0026#34; before_install: - pip install -U pip - pip install -U setuptools - pip install -r tests/requirements.txt - pip install -r tests/test-requirements.txt - pip install peru - mkdir -p tests/themes/elegant - mv templates tests/themes/elegant/ - mv static tests/themes/elegant/ - cd tests \u0026amp;\u0026amp; peru sync Example continuation (actions!) before_script: - npm install travis-ci script: - pelican content/ -o output/ after_success: - node trigger-build.js Publish to remote repo after_success: - rm -rf .git/ - git init - git config user.name \u0026#34;Travis CI\u0026#34; - git config user.email \u0026#34;travis@domain.com\u0026#34; - git config --global push.default simple - git remote add origin https://${GITHUB_TOKEN}@github.com/Pelican-Elegant/pelican-elegant.github.io.git - make github Fancy things Build one repo and deploy to another branch/repo Upload pypi package Call triggers etc Real world use cases Run \u0026rsquo;tox\u0026rsquo; for UT\u0026rsquo;s Test latest theme and plugins render Render documentation website on docs update Render latest CV Build and publish container Wrap up Ok, automation is ready, our project validates commits, PR\u0026rsquo;s, website generation\u0026hellip;\nWhat else?\nContainers!! Container creation - Quay DockerHub and Quay allow to automate build on branch commit Docker Hub On each commit, a new container will be built You said to be water\u0026hellip; Yes!\nTry https://github.com/iranzo/blog-o-matic/\nFork to your repo and get:\nminimal setup steps (Github token, travis-ci activation) Automated setup of Pelican + Elegant theme via travis-ci job that builds on each commit. Ready to be submitted to search engines via sitemap, and web claiming Questions? Pablo.Iranzo@gmail.com https://iranzo.io\n","permalink":"https://iranzo.io/presentations/2019-01-16-automation-travis-ghpages-containers/","summary":"\u003ch2 id=\"project-hosting-and-automation\"\u003eProject hosting and automation\u003c/h2\u003e\n\u003cp\u003e/me: Pablo Iranzo Gómez ( \u003ca href=\"https://iranzo.io\"\u003ehttps://iranzo.io\u003c/a\u003e )\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"you-got-a-shinny-new-project-what-now\"\u003eYou got a shinny new project? What now?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eCode usually also requires a webpage\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDocumentation, General Information, Developer information, etc\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWeb costs money\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHosting, Domain, Maintenance, etc\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"some-philosophy\"\u003e\u003ca href=\"https://www.youtube.com/watch?v=cJMwBwFj5nQ\"\u003eSome Philosophy\u003c/a\u003e\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eEmpty your mind, be shapeless, formless, like water.\nNow you put water in a cup, it becomes the cup, you put water into a bottle, it becomes the bottle\nYou put water in a teacup, it becomes the teapot\nNow water can flow or it can crash.\nBe water my friend\u003c/p\u003e","title":"Project automation with Travis, GitHub Pages and Quay"},{"content":"Introduction I\u0026rsquo;ve already covered some articles about automation with Travis-CI, GitHub, and one step that seems a show-stopper for many users when trying to build a website is on one side, the investment (domain, hosting, etc), the backend being used (WordPress, static generators, etc)\u0026hellip;\nWhile preparing a talk for a group of co-workers covering several of those aspects, I came with the idea to create Blog-o-Matic, implementing many of those \u0026rsquo;learnings\u0026rsquo; in a \u0026lsquo;canned\u0026rsquo; way that can be easy to consume by users.\nThe approach Blog-o-Matic, uses several discussed topics so far:\nGithub and GitHub Pages for hosting the source and the website Travis-CI.org for automating the update and generation process \u0026lsquo;Pelican\u0026rsquo; for static rendering of your blog from the markdown or asciidoc articles \u0026lsquo;Elegant\u0026rsquo; for the \u0026lsquo;Theme\u0026rsquo; peru for automating repository upgrades for plugins, etc The setup process is outlined at its README.md and just requires a few steps to setup that, from that point, will allow you to get your website published each time you commit a new document to the content folder.\nYou can also check the \u0026lsquo;generated\u0026rsquo; website after installation via https://iranzo.io/blog-o-matic\nDo not forget to update your pelican.conf file for fine-tuning and customization. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/01/09/blog-o-matic-quickly-get-a-github-hosted-blog-with-pelican-elegant-with-little-setup-steps./","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eI\u0026rsquo;ve already covered some articles about automation with Travis-CI, GitHub,\nand one step that seems a show-stopper for many users when trying to build a\nwebsite is on one side, the investment (domain, hosting, etc), the backend\nbeing used (WordPress, static generators, etc)\u0026hellip;\u003c/p\u003e\n\u003cp\u003eWhile preparing a talk for a group of co-workers covering several of those aspects, I came with the idea to create Blog-o-Matic, implementing many of those \u0026rsquo;learnings\u0026rsquo; in a \u0026lsquo;canned\u0026rsquo; way that can be easy to consume by users.\u003c/p\u003e","title":"Blog-o-Matic - quickly get a GitHub hosted blog with Pelican, Elegant with little setup steps."},{"content":"OSP Director OSP Director (or upstream TripleO) is a life-cycle manager for OpenStack based on the idea of using \u0026lsquo;OpenStack\u0026rsquo; to deploy \u0026lsquo;OpenStack\u0026rsquo;.\nTo do so, it creates a management \u0026lsquo;Undercloud\u0026rsquo;, that is configured and prepared for later deploying an \u0026lsquo;overcloud\u0026rsquo; which is the one that will later run the workloads.\nTripleO/Director, also automates the inspection of hosts and tagging to the roles they will perform later in the \u0026lsquo;overcloud\u0026rsquo; setup, such as \u0026lsquo;controller\u0026rsquo;, \u0026lsquo;compute\u0026rsquo;, \u0026lsquo;storage\u0026rsquo;, or even mixed roles via composable-roles support.\nBaremetal Nova instances From OSP Director point of view, the baremetal servers that will be used for computes, are represented like \u0026lsquo;OpenStack instances\u0026rsquo; using a special flavor and are controlled via IPMI mechanisms to control boot sequence, power status, etc via \u0026lsquo;ironic\u0026rsquo; component.\nOpenShift CNV OpenShift, can run on top of baremetal or on top of nova instances, and it\u0026rsquo;s one of the common scenarios to deploy OpenShift on top of OpenStack.\nIn this case, for CNV, we also want to benefit from increased performance of running Virtual Machines as Kubernetes workloads.\nIn order to setup a new host to be used as baremetal \u0026lsquo;instance\u0026rsquo; to run Kubernetes node on top we do need to follow TripleO/Director approach, via adding a new host to ironic managed hosts list:\nCreate a new host definition (if reusing the original one, and importing it, will cause many of the discovered attributes via introspection to be lost): Hint\nBear in mind that proper MAC address of the interface connected to the \u0026lsquo;provisioning\u0026rsquo; network must be specified as it\u0026rsquo;s used by PXE to boot the right image.\n{ \u0026#34;nodes\u0026#34;: [ { \u0026#34;pm_password\u0026#34;: \u0026#34;$YOURIPMIPASS\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;blade-ocp-node-1\u0026#34;, \u0026#34;pm_type\u0026#34;: \u0026#34;pxe_ipmitool\u0026#34;, \u0026#34;pm_addr\u0026#34;: \u0026#34;10.19.0.89\u0026#34;, \u0026#34;mac\u0026#34;: [\u0026#34;18:DB:F2:8C:D5:FA\u0026#34;], \u0026#34;arch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;pm_user\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;disk\u0026#34;: \u0026#34;40\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;4096\u0026#34; } ] } Then, import the new node into ironic database: openstack baremetal import ~/new_node.json Next, set the node as manageable and perform instrospection to validate the capabilities of the hardware: openstack baremetal node manage blade-ocp-node-1 openstack overcloud node introspect blade-ocp-node-1 --provide After this above, the node should have rebooted, started the introspection and reported back the values to Director node. Now, let\u0026rsquo;s set some specific values to match our \u0026lsquo;baremetal\u0026rsquo; flavour (nova flavor-show baremetal): ironic node-update $NODEID add properties/capabilities=\u0026#39;profile:baremetal,boot_option:local\u0026#39; ironic node-update $NODEID add properties/resources=\u0026#39;CUSTOM_BAREMETAL:1\u0026#39; As we plan to deploy RHCOS on the baremetal node, let\u0026rsquo;s prepare some steps: wget https://$HOSTNAME/$PATH_TO_THE_LATEST_FILE gunzip rhcos-4.0.6844-openstack.qcow2.gz openstack image create --file $DOWNLOADEDFILE.qcow2 rhcos-image Create a config.ign filename with your SSH key for \u0026lsquo;core\u0026rsquo; user and CA\u0026rsquo;s: { \u0026#34;ignition\u0026#34;: { \u0026#34;config\u0026#34;: {}, \u0026#34;security\u0026#34;: { \u0026#34;tls\u0026#34;: { \u0026#34;certificateAuthorities\u0026#34;: [ { \u0026#34;source\u0026#34;: \u0026#34;data:text/plain;charset=utf-8;base64,LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURDVENDQWZHZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFtTVJJd0VBWURWUVFMRXdsdmNHVnUKYzJocFpuUXhFREFPQmdOVkJBTVRCM0p2YjNRdFkyRXdIaGNOTVRnd09USTJNVGswTURVNFdoY05Namd3T1RJegpNVGswTURVNFdqQW1NUkl3RUFZRFZRUUxFd2x2Y0dWdWMyaHBablF4RURBT0JnTlZCQU1UQjNKdmIzUXRZMkV3CmdnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURtZVlYbEZMZ0ZjU1Jxak51NE0vSnIKL2JwNlQzbmRlZGlqU3Q1Ti9DbnlTblhMVlExR3pYMngzUGZ0aHhXKzYxRFlGUE5xZzNPUlJKbURLUGJHbU5LSQovTUVuZkgrZVlrekZXY3BxQVhQMXlzbEpVWUVLNWhMSlJ6cCtjSWxmQ09zSW9nSGpwRlZlRkdVRWtzUzR5cXY2CjJsYWgvRWdURkhFZkFmUlhTc1VCN0owcWRWdExsWGJJc0thdzJvSVJ2V1g4M0x1RzhmSnpBalZlUVVBQldpT1EKOHZwY2NteUd6eXc1STNmMkdZMWxkZnVNSXI5TjNtSUFhMDluMUNqTE1Eem9hbjViZ0NzWHJyYWlhWFFWbEMwZgovMGw4eXIyczM4VmUrZmhrckw4K0xKVHpxaDdvb3JtYWR5eFJPRUR2VFZhWlFPSi90MzMyNm9DSHRaS1JkWW0xCkFnTUJBQUdqUWpCQU1BNEdBMVVkRHdFQi93UUVBd0lDcERBUEJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWQKRGdRV0JCUzREQm43eTR6YmFXaWdjcjFFK0tjeUsvRnhGakFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBbllzVQpQTkpaVGNiMTZ1QVRzNEpwUGpoMGJPMG9MSmttSlVhOTJIRXVaVlhCd2FpbEVmdjBNdmpONUlFTnFoa3VXQllTCkl3RHdKTmd3cS80QU5xQUNlNG5uaWpUeUlvY3ExVEFkcnNldGd3Y3RDV3Bra3o5RTBPUVY4V29ucEV0TTZ2VEMKQlZKWjhITGFjY05HT21taXBDbEIvT0dncnRqU0t3SC9HTEtQcmp4MStWVmdaWnRuU0xVTk5OaDFTZ1oyeTh2OApZNlFVZUJYUmhTWkNvUXJadlEvYldQWXE2ZlprMFNCVDJnUm9QaGxXVys2aVhQVlhBUkVDU1lYbDQ2WnNKNnJ1CnJTZnBydEZBbDVFY0NSNDc4U2FYT0VxS0dlZEJEblZreXZQY29pSkdvT3VKNVljUWZ2WGNGQ0NYSE93T3c0dXQKWEhrN3dEZFdJVEUvV0cweWhRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\u0026#34;, \u0026#34;verification\u0026#34;: {}, }, ], }, }, \u0026#34;timeouts\u0026#34;: {}, \u0026#34;version\u0026#34;: \u0026#34;2.2.0\u0026#34;, }, \u0026#34;networkd\u0026#34;: {}, \u0026#34;passwd\u0026#34;: { \u0026#34;users\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;core\u0026#34;, \u0026#34;sshAuthorizedKeys\u0026#34;: [ \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC8k6sAkPLWIUwov0YPLi3ex7WAw35MGZhXyC7Seyu9703wO436NBHOtRGFwl8gTbMR9k/WG0tPbNS42H06Zpi8vCB+ITftqBCWjlKOgOQKnvEiC73JhgounQPvjhj2r/JEvrPQ8QThHnBPLjFk5dGH9AORanEcNr9BBG/YXCgdcXAgZL9pQ+jhoy0JRnKhGpQLGzyMeuUYOfxcivXHKZFZzZ4fluSskx1Wm+Zw1Tzbhr/rTTvrlkQVmzxMlOjN/k02/CdEDjDG5G8Qb2QKsIqnZZy05snnfeN0B2u8uEhcfvW1IvKrRT/BXYlU3E7Rz7cboJlJ54eiKRlZXCQtXDYCfdJwHudaNtL5w31F4xOObdpCv3ANQb6Ravr0LG91liKMjaYCGNYkrP7S3v7D7gkpDyjNn7DYqgwCk2D/oU/NeuMZCB6PsBB2R2rXexq0EIgFw/bwabASQghCkV3JkOj5kIRvrQfFb3jZJDMEoELvAWLhGg08qvHg1leV3xm8+coobXiGqdmoGPCPATGb/4Evl1DR3mSrLYh88TAInCCDzivBSr+SLOl2BSpbwmJ0suIS3AIT04KeHJMpicOsOZE8SuDrngHTlCasZaoMF9ny6GiK3/jX7VJuTmYjtG7unZ5C7U7tT0jdMUcWOYezEhvLc1EmZtDSCYycxAC9s6oKdw== Pablo.Iranzo@gmail.com\u0026#34;, ], }, ], }, \u0026#34;storage\u0026#34;: {}, \u0026#34;systemd\u0026#34;: {}, } Last step is to deploy the baremetal server with the configuration and flavor: openstack server create --nic net-id=\u0026lt;ctlplane_ID\u0026gt; --config-drive true --user-data config.ign --flavor baremetal --image \u0026lt;rhcos-image-id\u0026gt; blade-ocp-node-1 If everything went thru, a nova list will show the instance as spawning and after some more time, the server will be ready and accessible via ssh core@$SERVERIP.\nIf it doesn\u0026rsquo;t, ensure to setup nova in \u0026lsquo;debug\u0026rsquo; mode for Undercloud, restart the services and try again, usually, the \u0026rsquo;no valid hosts found\u0026rsquo; error is the most generic one but will give you some \u0026rsquo;tips\u0026rsquo; on which filter ruled servers out. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2019/01/08/osp-director-baremetal-hypervisor-for-coreos/","summary":"\u003ch2 id=\"osp-director\"\u003eOSP Director\u003c/h2\u003e\n\u003cp\u003eOSP Director (or upstream TripleO) is a life-cycle manager for OpenStack based on the idea of using \u0026lsquo;OpenStack\u0026rsquo; to deploy \u0026lsquo;OpenStack\u0026rsquo;.\u003c/p\u003e\n\u003cp\u003eTo do so, it creates a management \u0026lsquo;Undercloud\u0026rsquo;, that is configured and prepared for later deploying an \u0026lsquo;overcloud\u0026rsquo; which is the one that will later run the workloads.\u003c/p\u003e\n\u003cp\u003eTripleO/Director, also automates the inspection of hosts and tagging to the roles they will perform later in the \u0026lsquo;overcloud\u0026rsquo; setup, such as \u0026lsquo;controller\u0026rsquo;, \u0026lsquo;compute\u0026rsquo;, \u0026lsquo;storage\u0026rsquo;, or even mixed roles via \u003ccode\u003ecomposable-roles\u003c/code\u003e support.\u003c/p\u003e","title":"OSP Director baremetal hypervisor for CoreOS"},{"content":"Postgres across clusters For Postgres to work across clusters we do need to have the data being synchronized.\nWith some other databases we do have some master-master approach, but usually have very strict requirements on latency, bandwidth, etc that we cannot solve with On-Premise + external cloud providers.\nIf the replication is based on the storage level instead, then you face that database servers don’t deal well if the data changes underneath it, so it leads to data corruption, on top of the storage-level issues/requirements as well on bandwidth, latency, etc.\nOther approaches http://evol-monkey.blogspot.com/2015/10/postgresql-94-streaming-replication.html https://hackernoon.com/postgresql-cluster-into-kubernetes-cluster-f353cde212de PostgreSQL streaming replication over SSL There are several ways to accomplish this if you do a quick search on a web crawler, but we\u0026rsquo;ve gone the PSQL Streaming replication over SSL for our environment.\nA replication slot is (as defined in documentation)\nReplication slots provide an automated way to ensure that the master does not remove WAL segments until they have been received by all standby\u0026rsquo;s, and that the master does not remove rows which could cause a recovery conflict even when the standby is disconnected.\nBackground Quay requires a database that is available for each instance and provides required redundancy/replication Replicating via storage can lead to issues as the files will change underneath postgres Galera approach for MySQL (multi-master) is known to have issues even when running on the same cluster For Federation we do want applications to run in different clusters, so an approach where a hot-standby is ready will probably overcome the limitations of master-master and the requirements for available database for all quay instances. Investigation We started to investigate how others approached this situation and similar to what was done for another part of the setup for MongoDB, we went testing via replication over SSL.\nWe also checked several of the approaches, but one of them, PostDock images were lacking SSL support, but still had a nice way to do several overrides for configuration, etc via environment variables, which made them ideal for testing in an OpenShift/Kubernetes environment.\nThe environments we plan to use are:\nAIT cluster LEO cluster PIT cluster SSL For the SSL setup side we did:\nCreate Postgres Certs Seems that Postgres doesn\u0026rsquo;t check the certificate for the host \u0026lsquo;by default\u0026rsquo;, but just uses the certificate to encrypt, but still we did create the certificates using the name of the application in it for future usage.\nUsing the same certificate with all server names, we can also federate a single secret containing those certs rather than creating one SSL secret for each cluster.\n./generate-cert.sh postgres With output:\n2018/11/29 15:04:29 [INFO] generate received request 2018/11/29 15:04:29 [INFO] received CSR 2018/11/29 15:04:29 [INFO] generating key: rsa-2048 2018/11/29 15:04:29 [INFO] encoded CSR 2018/11/29 15:04:29 [INFO] signed certificate with serial number 34111709152443674697969629831350216041253590538 Now, we do have all the required certificates for postgres generated.\n-rw-r--r--. 1 iranzo iranzo 1001 nov 29 15:04 postgres.csr -rw-rw-r--. 1 iranzo iranzo 208 nov 29 15:04 postgres-csr.json -rw-------. 1 iranzo iranzo 1679 nov 29 15:04 postgres-key.pem -rw-rw-r--. 1 iranzo iranzo 1753 nov 29 15:04 postgres.pem Our setup configuration Setup:\nLEO will be the \u0026lsquo;master\u0026rsquo; with $PGDATA at /var/lib/postgresql/data in a local volume AIT will be the \u0026lsquo;slave\u0026rsquo; with $PGDATA at /var/lib/postgresql/data in a local volume PIT will be the \u0026lsquo;slave\u0026rsquo; with $PGDATA at /var/lib/postgresql/data in a local volume The first issue we found is that by default, PostDock lacks the SSL support, but as we were allowed to define settings for the configuration files we could override the settings via a variable named CONFIGS set to ssl:on,ssl_cert_file:'/etc/postgresql/server.crt',ssl_key_file:'/etc/postgresql/server.key'.\nThis required to craft a custom image (quay.io/iranzo/postdock:sslkeyschown) that we submitted as PR to PostDock.\nIn the meantime, we did use https://quay.io to store our image and rebuild whenever we changed the code at our custom repo.\nThe changes in our image are very simple:\ndiff --git a/src/pgsql/bin/postgres/entrypoint.sh b/src/pgsql/bin/postgres/entrypoint.sh index b09652c..1298dcb 100755 --- a/src/pgsql/bin/postgres/entrypoint.sh +++ b/src/pgsql/bin/postgres/entrypoint.sh @@ -42,7 +42,25 @@ else fi fi -KEYS=$(egrep \u0026#39;(ssl_cert_file|ssl_key_file)\u0026#39; $PGDATA/postgresql.conf|cut -d \u0026#34;=\u0026#34; -f 2-) + +echo \u0026#34;\u0026gt;\u0026gt;\u0026gt; Trying to configure SSL\u0026#34; +# Tweak keys to avoid permission issues: +ORIGKEYS=$(echo $CONFIGS|tr \u0026#34;,\u0026#34; \u0026#34;\\n\u0026#34;|egrep \u0026#39;(ssl_cert_file|ssl_key_file)\u0026#39;|cut -d \u0026#34;:\u0026#34; -f 2-|tr \u0026#34;\\n\u0026#34; \u0026#34; \u0026#34;|tr -d \u0026#34;\\\u0026#39;\u0026#34;) +KEYS=\u0026#34;\u0026#34; + +echo \u0026#34;\u0026gt;\u0026gt;\u0026gt; Trying to move ${ORIGKEYS} to proper folder\u0026#34; +for file in ${ORIGKEYS}; do + # Check for file or link pointing to file + if [ -e /pg-ssl/$(basename ${file}) ]; then + echo \u0026#34;\u0026gt;\u0026gt;\u0026gt; Copying SSL file from /pg-ssl/$(basename ${file}) to ${file}\u0026#34; + mkdir -p $(dirname ${file}) + cat /pg-ssl/$(basename ${file}) \u0026gt; ${file} + KEYS=\u0026#34;$KEYS ${file}\u0026#34; + else + echo \u0026#34;\u0026gt;\u0026gt;\u0026gt; ERROR: SSL File ${file} doesn\u0026#39;t exist on disk\u0026#34; + fi +done + chown -R postgres $PGDATA $KEYS \u0026amp;\u0026amp; chmod -R 0700 $PGDATA $KEYS source /usr/local/bin/cluster/repmgr/configure.sh From this, and once \u0026lsquo;Quay\u0026rsquo; builds our image, we should, in the namespace for our project, add a new application via \u0026lsquo;Deploy image\u0026rsquo; with quay.io/iranzo/postdock:sslkeyschown (sources at https://github.com/iranzo/PostDock)\nThe application will ask for some configuration options, name for the new application, etc\nWe must configure some environment variables:\nEnvironment configuration PARTNER_NODES: IP\u0026rsquo;s of: postgres-leo.apps.e2e.bos.example.com,postgres-ait.apps.e2e.bos.example.com,postgres-aws.apps.e2e.bos.example.com (from below, but EXCLUDING the one we\u0026rsquo;re that we\u0026rsquo;ll be putting in CLUSTER_NODE_NETWORK_NAME) LEO: 10.19.227.153 AIT: 10.19.115.226 AWS: internal-aba91353afe1c11e89f350a50403e669-443870799.us-east-1.elb.amazonaws.com CLUSTER_NAME: quaydatabase POSTGRES_DB: quaydb DB_USERS: replication_user:replication_pass,quayuser:quaypass CONFIGS: ssl:on,ssl_cert_file:'/etc/postgresql/server.crt',ssl_key_file:'/etc/postgresql/server.key' NODE_NAME: Identifying name for this instance NODE_ID: $(NUMBER DIFFERENT FOR EACH NODE + 1000, f.e. 1002) CLUSTER_NODE_NETWORK_NAME: $(SAME AS NODE_NAME that would go in PARTNER NODES) REPLICATION_HOST: \u0026ldquo;postgres\u0026rdquo; (name of app in deployment) We did raise/update some issues:\nhttps://github.com/paunin/PostDock/issues/124 https://github.com/paunin/PostDock/issues/202 https://github.com/paunin/PostDock/issues/203 But finally as we had to build our own modified image we were able to circumvent them.\nWe did configure some more things to pass the certificates and key to the pods:\nConfigure a secret with server.crt and server.key based on the postgres.pem and postgres-key.pem for each one of them. As we\u0026rsquo;re using a secret for storing the certificates, we\u0026rsquo;ll use a volume exposing it to the host via the path /pg-ssl\n- mountPath: /pg-ssl name: volume-yisiz readOnly: true Our patched image, will find and move the certificates to their final destination (specified via environment variable in CONFIGS).\nThis image, also ensures valid permissions and ownership so that postgres can start and answer \u0026lsquo;SSL\u0026rsquo; via a:\niranzo   iranzo-save  …  RH  syseng  pit-hybrid  psql -h localhost -U replication_user -W replication_db -p 5432 Password for user replication_user: psql (10.6) SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: disabled) replication_db=# \\dt Did not find any relations. replication_db=# Additional settings Configure a volume for storing permanent data in /var/lib/postgresql/data so that is kept and not destroyed on pod destroy. The bittersweet wrap-up At this point we do have the postgres image started with SSL, however:\nIn order to use replication between environments, we need a way to access postgres The creation and configuration of SSL setup gets us a bit closer to it OpenShift HAPROXY requires SNI-capable client, which is not1 the case, hence we still cannot have the traffic get into the postgres instance and hence, replication doesn\u0026rsquo;t start as there\u0026rsquo;s no communication. All attempts to use a router (that only permits http and https or TLS-with-SNI) failed completely, as PSQL doesn\u0026rsquo;t yet have the support in place, failed, even some other hacks on initialization scripts to use the alternate port instead.\nUnfortunately, this became a major blocker at this time, not allowing us to setup a replicated postgres cluster across different OpenShift Clusters to have it as a database we can rely for setting up Quay on top.\nThe plot twist After finding no more ways, we discussed with our team members for putting more brains in this, and in conversations with Pep, Mario and Ryan, it was suggested to instead use a LoadBalancer IP, that imposes no restrictions on the traffic, this however could only be accomplished on 3 environments (LEO, AIT and AWS)\nAs AWS seems to have no DNS resolution, we\u0026rsquo;re limited to use IP\u0026rsquo;s in the server name (for the CLUSTER_NODE_NETWORK_NAME) so that in case of failover, AWS can reach them.\nWith the current setup, we only needed to define \u0026lsquo;persistent\u0026rsquo; storage for postgres so in case all three pods were destroyed at the same time, we do have some data to start over.\nThe final steps, outlined above in Our setup configuration, were updated to use:\nPARTNER_NODES containing the LB IP address for the Load Balancer defined in each environment EXCEPT our own\nCLUSTER_NODE_NETWORK_NAME: our LB IP as it would go in PARTNER_NODES for other clusters\nPARTNER_NODES: IP\u0026rsquo;s of: postgres-leo.apps.e2e.bos.example.com,postgres-ait.apps.e2e.bos.example.com,postgres-aws.apps.e2e.bos.example.com (from below, but EXCLUDING the one we\u0026rsquo;re that we\u0026rsquo;ll be putting in CLUSTER_NODE_NETWORK_NAME)\nLEO: 10.19.227.153 AIT: 10.19.115.226 AWS: internal-aba91353afe1c11e89f350a50403e669-443870799.us-east-1.elb.amazonaws.com NODE_ID: $(NUMBER DIFFERENT FOR EACH NODE + 1000, f.e. 1002)\nCLUSTER_NODE_NETWORK_NAME: $(SAME AS NODE_NAME that would go in PARTNER NODES)\nREPLICATION_HOST: \u0026ldquo;postgres\u0026rdquo; (name of app in deployment)\nWith this approach, the LoadBalancer effectively passes the data on port 5432 (even if it\u0026rsquo;s not SSL), and the cluster can form and start replication of data.\nSo next steps are:\nDefine DNS name pointing to all 3 IP\u0026rsquo;s that we can configure on apps Rely on psql standby to work in read-only mode and providing service Setup Quay to use that psql DNS name Automate the setup, including Quay ENV variable for storage \u0026lsquo;closer\u0026rsquo; to each cluster. Thread on psql-hackers and pinged by us Enjoy! (and if you do, you can Buy Me a Coffee ) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iranzo.io/blog/2019/01/08/postgres-repl-ssl-replication/","summary":"\u003ch1 id=\"postgres-across-clusters\"\u003ePostgres across clusters\u003c/h1\u003e\n\u003cp\u003eFor Postgres to work across clusters we do need to have the data being synchronized.\u003c/p\u003e\n\u003cp\u003eWith some other databases we do have some master-master approach, but\nusually have very strict requirements on latency, bandwidth, etc that we\ncannot solve with On-Premise + external cloud providers.\u003c/p\u003e\n\u003cp\u003eIf the replication is based on the storage level instead, then you face that database servers don’t deal well if the data changes underneath it, so it leads to data corruption, on top of the storage-level issues/requirements as well on bandwidth, latency, etc.\u003c/p\u003e","title":"Postgres repl SSL replication"},{"content":"Why this article? For Federation of OpenShift/Kubernetes clusters we want not only to demo some applications, but build a solution that covers all that will be needed in a real-world deployment.\nColleagues in the Solutions Engineering team have been working on demonstrating an application running on different clusters over a Federated Control Plane, allowing it to \u0026lsquo;roam\u0026rsquo; between clusters with shared data access as a way to demonstrate how an application can \u0026lsquo;scale\u0026rsquo; from on-premise deployment to external clouds in order to satisfy business requirements for peak demands.\nThis article tries to demonstrate on how to setup Quay, as a container registry for your applications, in a way that can work both for this Federated approach as well as High-Availability setup.\nWhat is Quay? The one liner would be: Quay is a container registry.\nThe extended one, would be more in the lines of:\nQuay is a container registry that can be used via the provided service at https://quay.io or can be used via Quay Enterprise as \u0026lsquo;on premise\u0026rsquo; registry.\nA container registry provides a point for containers to be downloaded or uploaded for later consumption.\nWhy quay? Quay allows to be configured either to use the provided service via https://quay.io or deploy it on premise, so that you gain additional control over the setup like which namespaces to replicate, policies, etc.\nQuay features clair analysis of images, allowing to detect known vulnerabilities from different sources to later check the images.\nAlso, allows to automate builds when a push to a repository has happened, allowing to quickly spawn an update container image when our tracked repository has a new commit that can be later updated in our OpenShift/Kubernetes image stream.\nQuay can also, when having several storage backends defined, do geo-replication, that allows OpenShift to grab the needed bytes from the closest backend while handling in the background the copy of new data to the other replicas.\nWhat is Swift? Swift is object storage provided by OpenStack Environment, defined at https://wiki.openstack.org/wiki/Swift as:\nThe OpenStack Object Store project, known as Swift, offers cloud storage software so that you can store and retrieve lots of data with a simple API. It\u0026rsquo;s built for scale and optimized for durability, availability, and concurrency across the entire data set. Swift is ideal for storing unstructured data that can grow without bound.\nSwift allows to define a container (not in the https://www.opencontainers.org/ sense but in a \u0026lsquo;data storage\u0026rsquo;).\nHow does it come to play with Quay? Quay allows to define storage backend based on swift that can be used together with other backends to provide geo-replication for availability.\nThe Setup process of Swift for Quay At the beginning it was not easy to find out all the required configuration variables or settings to define, but what finally worked for the setup described in this article was:\nCreate the Swift container First we do need to create a swift container in OpenStack that will hold the information from Quay:\nCreate swift container for holding the storage:\n(overcloud) [stack@osp-ait-director ~]$ openstack container create quay-enterprise Output: account container x-trans-id AUTH_b3b8ce2eca3c4556ac01c12772b0d141 quay-enterprise txd7d654f499db457da0fa0-005bf53d32 Configuration settings defined in Quay Setting Value Storage Engine OpenStack Storage (Swift) Swift Auth Version 3 Swift Auth URL \\$YOURKEYSTONE PUBLIC ENDPOINT (like http://aaa.bbb.ccc.ddd:5000) Swift container name \\$YOURCONTAINER (created with swift container create \u0026rsquo;name\u0026rsquo;) Storage path \\$WHATEVER (we did use /quay) Username \\$OSP_USERNAME (we did use admin) key/password \\$OSP_PASSWORD(we did use redhat) OS OPTIONS drop-down \u0026lt;\u0026mdash;\u0026ndash; This is an important one if not it will fail tenant_id \\$YOURTENANTID (I did use \u0026lsquo;admin\u0026rsquo; id as obtained with openstack project list|grep admin) Once done, Quay should \u0026lsquo;validate\u0026rsquo; the upload and swift list $YOURCONTAINER should give something like:\n(overcloud) [stack@osp-director ~]$ swift list quay-enterprise quay/_verify Showing that the verification file has been created succesfully.\nGeoreplication Quay allows to define several backends that can be also configured as a foundation for GeoReplication.\nGeoreplication in quay, is used to:\ndefine a \u0026lsquo;zone name\u0026rsquo; for a storage backend when booting Quay containers or apps, define which backend is \u0026lsquo;closest\u0026rsquo; (via environment variable QUAY_DISTRIBUTED_STORAGE_PREFERENCE) Quay will store and replicate on the different backends defined the information uploaded. The requirement is that the backend is an object storage, but you can also define \u0026lsquo;mixed\u0026rsquo; backends like for example GCE, AWS, Swift, Ceph RadosGW, etc. Quay will take care of the copy and once the backend \u0026lsquo;closest\u0026rsquo; to the quay instance is ready, it will serve from that one (or fallback to the one having the actual data).\nFor each one of the configured backends, a different number of settings will be requested to do the proper configuration.\nGoogle Cloud Storage For setting Google Cloud Storage, we did use the following values:\nSetting Value Location ID $yourvalue Storage Engine Google Cloud Storage Cloud Access Key GOOGXXXXXXXX Cloud Secret Key pksXXXXXX GCS Bucket quay-enterprise Storage Directory /datastorage/registry Amazon S3 For setting Amazon S3 storage, we did use the following values:\nSetting Value Location ID $yourvalue Storage Engine Amazon S3 S3 Bucket quay-enterprise Storage Directory /quay/ait AWS Access Key AKIAXXXXXXXX AWS Secret Key /9bkfXXXXXXXXXX Validation of the replication Once all the defined storage backends have been configured, there\u0026rsquo;s a button in the interface that will validate access and create the \u0026lsquo;marker\u0026rsquo; files on each storage backend to verify the access permissions to them similar to what was shown on Swift configuration.\nQuay SSL deployment This procedure will guide you through the configuration of Quay with SSL certificates so the deployment is both secure and deployed in a highly available enterprise way.\nSSL Certificates Setting up certificates requires several steps involving:\nCreating a CA Create a Certificate Signing Request with all the possible quay hostnames Sign the certificates Upload the certificates on Quay Create CA Execution of generate-ca.sh\n./generate-ca.sh With output:\n2018/11/29 15:01:13 [INFO] generating a new CA key and certificate from CSR 2018/11/29 15:01:13 [INFO] generate received request 2018/11/29 15:01:13 [INFO] received CSR 2018/11/29 15:01:13 [INFO] generating key: rsa-2048 2018/11/29 15:01:13 [INFO] encoded CSR 2018/11/29 15:01:13 [INFO] signed certificate with serial number 432855461933024796113382064222588753737107796958 And creates:\n-rw-rw-r--. 1 iranzo iranzo 232 nov 29 15:01 ca-config.json -rw-r--r--. 1 iranzo iranzo 1001 nov 29 15:01 ca.csr -rw-rw-r--. 1 iranzo iranzo 208 nov 29 15:01 ca-csr.json -rw-------. 1 iranzo iranzo 1679 nov 29 15:01 ca-key.pem -rw-rw-r--. 1 iranzo iranzo 1359 nov 29 15:01 ca.pem Create Quay Certs We are going to use the same certificate for each replica, we need to know the hostname that will be used on the OCP routes beforehand in order to use it for the SAN property in the certificate.\nHaving each of the routes in the SAN makes it possible to use the same cert for each replica, so we can federate a single secret containing those certs rather than creating one SSL secret for each cluster.\nExecution of generate-cert.sh\n./generate-cert.sh quay-enterprise With output:\n2018/11/29 15:01:26 [INFO] generate received request 2018/11/29 15:01:26 [INFO] received CSR 2018/11/29 15:01:26 [INFO] generating key: rsa-2048 2018/11/29 15:01:26 [INFO] encoded CSR 2018/11/29 15:01:26 [INFO] signed certificate with serial number 570248288515480165542553575432827729263232937534 Now, we have all the required certificates for Quay generated.\n-rw-r--r--. 1 iranzo iranzo 1001 nov 29 15:01 quay-enterprise.csr -rw-rw-r--. 1 iranzo iranzo 208 nov 29 15:01 quay-enterprise-csr.json -rw-------. 1 iranzo iranzo 1675 nov 29 15:01 quay-enterprise-key.pem -rw-rw-r--. 1 iranzo iranzo 1809 nov 29 15:01 quay-enterprise.pem Load Quay certificates Now using the web interface of quay, we can start uploading the certificates required for operation.\nCustom SSL Certificates Here, load the quay.pem\nServer Configuration **TLS** Certificate: load the quay-enterprise.pem Key: load the quay-enterprise-key.pem\nAt this point Quay will be using the certificates, but by default the service created only works on port 80.\nREMEMBER: We\u0026rsquo;ll need to edit:\nthe app the route So that both work on port 443 instead of port 80 that is used by default.\nWe\u0026rsquo;ll also need to load ca.pem and quay-enterprise.pem into our /etc/pki/ca-trust/source/anchors folder and then run:\nupdate-ca-trust extract service docker restart # Now let\u0026#39;s go login into the registry docker login https://quay-enterprise-quay-enterprise.apps.leo-fed.e2e.bos.example.com # Let\u0026#39;s download a container: docker pull quay.io/mavazque/pacman # let\u0026#39;s tag and upload to our registry docker images|grep pacman quay.io/mavazque/pacman latest 6c5211874850 29 hours ago 689 MB # Tag and upload # Find image docker images|grep pacman quay.io/mavazque/pacman latest 6c5211874850 29 hours ago 689 MB # Tag image docker tag 6c5211874850 quay-enterprise-quay-enterprise.apps.leo-fed.e2e.bos.example.com/admin/pacman:latest # Upload image docker push quay-enterprise-quay-enterprise.apps.leo-fed.e2e.bos.example.com/admin/pacman The push refers to a repository [quay-enterprise-quay-enterprise.apps.leo-fed.e2e.bos.example.com/admin/pacman] fb2d3be23d30: Pushed c02d067bd364: Pushed 3edd111b03f2: Pushed 2221babd26ac: Pushed 911d290ce59f: Pushed fae583f1d4e5: Pushed 9d22e51f0c6d: Pushed 58b8d417193d: Pushing [===================================\u0026gt; ] 225.5 MB/320.1 MB 704a8634956f: Pushing [==================================================\u0026gt;] 127 MB 05f0b6bcfa5c: Pushed 0972cc82b682: Pushing [=============================\u0026gt; ] 74.2 MB/126.8 MB OpenShift certificate Unless we also load the SSL certificate from Quay into OpenShift, we\u0026rsquo;ll be facing issues like this: Wrap up At this point, we\u0026rsquo;ve Quay setup for using geo-replication for HA and faster speeds from each cluster, we do have an underlying postgres setup using replication across sites and the URL\u0026rsquo;s have been secured for being used by our servers downloading images from there to deploy our applications.\nHope that you\u0026rsquo;ve enjoyed it!\n","permalink":"https://iranzo.io/blog/2019/01/08/quay-for-federation/","summary":"\u003ch2 id=\"why-this-article\"\u003eWhy this article?\u003c/h2\u003e\n\u003cp\u003eFor Federation of OpenShift/Kubernetes clusters we want not only to demo some applications, but build a solution that covers all that will be needed in a real-world deployment.\u003c/p\u003e\n\u003cp\u003eColleagues in the Solutions Engineering team have been working on\ndemonstrating an application running on different clusters over a Federated Control Plane, allowing it to \u0026lsquo;roam\u0026rsquo; between clusters with shared data access as a way to demonstrate how an application can \u0026lsquo;scale\u0026rsquo; from on-premise deployment to external clouds in order to satisfy business requirements for peak demands.\u003c/p\u003e","title":"Quay for Federation"},{"content":"Android TV versus Android TV-Box Android, being used in lot of mobile devices is also part of TVs and set-top boxes.\nThe main difference between both approaches is the user interface and applications.\nAndroid TV-Box: it\u0026rsquo;s like a tablet or mobile connected to a TV and usually requires external keyboard, often integrated in the remote.\nAndroid TV: it\u0026rsquo;s a version of Android that incorporates TV changes in the OS so that it can be controlled with an arrow cursor remote plus back, home and microphone key.\nThe range of apps also differs, TV-Boxes have the full range of apps at a cost: they are not optimized for screen usage and require to point and click on several places for operation, while on the other side Android TV does have a reduced set but those apps are prepared for being used with simpler controls that are more natural for a TV.\nAndroid TV also has Chromecast support, so that you can not only replicate/mirror your mobile screen to the TV, but also cast content from apps supporting it, not only for playing, but also to play games between several users.\nAmong Android TV systems I\u0026rsquo;ve tested three:\nNVIDIA Shield TV 🛒#ad which acts both as Game Console and Android TV system Xiaomi Mi Box 🛒#ad which contains the Android TV System as an addon for any TV with HDMI input Sony Bravia Android TV 🛒#ad which directly integrates Android TV as the TV operating system NVIDIA and Xiaomi both allow to \u0026lsquo;convert\u0026rsquo; or \u0026lsquo;upgrade\u0026rsquo; any TV to an Android TV box and additionally can be upgraded in the future for a cheaper price tag than upgrading a TV.\nNetflix One of the features of an AndroidTV box is the ability to use it with Amazon Prime Video or with Netflix.\nUnfortunately, recently I started getting an error on the Xiaomi MiTV Box I bought for an older TV:\nAfter searching for tvq pm 5.2 5, I got to this Netflix FAQ: https://help.netflix.com/en/node/59709\nWhich more or less solved nothing, even technical support said that their supported platforms are Fire TV, which was not the kind of answer I wanted to hear.\nKodi to the rescue After some more deep research, a post suggested to use:\nKodi v18 (nightly if not released when you\u0026rsquo;re reading this) Enabling Netflix Addon Once configured (email and password), I was able to use Kodi to access Netflix using Kodi as a workaround while the problem is solved. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2018/12/30/android-tv-and-netflixs-tvq-pm-100-5.2.5-error/","summary":"\u003ch1 id=\"android-tv-versus-android-tv-box\"\u003eAndroid TV versus Android TV-Box\u003c/h1\u003e\n\u003cp\u003eAndroid, being used in lot of mobile devices is also part of TVs and set-top boxes.\u003c/p\u003e\n\u003cp\u003eThe main difference between both approaches is the user interface and applications.\u003c/p\u003e\n\u003cp\u003eAndroid TV-Box: it\u0026rsquo;s like a tablet or mobile connected to a TV and usually requires external keyboard, often integrated in the remote.\u003c/p\u003e\n\u003cp\u003eAndroid TV: it\u0026rsquo;s a version of Android that incorporates TV changes in the OS so that it can be controlled with an arrow cursor remote plus back, home and microphone key.\u003c/p\u003e","title":"Android TV and Netflix's tvq-pm-100 (5.2.5) error"},{"content":"Introduction After setting up build automation we also wanted it not to happen only when updating the documentation repository.\nBesides hosting documentation, Elegant website also serves as a live demo of the current release. This meant, the website should be regenerated and updated every time when a documented is added or edited, and also when Elegant theme is updated.\nGithub and Travis doesn\u0026rsquo;t offer dependent builds out of the box, so the trick goes to \u0026lsquo;signal\u0026rsquo; via a github token to trigger a Travis-CI build.\nThe technical solution The approach goes via tweaking the \u0026rsquo;test validation\u0026rsquo; .travis.yaml and adding some more steps:\nThe initial file (similar to the one in our previous article, but for running the \u0026lsquo;page build\u0026rsquo; with latest repo checkout) looks like:\n# Copyright (C) 2017, 2018 Pablo Iranzo Gómez \u0026lt;Pablo.Iranzo@gmail.com\u0026gt; language: python dist: trusty sudo: required python: - \u0026#34;3.5\u0026#34; # prepare and move data for execution before_install: - pip install -U pip - pip install -U setuptools - pip install -r tests/requirements.txt - pip install -r tests/test-requirements.txt - pip install peru - mkdir -p tests/themes/elegant - mv templates tests/themes/elegant/ - mv static tests/themes/elegant/ - cd tests \u0026amp;\u0026amp; peru sync script: - pelican content/ -o output/ Is then modified to add:\nbefore_script: - npm install travis-ci after_success: - node trigger-build.js This installs Travis-CI utilities and runs a custom script \u0026rsquo;trigger-build.js\u0026rsquo; with node, which in turn actually triggers Travis build.\nThe script, downloaded from Kamran Ayub blog has been edited to specify the \u0026lsquo;repo\u0026rsquo; we will trigger and the name of the environment variable containing the token:\n#!js var Travis = require(\u0026#39;travis-ci\u0026#39;); // change this var repo = \u0026#34;Pelican-Elegant/documentation\u0026#34;; var travis = new Travis({ version: \u0026#39;2.0.0\u0026#39; }); travis.authenticate({ // available through Travis CI // see: http://kamranicus.com/blog/2015/02/26/continuous-deployment-with-travis-ci/ github_token: process.env.TRATOKEN }, function (err, res) { if (err) { return console.error(err); } travis.repos(repo.split(\u0026#39;/\u0026#39;)[0], repo.split(\u0026#39;/\u0026#39;)[1]).builds.get(function (err, res) { if (err) { return console.error(err); } travis.requests.post({ build_id: res.builds[0].id }, function (err, res) { if (err) { return console.error(err); } console.log(res.flash[0].notice); }); }); }); As you can see, in line 14, it grabs the github token from environment variable TRATOKEN that we\u0026rsquo;ve defined in Travis-CI environment for the build.\nThis is similar to what we did in the documentation repo to push the built website to another repo.\nWith this solution in place, when a new commit is merged on \u0026lsquo;master\u0026rsquo; branch on the \u0026rsquo;theme\u0026rsquo; repo (elegant), Travis does get invoked to schedule a build on the documentation repo, thus, rendering the live website with latest templates.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2018/12/11/triggering-a-travis-ci-build-on-another-repo/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eAfter setting up \u003ca href=\"/blog/2018/12/07/how-to-automate-via-travis-ci-publishing-of-new-pelican-pages-to-github-pages/\"\u003ebuild automation\u003c/a\u003e we also wanted it not to happen only when updating the \u003ccode\u003edocumentation\u003c/code\u003e repository.\u003c/p\u003e\n\u003cp\u003eBesides hosting documentation, Elegant website also serves as a live demo of the current release. This meant, the website should be regenerated and updated every time when a documented is added or edited, and also when Elegant theme is updated.\u003c/p\u003e\n\u003cp\u003eGithub and Travis doesn\u0026rsquo;t offer dependent builds out of the box, so the trick goes to \u0026lsquo;signal\u0026rsquo; via a github token to trigger a Travis-CI build.\u003c/p\u003e","title":"Triggering a Travis-CI build on another repo"},{"content":"Background Elegant theme for pelican has been undergoing a big change from individual-driven effort to community, as part of this, one of the tasks to accomplish, has been the decoupling from author blog to project site for documentation.\nAs we wanted this process to be both automated and allowed us to demonstrate via eat-your-own-dog-food that the theme works and how it looks, the idea was to automate the rendering of Pelican website with new documents.\nUnder the hood Setting an automated build required several steps to be done:\nGet a GitHub token that could be used by Travis for pushing to a repository (and configure it in Travis environment variables for the repository in a variable named GITHUB_TOKEN) run unit tests for validating new PR before merging configure Travis so that it downloads required dependencies in order to run pelican and then publish the generated web to the repository a GitHub pages enabled repository so that resulting files can be viewed as a webpage. One of the key pieces is a properly configured .travis.yaml like the one we started using:\nlanguage: python dist: trusty sudo: required python: - \u0026#34;3.5\u0026#34; before_install: - pip install -U pip - pip install -U setuptools - pip install -r requirements.txt - pip install -r test-requirements.txt - pip install peru - peru sync - pip install tox script: - tox - make html after_success: - rm -rf .git/ - git init - git config user.name \u0026#34;Travis CI\u0026#34; - git config user.email \u0026#34;travis@domain.com\u0026#34; - git config --global push.default simple - git remote add origin https://${GITHUB_TOKEN}@github.com/Pelican-Elegant/pelican-elegant.github.io.git - make github Image setup So, from above file we do:\nlanguage: python dist: trusty sudo: required python: - \u0026#34;3.5\u0026#34; Configure language as Python Select distribution Confirm we require sudo access Configure Python version as 3.5 All of this depends on Travis Image being used and their documentation\nPreparation of environment Now, we\u0026rsquo;ll prepare the environment for our tests:\nbefore_install: - pip install -U pip - pip install -U setuptools - pip install -r requirements.txt - pip install -r test-requirements.txt - pip install peru - peru sync - pip install tox We do install pip, setuptools, repository and test requirements, peru and tox.\nPeru is used to grab additional dependencies for Elegant (plugins, latest theme, etc)\nActual tests This is really easy in our case:\nscript: - tox - make html We run \u0026rsquo;tox\u0026rsquo; that allows to automate Python virtualenv and tests and then, use the Makefile from Pelican to build the site and tests plugins, etc\nIf everything succeeds, we\u0026rsquo;re ready for the next step (publishing)\nAfter tests passed All the environment setup and tests have succeed now, we do need to push the site \u0026rsquo;live'\nafter_success: - rm -rf .git/ - git init - git config user.name \u0026#34;Travis CI\u0026#34; - git config user.email \u0026#34;travis@domain.com\u0026#34; - git config --global push.default simple - git remote add origin https://${GITHUB_TOKEN}@github.com/Pelican-Elegant/pelican-elegant.github.io.git - make github This piece does the final step, first removes info about the repository containing the actual documentation and allows us to initialize a new one, that we make it point towards the repository we\u0026rsquo;re pushing (so that we keep separate actual website content from \u0026lsquo;rendered\u0026rsquo; website).\nIn the final step, \u0026lsquo;make github\u0026rsquo; uses the makefile provided with pelican to push the changes to the \u0026lsquo;master\u0026rsquo; branch of the target repository, that then, is ready to be served via github pages as a regular web server would do.\nWrap up So, right now we\u0026rsquo;ve accomplished several things:\nWe do use pelican in the same way that we\u0026rsquo;ll do for our own website We do also have as a consequence, a \u0026rsquo;live\u0026rsquo; demo of latest master branch showcasing features We did automate publishing of webpage as soon as contributors send new articles and are approved for merge All requires no extra change to regular workflow as \u0026lt;Travis-ci.org\u0026gt; is the glue here putting together all the pieces. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2018/12/07/how-to-automate-via-travis-ci-publishing-of-new-pelican-pages-to-github-pages/","summary":"\u003ch1 id=\"background\"\u003eBackground\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Pelican-Elegant/elegant\"\u003eElegant\u003c/a\u003e theme for pelican has been undergoing a big change from individual-driven effort to community, as part of this, one of the tasks to accomplish, has been the decoupling from author blog to project site for documentation.\u003c/p\u003e\n\u003cp\u003eAs we wanted this process to be both automated and allowed us to demonstrate via eat-your-own-dog-food that the theme works and how it looks, the idea was to automate the rendering of Pelican website with new documents.\u003c/p\u003e","title":"How to automate via Travis CI publishing of new pelican pages to GitHub pages"},{"content":"Introduction In the article \u0026ldquo;Contributing to OpenStack\u0026rdquo; we did cover on how to prepare accounts and prepare your changes for submission upstream (and even how to find low hanging fruits to start contributing).\nHere, we\u0026rsquo;ll cover what happens behind the scene to get change published.\nUpstream workflow Peer review Upstream contributions to OSP and other projects are based on Peer Review, that means that once a new set of code has been submitted, several steps for validation are required/happen before having it implemented.\nThe last command executed (git-review) on the submit sequence (in the prior article) will effectively submit the patch to the defined git review service (git-review -s does the required setup process) and will print an URL that can be used to access the review.\nEach project might have a different review platform, but usually for OSP it\u0026rsquo;s https://review.openstack.org while for other projects it can be https://gerrit.ovirt.org, https://gerrithub.io, etc (this is defined in .gitreview file in the repository).\nA sample .gitreview file looks like:\n[gerrit] host=review.gerrithub.io port=29418 project=citellusorg/citellus.git For a review example, we\u0026rsquo;ll use one from gerrithub from Citellus project:\nhttps://review.gerrithub.io/#/c/380646/\nHere, we can see that we\u0026rsquo;re on review 380646 and that\u0026rsquo;s the link that allows us to check the changes submitted (the one printed when executing git-review).\nCI tests (Verified +1) Once a review has been submitted, usually the bots are the first ones to pick them and run the defined unit testing on the new changes, to ensure that it doesn\u0026rsquo;t break anything (based on what is defined to be tested).\nThis is a critical point as:\nTests need to be defined if new code is added or modified to ensure that later updates doesn\u0026rsquo;t break this new code without someone being aware. Infrastructure should be able to test it (for example you might need some specific hardware to test a card or network configuration) Environment should be sane so that prior runs doesn\u0026rsquo;t affect the validation. OSP CI can be checked at \u0026lsquo;Zuul\u0026rsquo; http://zuul.openstack.org/ where you can \u0026lsquo;input\u0026rsquo; the number for your review and see how the different bots are running CI tests on it or if it\u0026rsquo;s still queued.\nIf everything is OK, the bot will \u0026lsquo;vote\u0026rsquo; your change as Verified +1 allowing others to see that it should not break anything based on the tests performed\nIn the case of OSP, there\u0026rsquo;s also third-party CI\u0026rsquo;s that can validate other changes by third party systems. For some of them, the votes are counting towards or against the proposed change, for others it\u0026rsquo;s just a comment to take into account.\nEven if sometimes you know that your code is right, there\u0026rsquo;s a failure because of the infrastructure, in those cases, writing a new comment saying recheck, will schedule a new CI test run.\nThis is common usually during busy periods when it\u0026rsquo;s harder for the scheduler to get available resources for the review validation. Also, sometimes there are errors in the configuration of CI that must be fixed in order to validate those changes.\nNote: you can run some of the tests on your system to validate faster if you\u0026rsquo;ve issues by running tox this will setup virtual environment for tests to be run so it\u0026rsquo;s easier to catch issues before upstream CI does (so it\u0026rsquo;s always a good idea to run tox even before submitting the review with git-review to detect early errors).\nThis is however not always possible as some changes include requirements like testing upgrades, full environment deployments, etc that cannot be done without the required preparation steps or even the infrastructure.\nCode Review+2 This is probably the \u0026rsquo;longest\u0026rsquo; process, it requires peers to be added as \u0026lsquo;reviewer\u0026rsquo; (you can get an idea on the names based on other reviews submitted for the same component) or they will pick up new reviews as the pop un on notification channels or pending queues.\nOn this, you must prepare mentally for everything\u0026hellip; developers could suggest to use a different approach, or highlight other problems or just do some small nit comments to fixes like formating, spacing, var naming, etc.\nAfter each comment/change suggested, repeat the workflow for submitting a new patchset, but make sure you\u0026rsquo;re using the same review id (that\u0026rsquo;s by keeping the commit id that is appended): this allows the Code Review platform to identify this change as an update to a prior one, and allow you for example to compare changes across versions, etc. (and also notify the prior reviewers of new changes).\nOnce reviewers are OK with your code, and with some \u0026lsquo;Core\u0026rsquo; developers also agreeing, you\u0026rsquo;ll see some voting happening (-2..+2) meaning they like the change in its actual form or not.\nOnce you get Code Review +2 and with the prior Verified +1 you\u0026rsquo;re almost ready to get the change merged.\nWorkflow+1 Ok, last step is to have someone with Workflow permissions to give a +1, this will \u0026lsquo;seal\u0026rsquo; the change saying that everything is ok (as it had CR+2 and Verified+1) and change is valid\u0026hellip;\nThis vote will trigger another build by CI, and when finished, the change will be merged into the code upstream, congratulations!\nCannot merge, please rebase Sometimes, your change is doing changes on the same files that other programmers did on the code, so there\u0026rsquo;s no way to automatically \u0026lsquo;rebase\u0026rsquo; the change, in this case the bad news is that you need to:\ngit checkout master # to change to master branch git pull # to push latest upstream changes git checkout yourbranch # to get back to your change branch git rebase master # to apply your changes on top of current master After this step, it might be required to manually fix the code to solve the conflicts and follow instructions given by git to mark them as reviewed.\nOnce it\u0026rsquo;s done, remember to do like with any patchset you submitted afterwards:\ngit commit --amend # to commit the new changes on the same commit Id you used git-review # to upload a new version of the patchset This will start over the progress, but will, once completed to get the change merged.\nHow do we do it with Citellus? In Citellus we\u0026rsquo;ve replicated more or less what we\u0026rsquo;ve upstream\u0026hellip; even the use of tox.\nCitellus does use https://gerrithub.io (free service that hooks on github and allows to do PR)\nWe\u0026rsquo;ve setup a machine that runs Jenkins to do \u0026lsquo;CI\u0026rsquo; on the tests we\u0026rsquo;ve defined (mostly for python wrapper and some tests) and what effectively does is to run tox, and also, we do use https://travis-ci.org free Tier to repeat the same on other platform.\ntox is a tool that allows to define several commands that are executed inside python virtual environments, so without touching your system libraries, it can get installed new ones or removed just for the boundaries of that test, helping into running:\npep8 (python formating compliance) py27 (python 2.7 environment test) py35 (python 3.5 environment test) The py* tests are just to validate the code can run on both base python versions, and what they do is to run the defined unit testing scripts under each interpreter to validate.\nFor local test, you can run tox and it will go trough the different tests defined and report status\u0026hellip; if everything is ok, it should be possible that your new code review passes also CI.\nJenkins will do the +1 on verified and \u0026lsquo;core\u0026rsquo; reviewers will give +2 and \u0026lsquo;merge\u0026rsquo; the change once validated. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2018/10/16/contributing-to-osp-upstream-a.k.a.-peer-review/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the article \u0026ldquo;\u003ca href=\"/blog/2016/07/21/contributing-to-openstack/\"\u003eContributing to OpenStack\u003c/a\u003e\u0026rdquo; we did cover on how to prepare accounts and prepare your changes for submission upstream (and even how to find \u003ccode\u003elow hanging fruits\u003c/code\u003e to start contributing).\u003c/p\u003e\n\u003cp\u003eHere, we\u0026rsquo;ll cover what happens behind the scene to get change published.\u003c/p\u003e\n\u003ch2 id=\"upstream-workflow\"\u003eUpstream workflow\u003c/h2\u003e\n\u003ch3 id=\"peer-review\"\u003ePeer review\u003c/h3\u003e\n\u003cp\u003eUpstream contributions to OSP and other projects are based on Peer Review, that means that once a new set of code has been submitted, several steps for validation are required/happen before having it implemented.\u003c/p\u003e","title":"Contributing to OSP upstream a.k.a. Peer Review"},{"content":"Peru a repository synchronizer Some projects upstream bind together lot of files which might not be of interest, but still the convenience of a git pull to get latest updates, makes you to download the whole repository for just a bunch of files or folders.\nFor example, this website uses Pelican to generate the webpages out of markdown files. Pelican does have a rich set of plugins but all of them are in the same folder in the git checkout.\nHere, is where peru comes in to play. Peru (hosted at https://github.com/buildinspace/peru) comes handy at this task.\nYou can install peru from pipsi:\npipsi install peru And will provide you a command-line tool that uses a yaml file for definition of repositories, like the one I do use here:\nimports: # The dircolors file just goes at the root of our project. sitemap: plugins/ better_codeblock_line_numbering: plugins/ better_figures_and_images: plugins/ yuicompressor: plugins/ git module sitemap: url: git@github.com:getpelican/pelican-plugins.git pick: sitemap rev: ead70548ce2c78ed999273e265e3ebe13b747d83 git module yuicompressor: url: git@github.com:getpelican/pelican-plugins.git pick: yuicompressor rev: ead70548ce2c78ed999273e265e3ebe13b747d83 git module better_figures_and_images: url: git@github.com:getpelican/pelican-plugins.git pick: better_figures_and_images rev: ead70548ce2c78ed999273e265e3ebe13b747d83 git module better_codeblock_line_numbering: url: git@github.com:getpelican/pelican-plugins.git pick: better_codeblock_line_numbering rev: ead70548ce2c78ed999273e265e3ebe13b747d83 In this way, peru sync will download from the provided repositories the specified folder into the destination indicated, allowing you to integrate \u0026ldquo;other repositories files\u0026rdquo; in your own workflow.\nFor example, if you want to use citellus repository at one specific point in time, to be integrated in your code, you could use:\nimports: citellus: ./ git module citellus: url: git@github.com:citellusorg/citellus.git pick: citellusclient/plugins/ rev: 1ee1c6a36f51e8a7c809d5162004fb57ee99b168 This will checkout citellus repository at one specific point in time and merge in your current folder.\nEnjoy! (and if you do, you can Buy Me a Coffee ) Pablo\n","permalink":"https://iranzo.io/blog/2018/09/25/peru-for-syncing-specific-git-repository-files/","summary":"\u003ch2 id=\"peru-a-repository-synchronizer\"\u003ePeru a repository synchronizer\u003c/h2\u003e\n\u003cp\u003eSome projects upstream \u003ccode\u003ebind\u003c/code\u003e together lot of files which might not be of interest, but still the\nconvenience of a \u003ccode\u003egit pull\u003c/code\u003e to get latest updates, makes you to download the whole\nrepository for just a bunch of files or folders.\u003c/p\u003e\n\u003cp\u003eFor example, this website uses \u003ca href=\"http://getpelican.com/\"\u003ePelican\u003c/a\u003e to generate the webpages out of markdown files. Pelican does have a rich set of \u003ca href=\"https://github.com/getpelican/pelican-plugins\"\u003eplugins\u003c/a\u003e but all of them are in the same folder in the \u003ccode\u003egit checkout\u003c/code\u003e.\u003c/p\u003e","title":"Peru for syncing specific git repository files"},{"content":"New blog post! After some great help and collaboration from all the colleagues reviewing, suggesting edits, we were able to prepare a blog post on Citellus, the tool we develop as part of our daily work solving cases.\nCheck it out at Citellus: System configuration validation tool\nEnjoy! (and if you do, you can Buy Me a Coffee ) Pablo\n","permalink":"https://iranzo.io/blog/2018/06/07/first-post-on-red-hat-blog/","summary":"\u003ch2 id=\"new-blog-post\"\u003eNew blog post!\u003c/h2\u003e\n\u003cp\u003eAfter some great help and collaboration from all the colleagues reviewing, suggesting edits, we were able to prepare a blog post on \u003ca href=\"https://risuorg.github.io\"\u003eCitellus\u003c/a\u003e, the tool we develop as part of our daily work solving cases.\u003c/p\u003e\n\u003cp\u003eCheck it out at \u003ca href=\"https://www.redhat.com/en/blog/citellus-system-configuration-validation-tool?scid=701f2000000tnTlAAI\"\u003eCitellus: System configuration validation tool\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e\n\n\u003cp\u003ePablo\u003c/p\u003e","title":"First post on Red Hat blog"},{"content":"Presentation at SuperSEC 2018! I\u0026rsquo;ve got a slot at SuperSEC 2018 (https://supersec.es/): Congress on secure software development, happening in Almería, Spain on the weekend on 12-13 May.\nI\u0026rsquo;ll be presenting on 13th may at 10:50, and the slide deck and I\u0026rsquo;ll be updating this once I get the recording URL.\nSome data about the event:\nConference program and further data can be found at https://supersec.es/programa/ Twitter feed: https://twitter.com/SuperSEC_ES Shared Pics from attendees Press coverage: https://www.lavozdealmeria.com/noticia/12/almeria/151767/expertos-en-ciberseguridad-en-la-ual About the topics, the main focus was security on software from design phase to production and maintenance.\nSome of the presentations insisted on the costs not only for your brand, reputation or business damage, but also on the actual cost of fixing issues later on versus doing a secure development.\nFor the secure development approach, where most of the bugs are introduced and can be fixed for cheap, some tools were presented that help to early detect known coding mistakes, highlighting them in automated, and later, working on static code analysis and code review.\nIt was also interesting to see the EU focus via FOSSA-2 project to promote Open Source and hear about legal implications and intended roadmap for security in software, both for Spain and EU.\nCommon Criteria was also one of the topics as well as the code audits, penetration testing, etc (with over 20 slots you can imagine :) )\nOn our side, we were presenting about how Citellus can help in detecting current or future issues that affect your environment and how easily it can be extended to cover your use cases while contributing it back to community.\nOf course we were also highlighting how collaboration got Citellus enhanced with feedback from RDO Project users to cover not only RHEL6 and RHEL7 but also Fedora, CentOS and other distributions via more generic tests and functions that covers them.\nEnjoy! (and if you do, you can Buy Me a Coffee ) PD: Highlighted in Citellus blog at https://risuorg.github.io/blog/2018/04/16/supersec/\n","permalink":"https://iranzo.io/blog/2018/05/13/supersec-2018/","summary":"\u003ch2 id=\"presentation-at-supersec-2018\"\u003ePresentation at SuperSEC 2018!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve got a slot at SuperSEC 2018 (\u003ca href=\"https://supersec.es/\"\u003ehttps://supersec.es/\u003c/a\u003e): Congress on secure software development, happening in Almería, Spain on the weekend on 12-13 May.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ll be presenting on 13th may at 10:50, and the \u003ca href=\"/es/presentations/citellus/supersec2018/\"\u003eslide deck\u003c/a\u003e\nand I\u0026rsquo;ll be updating this once I get the recording URL.\u003c/p\u003e\n\u003cp\u003eSome data about the event:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConference program and further data can be found at \u003ca href=\"https://supersec.es/programa/\"\u003ehttps://supersec.es/programa/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTwitter feed: \u003ca href=\"https://twitter.com/SuperSEC_ES\"\u003ehttps://twitter.com/SuperSEC_ES\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://photos.google.com/share/AF1QipNI_PqleRD_ImbRhu7sOgfpoOYGLXMUdanNUov7GDktY4-A-o0b_8pEsKnlBnLgZg?key=VEw2bDRDNHJ3QXdYR0NReXZrcWdOa2JjSEdyNWhB\"\u003eShared Pics from attendees\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePress coverage:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.lavozdealmeria.com/noticia/12/almeria/151767/expertos-en-ciberseguridad-en-la-ual\"\u003ehttps://www.lavozdealmeria.com/noticia/12/almeria/151767/expertos-en-ciberseguridad-en-la-ual\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAbout the topics, the main focus was security on software from design phase to production and maintenance.\u003c/p\u003e","title":"SuperSEC 2018!"},{"content":"Citellus: Detecting common pitfalls of deployments https://github.com/citellusorg/citellus\nRecording at https://www.youtube.com/watch?v=SDzzqrUdn5A\nWhat is Citellus? Citellus is a framework populated by community-contributed scripts that automate detecting problems, including configuration issues, conflicts with package versions, and more. History: how did was it started? The tool, started by Robin Černín after a long weekend shift checking one and over again several sosreports for the same data on different hosts.\nIt started with some tests + shell wrapper, and was added a python wrapper by Pablo Iranzo to bring in more powerful features.\nAfter some design discussions with Engineering, a simpler reporting and design of tests was implemented.\nWhat can you do with Citellus? Run against a sosreport or live environment. Solve issues faster thanks to the information it provides. Use the community-provided plugins for detecting actual or potential issues. Code new plugins in your language of choice (bash, python, ruby, etc.) to extend functionality. Contribute them upstream for others to benefit. Use that information as part of proactive insights about the systems. Real life examples? For example, with Citellus you can detect: Failed keystone token purges. missing parameters in expired ceilometer data, which can lead to filling up your hard drive. unsynced NTP. outdated packages that have known critical issues. others! (119 plugins as of this writing some of them with more than one issue detected) Whatever else you can imagine or code 😉 Some numbers on plugin count: bugzilla : 15 [\u0026#39;openstack/swift: 1\u0026#39;, \u0026#39;openstack/keystone/templates: 1\u0026#39;, \u0026#39;openstack/ceph: 1\u0026#39;, \u0026#39;httpd: 1\u0026#39;, \u0026#39;openstack/keystone: 1\u0026#39;, \u0026#39;openstack/ceilometer: 1\u0026#39;, \u0026#39;openstack/nova: 2\u0026#39;, \u0026#39;openstack/httpd: 1\u0026#39;, \u0026#39;systemd: 1\u0026#39;, \u0026#39;openstack/tripleo: 1\u0026#39;, \u0026#39;openstack/neutron: 4\u0026#39;] launchpad : 1 [\u0026#39;openstack/keystone: 1\u0026#39;] network : 1 [] openstack : 54 [\u0026#39;nova: 8\u0026#39;, \u0026#39;mysql: 6\u0026#39;, \u0026#39;rabbitmq: 3\u0026#39;, \u0026#39;containers/rabbitmq: 1\u0026#39;, \u0026#39;network: 3\u0026#39;, \u0026#39;glance: 1\u0026#39;, \u0026#39;containers/docker: 2\u0026#39;, \u0026#39;containers: 4\u0026#39;, \u0026#39;cinder: 1\u0026#39;, \u0026#39;hardware: 1\u0026#39;, \u0026#39;swift: 2\u0026#39;, \u0026#39;systemd: 1\u0026#39;, \u0026#39;iptables: 1\u0026#39;, \u0026#39;ceph: 4\u0026#39;, \u0026#39;keystone: 3\u0026#39;, \u0026#39;system: 1\u0026#39;, \u0026#39;redis: 1\u0026#39;, \u0026#39;neutron: 1\u0026#39;, \u0026#39;crontab: 3\u0026#39;, \u0026#39;ceilometer: 3\u0026#39;] pacemaker : 7 [] security : 12 [\u0026#39;meltdown: 2\u0026#39;, \u0026#39;spectre: 8\u0026#39;] system : 28 [] virtualization : 1 [] The goal Be so damn simple to write new plugins that anyone can do them. Allow to write tests in whatever language of choice (bash, python, perl, etc) as long as they conform to some standards. Allow anyone to submit new plugins. How to run it manually? ![](citellusrun.png\u0026quot; height=\u0026ldquo;20%\u0026rdquo; border=0\u0026gt;\nNote: Change speaker after this\nHighlights Plugins in your language of choice. Allows to dump output to json file to be processed by other tools. Allow to visualize html from json output. Ansible playbook support (live and snapshoot if crafted playbooks) Core implemented as extension to easily expand with new ones. Save / restore default settings HTML Interface Create by using \u0026ndash;output and \u0026ndash;web, open the generated citellus.html over http. ![](www.png\u0026quot; height=\u0026ldquo;40%\u0026rdquo; border=0\u0026gt; Why upstream? This is an open source project. All the scripts should be committed upstream and shared (and we are willing to foster this) Project on GitHub: https://github.com/citellusorg/citellus/ We want contributions to happen from anyone. We follow an approach similar to other OpenSource projects: we do use Gerrit for reviewing the code and UT\u0026rsquo;s for validating basic functionality. How do I contribute? At the moment, there’s a strong presence on OpenStack plugins as it is where we solve our issues on everyday basis, but allows anything, as long as there are tests written for it.\nFor example, it will be easy to report on systems registered against RHN instead of CDN or systems with a specific version of pacemaker known to have lot of issues or check amount of free memory or memory usage from a process.\nRead contributing doc at: https://github.com/citellusorg/citellus/blob/master/CONTRIBUTING.md for more details.\nCitellus vs other tools XSOS: Provides information on ram usage, etc, no analysis, more like a ‘fancy’ sosreport viewer.\nTripleO-validations: only runs live from the environment, for customer support most of times we cannot afford to do that.\nWhy not sosreports? It’s not Citellus or ‘sosreports’, SOS collects data from the system, Citellus, runs tests/plugins against the data collected. Sosreport is installed in RHEL base channels, this makes it well spread, but also, slower to get changes. Frequently, data about errors or errors to be, is already in sosreports. Citellus is based on known issues and easy to extend with new ones, requires faster devel cycle, targeting more a DEVOPS or support teams as target audience. Note: Change speaker after this\nHow does it work under the hood? Philosophy is very simple:\nCitellus is just a simple wrapper. Allows to specify on sosreport and test folders Finds tests available in test folders Executes each test against sosreport and reports return status Framework written in python which features option parsing, parallel execution of tests, filtering, etc. What about the plugins? Tests are even simpler:\nWritten in whatever language of choice as long as they can be executed from shell. Output messages to ‘stderr’ (\u0026gt;\u0026amp;2) When using strings like echo $”string” bash’s built-in i18n is used so you can translate in your language. Return $RC_OKAY for success / $RC_FAILED for error / $RC_SKIPPED for skipped tests / Other for unexpected error What about the plugins? (continuation) Will inherit some env vars like root folder for sosreport (empty for live) (CITELLUS_ROOT) or if running live (CITELLUS_LIVE) that provide required details. No user input should be required. Live tests can for example query DB and ones in sosreport check values on logs Some execution and script examples? Check disk usage:\n#!/bin/bash # Load common functions [ -f \u0026#34;${CITELLUS_BASE}/common-functions.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;${CITELLUS_BASE}/common-functions.sh\u0026#34; # description: error if disk usage is greater than $CITELLUS_DISK_MAX_PERCENT : ${CITELLUS_DISK_MAX_PERCENT=75} if [[ $CITELLUS_LIVE = 0 ]]; then is_required_file \u0026#34;${CITELLUS_ROOT}/df\u0026#34; DISK_USE_CMD=\u0026#34;cat ${CITELLUS_ROOT}/df\u0026#34; else DISK_USE_CMD=\u0026#34;df -P\u0026#34; fi result=$($DISK_USE_CMD |awk -vdisk_max_percent=$CITELLUS_DISK_MAX_PERCENT \u0026#39;/^\\/dev/ \u0026amp;\u0026amp; substr($5, 0, length($5)-1) \u0026gt; disk_max_percent { print $6,$5 }\u0026#39;) if [ -n \u0026#34;$result\u0026#34; ]; then echo \u0026#34;${result}\u0026#34; \u0026gt;\u0026amp;2 exit $RC_FAILED else exit $RC_OKAY fi Ready for deep dive on tests? There are more tests for OpenStack at the moment as this is the speciality where it started, but it’s open and able to extend to whatever is needed.\nEach test should take care of checking if it should run or not and output return code and stderr. Wrapper just runs all the tests or specific ones (filtering options)\nHow to start a new plugin (example) Write a script in ~/~/.../plugins/core/rhev/hosted-engine.sh chmod +x hosted-engine.sh Requirements: return code must be $RC_OKAY (ok), $RC_FAILED (failed) or $RC_SKIPPED (skipped) Messages to be printed on stderr are displayed on failed or ‘skipped’ if verbose enabled Running against ‘sosreport’, CITELLUS_ROOT contains path to sosreport folder provided. CITELLUS_LIVE contains 0 or 1 if running against live or not How to start a new plugin (continuation) if [ “$CITELLUS_LIVE” = “0” ]; then grep -q ovirt-hosted-engine-ha $CITELLUS_ROOT/installed-rpms returncode=$? if [ “x$returncode” == “x0” ]; then exit $RC_OKAY else echo “ovirt-hosted-engine is not installed “ \u0026gt;\u0026amp;2 exit $RC_FAILED fi else echo “Not running on Live system” \u0026gt;\u0026amp;2 exit $RC_SKIPPED fi How to start a new plugin (with functions) # Load common functions [ -f \u0026#34;${CITELLUS_BASE}/common-functions.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;${CITELLUS_BASE}/common-functions.sh\u0026#34; if is_rpm ovirt-hosted-engine-ha; then exit $RC_OKAY else echo “ovirt-hosted-engine is not installed “ \u0026gt;\u0026amp;2 exit $RC_FAILED fi How to test your plugin? Use tox to run some UT\u0026rsquo;s (utf8, bashate, python 2.7, python 3.5)\nSpecify the plugin to use:\n[piranzo@host citellus]$ ~/citellus/citellus.py sosreport-20170724-175510/crta02 -i hosted-engine.sh _________ .__ __ .__ .__ \\_ ___ \\|__|/ |_ ____ | | | | __ __ ______ / \\ \\/| \\ __\\/ __ \\| | | | | | \\/ ___/ \\ \\___| || | \\ ___/| |_| |_| | /\\___ \\ \\______ /__||__| \\___ \u0026gt;____/____/____//____ \u0026gt; \\/ \\/ \\/ mode: fs snapshot sosreport-20170724-175510/crta02 # ~/~/.../plugins/core/rhev/hosted-engine.sh: failed “ovirt-hosted-engine is not installed “ What is Magui Introduction Citellus works on individual sosreports against a set of tests (all by default), but some problems require checks across several systems. For example, Galera requires to check seqno across all controllers running database.\nWhat does M.a.g.u.i. Does? It runs citellus against each sosreport, gathers and groups the data per plugin. Runs its own plugins against the data received to highlight issues that depend on several systems Allows to grab remote host data via ansible host lists How does it looks like? It’s delivered in citellus repo and can be executed by specifying sosreports:\n[piranzo@collab-shell]$ ~/citellus/magui.py * -i seqno _ _( )_ Magui: (_(ø)_) /(_) Multiple Analisis Generic Unifier and Interpreter \\| |/ {\u0026#39;~/~/.../plugins/core/openstack/mysql/seqno.sh\u0026#39;: {\u0026#39;controller0\u0026#39;: {\u0026#39;err\u0026#39;: u\u0026#39;2b65adb0-787e-11e7-81a8-26480628c14c:285019879\\n\u0026#39;, \u0026#39;out\u0026#39;: u\u0026#39;\u0026#39;, \u0026#39;rc\u0026#39;: 10}, \u0026#39;controller1\u0026#39;: {\u0026#39;err\u0026#39;: u\u0026#39;2b65adb0-787e-11e7-81a8-26480628c14c:285019879\\n\u0026#39;, \u0026#39;out\u0026#39;: u\u0026#39;\u0026#39;, \u0026#39;rc\u0026#39;: 10}, \u0026#39;controller2\u0026#39;: {\u0026#39;err\u0026#39;: u\u0026#39;2b65adb0-787e-11e7-81a8-26480628c14c:285019878\\n\u0026#39;, \u0026#39;out\u0026#39;: u\u0026#39;\u0026#39;, \u0026#39;rc\u0026#39;: 10}}} On this example, UUID and SEQNO is shown for each controller and we can see that controller 2 has different SEQNO to the other two nodes. Next steps with Magui Plugins as of this writing: Aggregate data from citellus sorted by plugin for quick comparison Show \u0026lsquo;metadata\u0026rsquo; extension separated to quickly compare across values pipeline-yaml different across sosreports seqno and highest seqno in Galera release check across hosts Action Items Add more plugins! Evangelize about the tool so we can work together in solving our common issues on the same framework. Get moving fast enough that the tool has continuity, other tools just died by having a ‘solo’ developer working on spare time Start implementing more tests in Magui that provide real intelligence (for example we do report and check on seqno or pipeline-yaml but lot of other issues can benefit from this). Are you still there? THANK YOU FOR ATTENDING!!\nQuestions? For additional questions, come to #citellus on Freenode or email us:\n\u0026lt;mailto:citellus AT googlegroups.com.com\u0026gt; https://groups.google.com/forum/#!forum/citellus Other resources Blog posts: Citellus tagged posts\nCitellus Framework for detecting known issues\nMagui for analysis of issues across several hosts\nJenkins for running CI tests\ni18n and bash\nRecent changes in Magui and Citellus\nDevConf.cz 2018 recording\nDevConf.cz 2018 https://devconfcz2018.sched.com/event/DJXG/detect-pitfalls-of-osp-deployments-with-citellus\n","permalink":"https://iranzo.io/presentations/citellus/devconfcz-2018/devconfcz2018-presentation-revealmd/","summary":"\u003ch2 id=\"citellus\"\u003e\u003ca href=\"https://github.com/citellusorg/citellus\"\u003eCitellus\u003c/a\u003e:\u003c/h2\u003e\n\u003ch3 id=\"detecting-common-pitfalls-of-deployments\"\u003eDetecting common pitfalls of deployments\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"citellus.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/citellusorg/citellus\"\u003ehttps://github.com/citellusorg/citellus\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eRecording at \u003ca href=\"https://www.youtube.com/watch?v=SDzzqrUdn5A\"\u003ehttps://www.youtube.com/watch?v=SDzzqrUdn5A\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-is-citellus\"\u003eWhat is Citellus?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCitellus is a framework populated by community-contributed scripts that automate detecting problems, including configuration issues, conflicts with package versions, and more.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"history-how-did-was-it-started\"\u003eHistory: how did was it started?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe tool, started by Robin Černín after a long weekend shift checking one and over again several sosreports for the same data on different hosts.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIt started with some tests + shell wrapper, and was added a python wrapper by Pablo Iranzo to bring in more powerful features.\u003c/p\u003e","title":"DevConf.cz 2018: Citellus - Detecting common pitfalls of deployments"},{"content":"Presentation at DevConf.cz 2018! As highlighted in the prior edition of the \u0026lsquo;What\u0026rsquo;s new\u0026rsquo;, we got a slot for DevConf.cz 2018.\nDuring that slot, my colleagues Martin, Pablo and myself were presenting on the history and basics of Citellus and how it helps on debugging issues and providing faster analysis of already known ones.\nIt is possible to watch the recording at https://www.youtube.com/watch?v=SDzzqrUdn5A and the slides used at: https://github.com/citellusorg/citellus/blob/master/doc/devconfcz2018-presentation-revealmd.md Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2018/01/27/devconf.cz-2018/","summary":"\u003ch2 id=\"presentation-at-devconfcz-2018\"\u003ePresentation at DevConf.cz 2018!\u003c/h2\u003e\n\u003cp\u003eAs highlighted in the prior edition of the \u003ca href=\"/blog/2018/01/16/recent-changes-in-magui-and-citellus/\"\u003e\u0026lsquo;What\u0026rsquo;s new\u0026rsquo;\u003c/a\u003e, we got a slot for DevConf.cz 2018.\u003c/p\u003e\n\u003cp\u003eDuring that \u003ca href=\"https://devconfcz2018.sched.com/event/DJXG/detect-pitfalls-of-osp-deployments-with-citellus\"\u003eslot\u003c/a\u003e, my colleagues Martin, Pablo and myself were presenting on the history and basics of Citellus and how it helps on debugging issues and providing faster analysis of already known ones.\u003c/p\u003e\n\u003cp\u003eIt is possible to watch the recording at \u003ca href=\"https://www.youtube.com/watch?v=SDzzqrUdn5A\"\u003ehttps://www.youtube.com/watch?v=SDzzqrUdn5A\u003c/a\u003e and the slides used at:\n\u003ca href=\"https://github.com/citellusorg/citellus/blob/master/doc/devconfcz2018-presentation-revealmd.md\"\u003ehttps://github.com/citellusorg/citellus/blob/master/doc/devconfcz2018-presentation-revealmd.md\u003c/a\u003e\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"DevConf.cz 2018!"},{"content":"What\u0026rsquo;s new? During recent weeks we\u0026rsquo;ve been coding and performing several changes to Citellus and Magui.\nChecking the latest logs or list of issues open and closed on github is probably not an easy task or the best way to get \u0026lsquo;up-to-date\u0026rsquo; with changes, so I\u0026rsquo;ll try to compile a few here.\nFirst of all, we\u0026rsquo;re going to present it at DevConf.cz 2018, so come stop-by if assisting :-)\nSome of the changes include\u0026hellip;\nCitellus New functions for bash scripts! We\u0026rsquo;ve created lot of functions to check different things: installed rpm rpm over specific version compare dates over X days regexp in file etc.. Functions do allow to do quicker plugin development. save/restore options so they can be loaded automatically for each execution Think of enabled filters, excluded, etc metadata added for plugins and returned as dictionary plugin has a unique ID for all installations based on plugin relative path and plugin name We do use that ID in magui to select the plugin data we\u0026rsquo;ll be acting on plugin priority! Plugins are assigned a number between 0 and 1000 that represents how likely it\u0026rsquo;s going to affect your environment, and you can filter also on it with --prio extended via \u0026rsquo;extensions\u0026rsquo; to provide support for other plugins moved prior plugins to be core extension ansible playbook support via ansible-playbook command metadata plugins that just generate metadata (hostname, date for sosreport, etc) Web Interface!! David Valee Delisle did a great job on preparing an html that loads citellus.json and shows it graphically. Thanks to his work, we did extended some other features like priority, categories, etc that are calculated via citellus and consumed via citellus-www. Interface can also load magui.json (with ?json=magui.json) and show it\u0026rsquo;s output. We did extend citellus to take --web to automatically create the json named citellus.json on the folder specified with -o and copy the citellus.html file there. So if you provide sosreports over http, you can point to citellus.html to see graphical status! (check latest image at citellus website as www.png ) Increased plugin count! Now we do have more than 119 across different categories A new plugin in python reboot.py that checks for unexpected reboots Spectre/Meltdown security checks! Magui If there\u0026rsquo;s an existing citellus.json magui does load it to speed it up process across multiple sosreports. Magui can also use ansible-playbook to copy citellus program to remote host and run there the command, and bring back the generated citellus.json so you can quickly run citellus across several hosts without having to manually perform operations or generate sosreports. Moved prior data to two plugins: citellus-outputs Citellus plugins output arranged by plugin and sosreport citellus-metadata Outputs metadata gathered by metadata plugins in citellus arranged by plugin and sosreport First plugins that compare data received from citellus on global level Plugins are written in python and use each plugin id to just work on the data they know how to process pipeline-yaml Checks if pipeline.yaml and warns if is different across hosts seqno Checks latest galera seqno on hosts release Reports RHEL release across hosts and warns if is different across hosts Enable quiet mode on the data received from citellus as well as local plugins, so only outputs with ERROR or different output on sosreports is shown, even on magui plugins. Wrap up! As you can see we\u0026rsquo;ve been busy trying to improve plugins, Citellus framework and Magui as well.\nWe\u0026rsquo;ve been also busy demonstrating to others it\u0026rsquo;s value and raising lot of new issues and closing them with our commits (294 requests closed so far).\nSo, come and tell us what else are you missing or how can we improve it to suit your needs (or code them yourself and submit a review!) Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2018/01/16/recent-changes-in-magui-and-citellus/","summary":"\u003ch2 id=\"whats-new\"\u003eWhat\u0026rsquo;s new?\u003c/h2\u003e\n\u003cp\u003eDuring recent weeks we\u0026rsquo;ve been coding and performing several changes to \u003ca href=\"/blog/2017/07/26/citellus-framework-for-detecting-known-issues-in-systems./\"\u003eCitellus\u003c/a\u003e and \u003ca href=\"/blog/2017/07/31/magui-for-analysis-of-issues-across-several-hosts./\"\u003eMagui\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eChecking the latest logs or list of issues open and closed on github is probably not an easy task or the best way to get \u0026lsquo;up-to-date\u0026rsquo; with changes, so I\u0026rsquo;ll try to compile a few here.\u003c/p\u003e\n\u003cp\u003eFirst of all, we\u0026rsquo;re going to present it at \u003ca href=\"https://devconfcz2018.sched.com/event/DJXG/detect-pitfalls-of-osp-deployments-with-citellus\"\u003eDevConf.cz 2018\u003c/a\u003e, so come stop-by if assisting :-)\u003c/p\u003e\n\u003cp\u003eSome of the changes include\u0026hellip;\u003c/p\u003e","title":"Recent changes in Magui and Citellus"},{"content":"Introduction In order to improve Citellus and Magui, we did implement some Unit testing to improve code quality.\nThe tests written were made in python and with some changes it was also possible to validate the actual tests.\nAlso, we did prepare the strings in python using gettext library so the actual messages can be translated to the language of choice (defaults to en, but can be changed via --lang modifier of citellus).\nBashate for bash code validation One of the things I did miss was to have some kind of tox8 for validate format, and locate some errors. After some research I came to bashate, and as it was written in python was very easy to integrate:\nUpdate test-requirements.txt to request bashate for \u0026rsquo;tests'\nEditing tox.ini to add a new section\n[testenv:bashate] commands = bash -c \u0026#39;find citellus -name \u0026#34;*.sh\u0026#34; -type f -print0 | xargs -0 bashate -i E006\u0026#39; This change makes that execution of tox also pulls the output of bashate so all the integration already done for CI, was automatically update to do bash formatting too :-)\nBash i18n Another topic that was interesting is the ability to easily write code in one language and via poedit or equivalent editors, be able to localize it.\nIn python is more or less easy as we did for citellus code, but I wasn\u0026rsquo;t aware of any way of doing that for bash scripts (such as the plugins we do use for citellus).\nDoing a simple man bash gives some hints somewhat hidden:\n--dump-po-strings Equivalent to -D, but the output is in the GNU gettext po (portable object) file format. So, bash has a way to dump po strings (to be edited with poedit or your editor of choice), so only a bit more search was required to find how to really do it.\nApparently is a lot easier than I expected, as long as we take some considerations:\nLANG shouldn\u0026rsquo;t be C as it disables i18n Environment variable TEXTDOMAIN should indicate the filename containing the translated strings. Environment variable TEXTDOMAINDIR should contain the path to the root of the folder containing the translations, for example: TEXTDOMAIN=citellus/locale And language file for en as: citellus/locale/en/LC_MESSAGES/$TEXTDOMAIN.mo Now, the \u0026ldquo;trickier\u0026rdquo; part was to prepare scripts\u0026hellip;\n# Legacy way echo \u0026#34;String\u0026#34; # i18n way echo $\u0026#34;String\u0026#34; # Difficult... isn\u0026#39;t it? This change makes \u0026lsquo;bash\u0026rsquo; to lock for the string inside $TEXTDOMAINDIR/locale/$LANG/LC_MESSAGES/$TEXTDOMAIN.mo and do on the fly replacement of the strings for the translated ones (or fallback to the one echoed).\nIn citellus we did implement it by exporting the extra variables defined above, so scripts, as well as framework is ready for translation!.\nJust in case, some remarks:\nI found some complains when same script outputs the same string in several places, what I did, is to create a VAR and echo that var.\nAs we\u0026rsquo;ve strings in citellus.py, magui.py, etc and the bash files, I did update a script to extract the required strings:\n# Extract python strings python setup.py extract_messages -F babel.cfg -k _L # Extract bash strings find citellus -name \u0026#34;*.sh\u0026#34; -exec bash --dump-po-strings \u0026#34;{}\u0026#34; \\; \u0026gt; citellus/locale/citellus-plugins.pot # Merge bash and python strings msgcat -F citellus/locale/citellus.pot citellus/locale/citellus-plugins.pot \u0026gt; citellus/locale/citellus-new.pot # Move file to destination cat citellus/locale/citellus-new.pot \u0026gt; citellus/locale/citellus.pot In this way, we\u0026rsquo;re ready to use on editor to translate all the strings for the whole citellus + plugins.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2017/10/26/i18n-and-bash8-in-bash/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn order to improve \u003ca href=\"/blog/2017/07/26/citellus-framework-for-detecting-known-issues-in-systems./\"\u003eCitellus\u003c/a\u003e and \u003ca href=\"https://iranzo.io/blog/2017/07/31/magui-for-analysis-of-issues-across-several-hosts./\"\u003eMagui\u003c/a\u003e, we did implement some \u003ca href=\"/blog/2017/08/17/jenkins-for-running-ci-tests/\"\u003eUnit testing\u003c/a\u003e to improve code quality.\u003c/p\u003e\n\u003cp\u003eThe tests written were made in python and with some changes it was also possible to validate the actual tests.\u003c/p\u003e\n\u003cp\u003eAlso, we did prepare the strings in python using gettext library so the actual messages can be translated to the language of choice (defaults to en, but can be changed via \u003ccode\u003e--lang\u003c/code\u003e modifier of citellus).\u003c/p\u003e","title":"i18n and 'bash8' in bash"},{"content":"Why? While working on Citellus and Magui it soon became evident that Unit testing for validating the changes was a requirement.\nInitially, using a .travis.yml file contained in the repo and the free service provided by https://travis-ci.org we soon got https://github.com repo providing information about if the builds succeeded or not.\nWhen it was decided to move to https://gerrithub.io to work in a more similar way to what is being done in upstream, we improved on the code commenting (peer review), but we lost the ability to run the tests in an automated way until the change was merged into github.\nAfter some research, it became more or less evident that another tool, like Jenkins was required to automate the UT process and report to individual reviews about the status.\nSetup Some initial steps are required for integration:\nCreate ssh keypair for Jenkins to use Creating github account to be used by Jenkins and configuring above ssh keypair Login into gerrithub with that account Setup Jenkins and build jobs Allow on the parent project, access to Jenkins github account permission to +1/-1 on Verify In order to setup the Jenkins environment a new VM was spawned in one of our RHV servers.\nThis VM was installed with:\n20 Gigabytes of HDD 2 Gigabytes of RAM 2 VCPU Red Hat Enterprise Linux 7 \u0026lsquo;base install\u0026rsquo; Tuning the OS RHEL7 provides a stable environment for run on, but at the same time we were lacking some of the latest tools we\u0026rsquo;re using for the builds.\nAs a dirty hack, it was altered in what is not a recommended way, but helped to quickly check as proof of concept if it would work or not.\nOnce OS was installed, some commands (do not run in production) were used:\npip install pip # to upgrade pip pip install -U tox # To upgrade to 2.x version # Install python 3.5 on the system yum -y install openssl-devel gcc wget https://www.python.org/ftp/python/3.5.0/Python-3.5.0.tgz tar xvzf Python-3.5-0.tgz cd Python* ./configure # This will install in alternate folder in system not to replace user-wide python version make altinstall # this is required to later allow tox to find the command as \u0026#39;jenkins\u0026#39; user ln -s /usr/local/bin/python3.5 /usr/bin/ Installing Jenkins For the Jenkins installation it\u0026rsquo;s easier, there\u0026rsquo;s a \u0026lsquo;stable\u0026rsquo; repo for RHEL and the procedure is documented:\nwget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key yum install jenkins java chkconfig jenkins on service jenkins start firewall-cmd --zone=public --add-port=8080/tcp --permanent firewall-cmd --zone=public --add-service=http --permanent firewall-cmd --reload This will install and start Jenkins and enable the firewall to access it.\nIf you can get to the url of your server at the port 8080, you\u0026rsquo;ll be presented an initial procedure for installing Jenkins.\nDuring it, you\u0026rsquo;ll be asked for a password on a file on disk and you\u0026rsquo;ll be prompted to create an user we\u0026rsquo;ll be using from now on to configure.\nAlso, we\u0026rsquo;ll be offered to deploy the most common set of plugins, choose that option, and later we\u0026rsquo;ll add the gerrit plugin and Python.\nConfigure Jenkins Once we can login into gerrit, we need to enter the administration area, and install new plugins and install Gerrit Trigger.\nAbove link details how to do most of the setup, in this case, for gerrithub, we required:\nHostname: our hostname Frontend URL: https://review.gerrithub.io SSH Port: 29418 Username: our-github-jenkins-user SSH private key: path_to_private_sshkey Once done, click on Test Connection and validate if it worked.\nAt the time of this writing, version reported by plugin was 2.13.6-3044-g7e9c06d when connected to gerrithub.io.\nCreating a Job Now, we need to create a Job (first option in Jenkins list of jobs).\nName: Citellus Discard older executions: Max number of executions to keep: 10 Source code Origin: Git URL: ssh://\u0026lt;username\u0026gt;@review.gerrithub.io:29418/citellusorg/citellus Credentials: jenkins (Created based on the ssh keypair defined above) Branches to build: $GERRIT_REFNAME. Advanced Refspec: $GERRIT_REFSPEC Add additional behaviours Strategy for choosing what to build: Choosing strategy Gerrit Trigger Triggers for launch: Change Merged Commend added with regexp: recheck Patchset created Ref Updated Gerrit Project: Type: plain Pattern: citellusorg/citellus Branches: Type: Path Pattern: ** Execute: Python script: import os import tox os.chdir(os.getenv(\u0026#34;WORKSPACE\u0026#34;)) # environment is selected by ``TOXENV`` env variable tox.cmdline() From this point, any new push (review) made against Gerrit will trigger a Jenkins build (in this case, running tox). Additionally, a manual trigger of the job can be executed to validate the behavior.\nChecking execution In our project, tox checks some UT\u0026rsquo;s on python 2.7, and python 3.5, as well as python\u0026rsquo;s Flake8 compliance.\nNow, Jenkins will build, and post messages on the review, stating that the build has started and the results of it, setting also the \u0026lsquo;Verified\u0026rsquo; flag.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2017/08/17/jenkins-for-running-ci-tests/","summary":"\u003ch2 id=\"why\"\u003eWhy?\u003c/h2\u003e\n\u003cp\u003eWhile working on \u003ca href=\"https://iranzo.io/blog/2017/07/26/citellus-framework-for-detecting-known-issues-in-systems./\"\u003eCitellus\u003c/a\u003e and \u003ca href=\"https://iranzo.io/blog/2017/07/31/magui-for-analysis-of-issues-across-several-hosts./\"\u003eMagui\u003c/a\u003e it soon became evident that Unit testing for validating the changes was a requirement.\u003c/p\u003e\n\u003cp\u003eInitially, using a \u003ccode\u003e.travis.yml\u003c/code\u003e file contained in the repo and the free service provided by \u003ca href=\"https://travis-ci.org\"\u003ehttps://travis-ci.org\u003c/a\u003e we soon got \u003ca href=\"https://github.com\"\u003ehttps://github.com\u003c/a\u003e repo providing information about if the builds succeeded or not.\u003c/p\u003e\n\u003cp\u003eWhen it was decided to move to \u003ca href=\"https://gerrithub.io\"\u003ehttps://gerrithub.io\u003c/a\u003e to work in a more similar way to what is being done in upstream, we improved on the code commenting (peer review), but we lost the ability to run the tests in an automated way until the change was merged into github.\u003c/p\u003e","title":"Jenkins for running CI tests"},{"content":"Background Citellus allows to check a sosreport against known problems identified on the provided tests.\nThis approach is easy to implement and easy to test but has limitations when a problem can span across several hosts and only the problem reveals itself when a general analysis is performed.\nMagui tries to solve that by running the analysis functions inside citellus across a set of sosreports, unifying the data obtained per citellus plugin.\nAt the moment, Magui just does the grouping of the data and visualization, for example, give it a try with the seqno plugin of citellus to report the sequence number in Galera database:\n[user@host folder]$ magui.py * -f seqno # (filtering for ‘seqno’ plugins). {\u0026#39;/home/remote/piranzo/citellus/citellus/plugins/openstack/mysql/seqno.sh\u0026#39;: {\u0026#39;ctrl0.localdomain\u0026#39;: {\u0026#39;err\u0026#39;: \u0026#39;08a94e67-bae0-11e6-8239-9a6188749d23:36117633\\n\u0026#39;, \u0026#39;out\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;rc\u0026#39;: 0}, \u0026#39;ctrl1.localdomain\u0026#39;: {\u0026#39;err\u0026#39;: \u0026#39;08a94e67-bae0-11e6-8239-9a6188749d23:36117633\\n\u0026#39;, \u0026#39;out\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;rc\u0026#39;: 0}, \u0026#39;ctrl2.localdomain\u0026#39;: {\u0026#39;err\u0026#39;: \u0026#39;08a94e67-bae0-11e6-8239-9a6188749d23:36117633\\n\u0026#39;, \u0026#39;out\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;rc\u0026#39;: 0}}} Here, we can see that the sequence number on the logs is the same for the hosts.\nThe goal, once this has been discussed and determined, is to write plugins that get the raw data from citellus and applies logic on top by parsing the raw data obtained by the increasing number of citellus plugins and is able to detect issues like, for example:\nGalera seqno cluster status NTP synchronization across nodes etc Enjoy! (and if you do, you can Buy Me a Coffee ) PD: We\u0026rsquo;ve proposed this to be a talk in upcoming OSP Summit 2017 in Sydney, so if you want to see us there, don\u0026rsquo;t forget to vote on https://www.openstack.org/summit/sydney-2017/vote-for-speakers#/19095\n","permalink":"https://iranzo.io/blog/2017/07/31/magui-for-analysis-of-issues-across-several-hosts./","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2017/07/26/citellus-framework-for-detecting-known-issues-in-systems./\"\u003eCitellus\u003c/a\u003e allows to check a sosreport against known problems identified on the provided tests.\u003c/p\u003e\n\u003cp\u003eThis approach is easy to implement and easy to test but has limitations when a problem can span across several hosts and only the problem reveals itself when a general analysis is performed.\u003c/p\u003e\n\u003cp\u003eMagui tries to solve that by running the analysis functions inside citellus across a set of sosreports, unifying the data obtained per citellus plugin.\u003c/p\u003e","title":"Magui for analysis of issues across several hosts."},{"content":"Background Since I became Technical Account Manager for Cloud and later as Software Maintenance Engineer for OpenStack, I became officially part of Red Hat Support.\nWe do usually diagnose issues based on data from the affected systems, sometimes from one system, and most of the times, from several at once.\nIt might be controllers nodes for OpenStack, Computes running instances, IdM, etc\nIn order to make it easier to grab the required information, we rely on sosreport.\nSosreport has a set of plugins for grabbing required information from system, ranging from networking configuration, installed packages, running services, processes and even for some components, it can also check API, database queries, etc.\nBut that\u0026rsquo;s all, it does data gathering, packaging in a tarball but nothing else.\nIn OpenStack we\u0026rsquo;ve already identified common issues, so we create kbases for them, ranging from covering some documentation gaps, to specific use cases or configuration options.\nMany times, a missed configuration (documented) is causing headaches and can be checked with a simple checks, like TTL in ceilometer or stonith configuration on pacemaker.\nHere is where Citellus comes to play.\nCitellus The Citellus project https://github.com/citellusorg/citellus/ created by my colleague Robin, aims on creating a set of tests that can be executed against a live system or an uncompressed sosreport tarball (it depends on the test if it applies to one or the other).\nThe philosophy behind is very easy:\nThere\u0026rsquo;s a wrapper citellus.py which allows to select plugins to use, or folder containing plugins, verbosity, etc and a sosreport folder to act against. The wrapper does check the plugins available (can be anything executable from Linux, so bash, Python, etc are there to be used) Then it setups some environment variables like the path to find the data and proceeds to execute the plugins against, recording the output of them. The plugins, on their side, determine if: Plugin should be run or skipped if it\u0026rsquo;s a live system, a sosreport Plugin should run because of required file or package missing Provide return code of: $RC_OKAY for success $RC_FAILED for failure $RC_SKIPPED for skip anything else (Undetermined error) Provide stderr with relevant messages: Reason to be skipped Reason for failure etc The wrapper then sorts the output, and prints it based on settings (grouping skipped and ok by default) and detailing failures. You can check the provided plugins on the Github repository (and hopefully also collaborate sending yours).\nOur target is to keep plugins easy to write, so we can extend the plugin set as much as possible, highlighting were focus should be put at first and once typical issues are ruled out, check on the deeper analysis.\nEven if we\u0026rsquo;ve started with OpenStack plugins (that\u0026rsquo;s what we do for a living), the software is open to check against whatever is there, and we\u0026rsquo;ve reached to other colleagues in different speciality areas to provide more feedback or contributions to make it even more useful.\nAs Citellus works with sosreports it is easy to have it installed locally and test new tests.\nWriting a new test Leading by the example is probably easier, so let\u0026rsquo;s illustrate how to create a basic plugin for checking if a system is a RHV hosted engine:\n#!/bin/bash if [ \u0026#34;$CITELLUS_LIVE\u0026#34; = \u0026#34;0\u0026#34; ]; ## Checks if we\u0026#39;re running live or not then grep -q ovirt-hosted-engine-ha $CITELLUS_ROOT/installed-rpms ## checks package returncode=$? #stores return code if [ \u0026#34;x$returncode\u0026#34; == \u0026#34;x0\u0026#34; ]; then exit $RC_OKAY else echo \u0026#34;ovirt-hosted-engine is not installed \u0026#34; \u0026gt;\u0026amp;2 #Outputs info exit $RC_FAILED e #returns code to wrapper fi else echo \u0026#34;Not running on Live system\u0026#34; \u0026gt;\u0026amp;2 exit $RC_SKIPPED fi Above example is a bit hacky, as we count on wrapper not to output information if return code is $RC_OKAY, so it should have another conditional to write output or not.\nHow to debug? Easiest way to do trial-error would be to create a new folder for your plugins to test and use something like this:\n[user@host mytests]$ ~/citellus/citellus.py /cases/01884438/sosreport-20170724-175510/ycrta02.rd1.rf1 ~/mytests/ [-d debug] DEBUG:__main__:Additional parameters: [\u0026#39;/cases/sosreport-20170724-175510/hostname\u0026#39;, \u0026#39;/home/remote/piranzo/mytests/\u0026#39;] DEBUG:__main__:Found plugins: [\u0026#39;/home/remote/piranzo/mytests/ovirt-engine.sh\u0026#39;] _________ .__ __ .__ .__ \\_ ___ \\|__|/ |_ ____ | | | | __ __ ______ / \\ \\/| \\ __\\/ __ \\| | | | | | \\/ ___/ \\ \\___| || | \\ ___/| |_| |_| | /\\___ \\ \\______ /__||__| \\___ \u0026gt;____/____/____//____ \u0026gt; \\/ \\/ \\/ found #1 tests at /home/remote/piranzo/mytests/ mode: fs snapshot /cases/sosreport-20170724-175510/hostname DEBUG:__main__:Running plugin: /home/remote/piranzo/mytests/ovirt-engine.sh # /home/remote/piranzo/mytests/ovirt-engine.sh: failed \u0026#34;ovirt-hosted-engine is not installed \u0026#34; DEBUG:__main__:Plugin: /home/remote/piranzo/mytests/ovirt-engine.sh, output: {\u0026#39;text\u0026#39;: u\u0026#39;\\x1b[31mfailed\\x1b[0m\u0026#39;, \u0026#39;rc\u0026#39;: 1, \u0026#39;err\u0026#39;: \u0026#39;\\xe2\\x80\\x9covirt-hosted-engine is not installed \\xe2\\x80\\x9c\\n\u0026#39;, \u0026#39;out\u0026#39;: \u0026#39;\u0026#39;} That debug information comes from the python wrapper, if you need more detail inside your test, you can try set -x to have bash showing more information about progress.\nKeep always in mind that all functionality is based on return codes and the stderr message to keep it simple.\nEnjoy! (and if you do, you can Buy Me a Coffee ) Post Datum: We\u0026rsquo;ve proposed this to be a talk in upcoming OSP Summit 2017 in Sydney, so if you want to see us there, don\u0026rsquo;t forget to vote on https://www.openstack.org/summit/sydney-2017/vote-for-speakers#/19095\n","permalink":"https://iranzo.io/blog/2017/07/26/citellus-framework-for-detecting-known-issues-in-systems./","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eSince I became Technical Account Manager for Cloud and later as Software Maintenance Engineer for OpenStack, I became officially part of Red Hat Support.\u003c/p\u003e\n\u003cp\u003eWe do usually diagnose issues based on data from the affected systems, sometimes from one system, and most of the times, from several at once.\u003c/p\u003e\n\u003cp\u003eIt might be controllers nodes for OpenStack, Computes running instances, IdM, etc\u003c/p\u003e\n\u003cp\u003eIn order to make it easier to grab the required information, we rely on \u003ca href=\"https://github.com/sosreport/sos\"\u003esosreport\u003c/a\u003e.\u003c/p\u003e","title":"Citellus: framework for detecting known issues in systems."},{"content":"InfraRed is tool that allows to install/provision OpenStack. You can find the documentation for the project at http://infrared.readthedocs.io.\nAlso, developers and users are online in FreeNode at #infrared channel.\nWhy InfraRed? Deploying OSP with OSP-d (TripleO) requires several setup steps for preparation, deployment, etc. InfraRed simplifies them by automating with ansible most of those steps and configuration.\nIt allows to deploy several OSP versions Allows to ease connection to installed vm roles (Ceph, Computes, Controllers, Undercloud) Allows to define working environments so one InfraRed-running host can be used to manage different environments and much more\u0026hellip; Setup of InfraRed-running host Setting InfraRed is quite easy, at the moment the version 2 (branch on github) is working pretty well.\nWe\u0026rsquo;ll start with:\nClone GIT repo: git clone https://github.com/redhat-openstack/infrared.git Create a virtualenv so we can proceed with installation, later we\u0026rsquo;ll need to source it before each use. cd infrared ; virtualenv .venv \u0026amp;\u0026amp; source .venv/bin/activate Proceed with upgrade of pip and setuptools (required) and installation of InfraRed pip install --upgrade pip pip install --upgrade setuptools pip install . Remote host setup Once done, we need to setup the requirements on the host we\u0026rsquo;ll use to virtualize, this includes, having the system registered against a repository providing required packages.\nRegister RHEL7 and update: subscription-manager register (provide your credentials) subscription-manager attach --pool= (check pool number first) subscription-manager repos --disable=* for canal in rhel-7-server-extras-rpms rhel-7-server-fastrack-rpms rhel-7-server-optional-fastrack-rpms rhel-7-server-optional-rpms rhel-7-server-rh-common-rpms rhel-7-server-rhn-tools-rpms rhel-7-server-rpms rhel-7-server-supplementary-rpms rhel-ha-for-rhel-7-server-rpms;do subscription-manager repos --enable=$canal; done NOTES OSP7 did not contain RPM packaged version of images, a repo with the images needs to be defined like:\ntime infrared tripleo-undercloud --version $VERSION --images-task import --images-url $REPO_URL Check the parameter values\n--images-task import and --images-url\nCeph failed to install unless --storage-backend ceph was provided (open bug for that) Error reporting IRC or github RFE/BUGS Some bugs/RFE on the way to get implemented some day:\nAllow use of localhost to launch installation against local host Multi env creation, so several OSP-d versions are deployed on the same hypervisor but one launched Automatically add --storage-backend ceph when Ceph nodes defined Using Ansible to deploy InfraRed This is something that I began testing to automate the basic setup, still is needed to decide version to use, and do deployment of infrastructure VM\u0026rsquo;s but does some automation for setting up the hypervisors.\n--- - hosts: all user: root tasks: - name: Install git yum: name: - \u0026#34;git\u0026#34; - \u0026#34;python-virtualenv\u0026#34; - \u0026#34;openssl-devel\u0026#34; state: latest - name: \u0026#34;Checkout InfraRed to /root/infrared folder\u0026#34; git: repo: https://github.com/redhat-openstack/infrared.git dest: /root/infrared - name: Initialize virtualenv pip: virtualenv: \u0026#34;/root/infrared/.venv\u0026#34; name: setuptools, pip - name: Upgrade virtualenv pip pip: virtualenv: \u0026#34;/root/infrared/.venv\u0026#34; name: pip extra_args: --upgrade - name: Upgrade virtualenv setuptools pip: virtualenv: \u0026#34;/root/infrared/.venv\u0026#34; name: setuptools extra_args: --upgrade - name: Install InfraRed pip: virtualenv: \u0026#34;/root/infrared/.venv\u0026#34; name: file:///root/infrared/. This playbook will do checkout of git repo, setup extra pip commands to upgrade virtualenv\u0026rsquo;s deployed pip and setuptools, etc.\nDeploy environment examples This will show the commands that might be used to deploy some environments and some sample timings on a 64Gb RAM host.\nCommon requirements export HOST=myserver.com export HOST_KEY=~/.ssh/id_rsa export ANSIBLE_LOG_PATH=deploy.log Cleanup time infrared virsh --cleanup True --host-address $HOST --host-key $HOST_KEY OSP 9 (3 + 2) Define version to use export VERSION=9 time infrared virsh --host-address $HOST --host-key $HOST_KEY --topology-nodes \u0026#34;undercloud:1,controller:3,compute:2\u0026#34; real 11m19.665s user 3m7.013s sys 1m27.941s time infrared tripleo-undercloud --version $VERSION --images-task rpm real 48m8.742s user 10m35.800s sys 5m23.126s time infrared tripleo-overcloud --deployment-files virt --version 9 --introspect yes --tagging yes --post yes real 43m44.424s user 9m36.592s sys 4m39.188s OSP 8 (3+2) export VERSION=8 time infrared virsh --host-address $HOST --host-key $HOST_KEY --topology-nodes \u0026#34;undercloud:1,controller:3,compute:2\u0026#34; real 11m29.478s user 3m10.174s sys 1m28.276s time infrared tripleo-undercloud --version $VERSION --images-task rpm real 40m47.387s user 9m14.151s sys 4m24.820s time infrared tripleo-overcloud --deployment-files virt --version $VERSION --introspect yes --tagging yes --post yes real 42m57.315s user 9m2.412s sys 4m25.840s OSP 10 (3+2) export VERSION=10 time infrared virsh --host-address $HOST --host-key $HOST_KEY --topology-nodes \u0026#34;undercloud:1,controller:3,compute:2\u0026#34; real 10m54.710s user 2m42.761s sys 1m12.844s time infrared tripleo-undercloud --version $VERSION --images-task rpm real 43m10.474s user 8m34.905s sys 4m3.732s time infrared tripleo-overcloud --deployment-files virt --version $VERSION --introspect yes --tagging yes --post yes real 54m1.111s user 11m55.808s sys 6m1.023s OSP 7 (3+2+3) export VERSION=7 time infrared virsh --host-address $HOST --host-key $HOST_KEY --topology-nodes \u0026#34;undercloud:1,controller:3,compute:2,ceph:3\u0026#34; real 13m46.205s user 3m46.753s sys 1m47.422s time infrared tripleo-undercloud --version $VERSION --images-task import --images-url $URLTOIMAGES real 43m14.471s user 9m45.479s sys 4m53.126s time infrared tripleo-overcloud --deployment-files virt --version $VERSION --introspect yes --tagging yes --post yes --storage-backend ceph real 86m47.471s user 20m2.582s sys 9m42.577s Wrapping-up Please do refer to the InfraRed documentation to get deeper in its possibilities and if interested, consider contributing! Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2017/02/23/infrared-for-deploying-openstack/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/redhat-openstack/infrared/\"\u003eInfraRed\u003c/a\u003e is tool that allows to install/provision OpenStack. You can find the documentation for the project at \u003ca href=\"http://infrared.readthedocs.io\"\u003ehttp://infrared.readthedocs.io\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAlso, developers and users are online in FreeNode at #infrared channel.\u003c/p\u003e\n\u003ch2 id=\"why-infrared\"\u003eWhy InfraRed?\u003c/h2\u003e\n\u003cp\u003eDeploying OSP with OSP-d (TripleO) requires several setup steps for preparation, deployment, etc. InfraRed simplifies them by automating with ansible most of those steps and configuration.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt allows to deploy several OSP versions\u003c/li\u003e\n\u003cli\u003eAllows to ease connection to installed vm roles (Ceph, Computes, Controllers, Undercloud)\u003c/li\u003e\n\u003cli\u003eAllows to define working environments so one InfraRed-running host can be used to manage different environments\u003c/li\u003e\n\u003cli\u003eand much more\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"setup-of-infrared-running-host\"\u003eSetup of InfraRed-running host\u003c/h2\u003e\n\u003cp\u003eSetting InfraRed is quite easy, at the moment the version 2 (branch on github) is working pretty well.\u003c/p\u003e","title":"InfraRed for deploying OpenStack"},{"content":"Introduction I\u0026rsquo;ve started to get familiar with Ansible because, apart of getting more and more accepted for OSP-related tasks and installation, I wanted to automate some tasks we needed to setup some servers for the OpenStack group I work for.\nFirst of all, it\u0026rsquo;s recommended to get latest version of ansible (tested on RHEL7 and Fedora), but in order not to mess with the system python libraries, it\u0026rsquo;s convenient to use python\u0026rsquo;s virtual environments.\nA virtual Environment allows to create a \u0026lsquo;chroot\u0026rsquo;-like environment that might contain different library versions to the one installed with the system (but be careful as if it\u0026rsquo;s not kept track as part of the usually system patching process, it might become a security concern).\nvirtualenvs For creating a virtualenv, we require the package python-virtualenv installed on our system and executing virtualenv and a target folder, for example:\n[iranzo@iranzo ~]$ virtualenv .venv New python executable in /home/iranzo/.venv/bin/python2 Also creating executable in /home/iranzo/.venv/bin/python Installing setuptools, pip, wheel...done. From this point, we\u0026rsquo;ve a base virtualenv installed, but as we would like to install more packages inside we\u0026rsquo;ll first need to \u0026rsquo;enter\u0026rsquo; into it:\n. .venv/bin/activate And from there, we can list the available/installed packages:\n[iranzo@iranzo ~]$ pip list DEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning. appdirs (1.4.0) packaging (16.8) pip (9.0.1) pyparsing (2.1.10) setuptools (34.2.0) six (1.10.0) wheel (0.30.0a0) Now, all packages we install using pip will get installed to this folder, leaving system libraries intact.\nOnce we finished, to return back to system\u0026rsquo;s environment, we\u0026rsquo;ll execute deactivate.\npipsi In order to simplify the management we can make use of pipsi which not only allows to install Python packages as we\u0026rsquo;ll normally do with pip, but also, takes care of doing proper symlinks so the installed packages are available directly for execution.\nIf our distribution provides it, we can install pipsi on our system:\ndnf -y install pipsi But if not, we can use this workaround (for example, on RHEL7)\n# Use pip to install pipsi on the system (should be minor change not affecting other software installed) pip install pipsi From this point, we can use pipsi to take care of installation and maintenance (can do upgrades, removal, etc) of our python packages.\nFor example, we can install ansible by executing:\npipsi install ansible This might fail, as ansible, does some compiling and for doing so, it might require some development libraries on your system, have care of that to satisfy requirements for the packages.\nPrepare for ansible utilization At this point we\u0026rsquo;ve the ansible binary available for execution as pipsi did take care of setting up the required symlinks, etc\nAnsible uses an inventory file (can be provided on command line) so it can connect to the hosts listed there and apply playbooks which define the different actions to perform.\nThis file, for example, can consist of just a simple list of hosts to connect to like:\n192.168.1.1 192.168.1.2 myhostname.net And for starting we create a simple playbook, for example a hardware asset inventory:\n--- - hosts: all user: root tasks: - name: Display inventory of host debug: msg: \u0026#34;{{ inventory_hostname }} | {{ ansible_default_ipv4.address }} | | | {{ ansible_memtotal_mb }} | | | {{ ansible_bios_date }}\u0026#34; This will act on all hosts, as user root and will run a task which prints a debug message crafted based on the contents of some of the facts that ansible gathers on the execution host.\nTo run it is quite easy:\n[iranzo@iranzo labs]$ ansible-playbook -i myhost.net, inventory.yaml PLAY [all] ********************************************************************* TASK [setup] ******************************************************************* ok: [myhost.net] TASK [Display inventory of host] *********************************************** ok: [myhost.net] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;myhost.net | 192.168.1.1 | | | 14032 | | | 01/01/2011\u0026#34; } PLAY RECAP ********************************************************************* myhost.net : ok=2 changed=0 unreachable=0 failed=0 This has connected to the target host, and returned a message with hostname, ip address, some empty fields, total memory and bios date.\nThis is a quite simple script, but for example, we can use ansible to deploy ansible binary on our target host using other modules available, in this case, for simplicity, we\u0026rsquo;ll not be using pipsi for ansible installation.\n--- - hosts: all user: root tasks: - name: Install git yum: name: - \u0026#34;git\u0026#34; - \u0026#34;python-virtualenv\u0026#34; - \u0026#34;openssl-devel\u0026#34; state: latest - name: Install virtualenv pip: virtualenv: \u0026#34;/root/infrared/.venv\u0026#34; name: pipsi - name: Upgrade virtualenv pip pip: virtualenv: \u0026#34;/root/infrared/.venv\u0026#34; name: pip extra_args: --upgrade - name: Upgrade virtualenv setuptools pip: virtualenv: \u0026#34;/root/infrared/.venv\u0026#34; name: setuptools extra_args: --upgrade - name: Install Ansible pip: virtualenv: \u0026#34;/root/infrared/.venv\u0026#34; name: ansible At this point, the system should have ansible available from within the virtualenv we\u0026rsquo;ve created and should be available when executing:\n# Activate python virtualenv . .venv/bin/activate # execute ansible ansible-playbook -i hosts ansible.yaml Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2017/02/20/getting-started-with-ansible/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve started to get familiar with \u003ca href=\"http://www.ansible.com\"\u003eAnsible\u003c/a\u003e because,\napart of getting more and more accepted for OSP-related tasks and\ninstallation, I wanted to automate some tasks we needed to setup some servers\nfor the OpenStack group I work for.\u003c/p\u003e\n\u003cp\u003eFirst of all, it\u0026rsquo;s recommended to get latest version of ansible (tested on\nRHEL7 and Fedora), but in order not to mess with the system python libraries, it\u0026rsquo;s convenient to use python\u0026rsquo;s virtual environments.\u003c/p\u003e","title":"Getting started with Ansible"},{"content":"Since my prior post on Contributing to OpenStack, I liked the idea of using some automated tests to validate functionality and specifically, the corner cases that could arise when playing with the code.\nMost of the errors fixed so far on stampy, were related with some pieces of the code not properly handling UTF or some information returned, etc and still it has improved, the idea of ensuring that prior errors were not put back into the code when some other changes were performed, started to arise to be a priority.\nFor implementing them, I made use of nose, which can be executed with nosetests and are available on Fedora as \u0026lsquo;python-nose\u0026rsquo; and to provide further automation, I\u0026rsquo;ve also relied on tox also inspired n what OpenStack does.\nLet\u0026rsquo;s start with tox: once installed, a new configuration file is created for it, defining the different environments and configuration in a similar way to:\n[tox] minversion = 2.0 envlist = py27,pep8 skipsdist = True [testenv] passenv = CI TRAVIS TRAVIS_* deps = -r{toxinidir}/requirements.txt -r{toxinidir}/test-requirements.txt commands = /usr/bin/find . -type f -name \u0026#34;*.pyc\u0026#34; -delete nosetests \\ [] [testenv:pep8] commands = flake8 [testenv:venv] commands = {posargs} [testenv:cover] commands = coverage report [flake8] show-source = True exclude=.venv,.git,.tox,dist,doc,*lib/python*,*egg,build This file, defines two environments, one for validating pep8 for the python formatting and another one for validating python 2.7.\nThe environment definition for the tests, also performs some commands like executed the aforementioned nosetests to run the defined unit tests.\nAbove tox.ini also mentions requirements.txt and test-requirements.txt, which define the python packages required to validate the program, that will be automatically installed by tox on a virtualenv, so the alternate versions being used, doesn\u0026rsquo;t interfere with the system-wide ones we\u0026rsquo;re using.\nAbout the tests themselves, as nosetests does automatic discovery of tests to perform, I\u0026rsquo;ve created a new folder named tests/ and placed there some files in alphabetically order:\nls -l tests total 28 -rw-r--r--. 1 iranzo iranzo 709 nov 5 16:58 test_00-setup.py -rw-r--r--. 1 iranzo iranzo 739 nov 3 09:56 test_10-alias.py -rw-r--r--. 1 iranzo iranzo 456 nov 3 23:53 test_10-autokarma.py -rw-r--r--. 1 iranzo iranzo 581 nov 3 09:56 test_10-karma.py -rw-r--r--. 1 iranzo iranzo 3544 nov 5 18:19 test_10-process.py -rw-r--r--. 1 iranzo iranzo 477 nov 3 23:15 test_10-quote.py -rw-r--r--. 1 iranzo iranzo 230 nov 3 09:56 test_10-sendmessage.py First one test_00-setup takes the required commands to define the environment, as on each validation run of tox, a new environment should be available not to mask errors that could be overlooked.\n#!/usr/bin/env python # encoding: utf-8 from unittest import TestCase from stampy.stampy import config, setconfig, createdb, dbsql # Precreate DB for other operations to work try: createdb() except: pass # Define configuration for tests setconfig(\u0026#34;token\u0026#34;, \u0026#34;279488369:AAFqGVesZ-81n9sFafLQxUUCVO8_8L3JNEU\u0026#34;) setconfig(\u0026#34;owner\u0026#34;, \u0026#34;iranzo\u0026#34;) setconfig(\u0026#34;url\u0026#34;, \u0026#34;https://api.telegram.org/bot\u0026#34;) setconfig(\u0026#34;verbosity\u0026#34;, \u0026#34;DEBUG\u0026#34;) # Empty karma database in case it contained some leftover dbsql(\u0026#34;DELETE from karma\u0026#34;) dbsql(\u0026#34;DELETE from quote\u0026#34;) dbsql(\u0026#39;UPDATE SQLITE_SEQUENCE SET SEQ=0 WHERE NAME=\u0026#34;quote\u0026#34;\u0026#39;) class TestStampy(TestCase): def test_owner(self): self.assertEqual(config(\u0026#34;owner\u0026#34;), \u0026#34;iranzo\u0026#34;) This file creates the database if none is existing and defines some sample values, like DEBUG level, url for contacting telegram API servers, or even a token that can be used to test the functionality for sending messages.\nAlso, if the database is already existing, empties the karma table, quotes (and sets sequence to 0 to simulate TRUNCATE which is not available on sqlite)\nAn unittest is specified under the class inherited from TestCase imported from unittest, there for each one of the tests we want to performed, a new \u0026lsquo;definition\u0026rsquo; is created and after it an assert is used, for example assertEqual validates that the function call returns the value provided as secondary argument, failing otherwise.\nFrom that point, the tests are performed again in alphabetically order, so be careful in the naming of each tests or define a sequence number to use a top-to-bottom approach that will be probably easier to understand.\nFor example, for karma changes we\u0026rsquo;ve:\n#!/usr/bin/env python # encoding: utf-8 from unittest import TestCase from stampy.stampy import getkarma, updatekarma, putkarma class TestStampy(TestCase): def test_putkarma(self): putkarma(\u0026#34;patata\u0026#34;, 0) self.assertEqual(getkarma(\u0026#34;patata\u0026#34;), 0) def test_getkarma(self): self.assertEqual(getkarma(\u0026#34;patata\u0026#34;), 0) def test_updatekarmaplus(self): updatekarma(\u0026#34;patata\u0026#34;, 2) self.assertEqual(getkarma(\u0026#34;patata\u0026#34;), 2) def test_updatekarmarem(self): updatekarma(\u0026#34;patata\u0026#34;, -1) self.assertEqual(getkarma(\u0026#34;patata\u0026#34;), 1) Which starts by putting a known karma on a word, validating, verifying the query, update the value by a positive number and later, decrease it with a negative one.\nFor the aliases, we use a similar approach, as we also play with the karma changes when an alias is defined:\n#!/usr/bin/env python # encoding: utf-8 from unittest import TestCase from stampy.stampy import getkarma, putkarma, createalias, getalias, deletealias class TestStampy(TestCase): def test_createalias(self): createalias(\u0026#34;patata\u0026#34;, \u0026#34;creilla\u0026#34;) self.assertEqual(getalias(\u0026#34;patata\u0026#34;), \u0026#34;creilla\u0026#34;) def test_getalias(self): self.assertEqual(getalias(\u0026#34;patata\u0026#34;), \u0026#34;creilla\u0026#34;) def test_increasealiaskarma(self): updatekarma(\u0026#34;patata\u0026#34;, 1) self.assertEqual(getkarma(\u0026#34;patata\u0026#34;), 1) # Alias doesn\u0026#39;t get increased as the \u0026#39;aliases\u0026#39; modifications are in # process, not in the individual functions self.assertEqual(getkarma(\u0026#34;creilla\u0026#34;), 0) def test_removealias(self): deletealias(\u0026#34;patata\u0026#34;) self.assertEqual(getkarma(\u0026#34;creilla\u0026#34;), 0) def test_removekarma(self): putkarma(\u0026#34;patata\u0026#34;, 0) self.assertEqual(getkarma(\u0026#34;patata\u0026#34;), 0) Where an alias is created, verified, karma in creased on the word with an alias, and then the aliased value.\nAs noted in the above example, the individual function for the karma doesn\u0026rsquo;t take into consideration the aliases so this must be handled by processing a message set via process(messages) which has been also modified as well as other functions to allow the implementation of individual tests for them.\nThis will for sure end up with some more code rewriting so the functions can be fully tested individually and as a whole, to ensure that the bot behaves as intended\u0026hellip; and many more tests to come to the code.\nAs an end, an example of the execution of tox and the results raised:\ntox py27 installed: coverage==4.2,nose==1.3.7,prettytable==0.7.2 py27 runtests: PYTHONHASHSEED=\u0026#39;604985980\u0026#39; py27 runtests: commands[0] | /usr/bin/find . -type f -name *.pyc -delete py27 runtests: commands[1] | nosetests .................. ---------------------------------------------------------------------- Ran 18 tests in 14.996s OK pep8 installed: coverage==4.2,nose==1.3.7,prettytable==0.7.2 pep8 runtests: PYTHONHASHSEED=\u0026#39;604985980\u0026#39; pep8 runtests: commands[0] | flake8 WARNING:test command found but not installed in testenv cmd: /usr/bin/flake8 env: /home/iranzo/DEVEL/private/stampython/.tox/pep8 Maybe you forgot to specify a dependency? See also the whitelist_externals envconfig setting. __________________________________________________________________________ summary ___________________________________________________________________________ py27: commands succeeded pep8: commands succeeded congratulations :) If you\u0026rsquo;re using a CI system, like \u0026lsquo;Travis\u0026rsquo;, which is also available to https://github.com repositories, a .travis.yml can be added to the repository to ensure those tests are performed automatically on each code push:\nlanguage: python python: - 2.7 notifications: email: false before_install: - pip install pep8 - pip install misspellings - pip install nose script: # Run pep8 on all .py files in all subfolders # (I ignore \u0026#34;E402: module level import not at top of file\u0026#34; # because of use case sys.path.append(\u0026#39;..\u0026#39;); import \u0026lt;module\u0026gt;) - find . -name \\*.py -exec pep8 --ignore=E402,E501 {} + - find . -name \u0026#39;*.py\u0026#39; | misspellings -f - - nosetests Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2016/11/05/unit-testing-for-stampy/","summary":"\u003cp\u003eSince my prior post on \u003ca href=\"/blog/2016/07/21/contributing-to-openstack/\"\u003eContributing to OpenStack\u003c/a\u003e, I liked the idea of using some automated tests to validate functionality and specifically, the corner cases that could arise when playing with the code.\u003c/p\u003e\n\u003cp\u003eMost of the errors fixed so far on stampy, were related with some pieces of the code not properly handling UTF or some information returned, etc and still it has improved, the idea of ensuring that prior errors were not put back into the code when some other changes were performed, started to arise to be a priority.\u003c/p\u003e","title":"Unit testing for stampy"},{"content":"Contributing to an OpenSource project might take some time at the beginning, the good thing with OpenStack is that there are lot of guides on how to start and collaborate.\nWhat I did is to look for a bug in the project tagged as low-hanging-fruit, this allows to browse a large list of bugs that are classified as easy, so they are the best place for new starters to get familiar with the workflow.\nI did found an issue with weight which is supposed to be an integer, that was doing a conversion from float to integer (0.1 -\u0026gt; 0) which was considered invalid, and instead an error should be returned.\nWhen I checked the Neutron-LBaaS I found out where the problem was, as the value provided, was being converted to integer instead of validating it.\nBefore contributing you need to:\ncreate a LaunchPad account, join the OpenStack Foundation account as \u0026lsquo;Foundation Member\u0026rsquo; and setup a https://review.openstack.org/ account as described on Account Setup section in the Developer\u0026rsquo;s manual. Don\u0026rsquo;t bypass the git configuration steps at above guide as we\u0026rsquo;ll need them for next steps. Submitting a change is quite easy:\n# Select the project, \u0026#39;neutron-lbaas\u0026#39; for me each=\u0026#39;neutron-lbaas\u0026#39; git clone git@github.com:openstack/$each.git cd $each # This setups git-review, getting required hooks, etc git-review -s # create a new branch so we can keep our changes separate git branch new-branch # Edit files with changes git add $files git commit -m \u0026#34;Descriptive message\u0026#34; # send to upstream for review: git-review git-review will output an url you can use to preview your change, and the hooks will automatically add a \u0026lsquo;Change-ID\u0026rsquo; so subsequent changes are linked to it.\nNOTE: full reference is available at the Developer\u0026rsquo;s Guide\nThe biggest issue started here:\nIn order to not require a new function to validate integers, I\u0026rsquo;ve used the one for non-negative which already does this tests, but one of the reviewers suggested to write a function Functions were imported from neutron-lib so I submitted a second change to neutron-lib project As the change in neutron-lib couldn\u0026rsquo;t be marked as dependent as neutron-lbaas uses the build the version already published, I had to define another interim version of the function so that neutron-lbaas can use it in the meantime and raise another bug, to later remove this interim function once than neutron-lib includes the validate_integer function As part of the comments on neutron-lib review, it was found that it would be nice to validate values, so after some discussion, I moved to use the internal validate_values. Of course, validate_values is just doing data in valid_values, so it fails if data or valid_values are not comparable and doesn\u0026rsquo;t do conversion of depending on the values itselves, so this spin-off another review for improving the ´validate_values´ function. At the moment, I\u0026rsquo;m trying to close the one to neutron-lib to use the function already defined, and have it merged, and then continue with the other steps, like removing the interim function in neutron-lbaas and work on enhancing validate_values and close all the dependant LaunchPad bugs I\u0026rsquo;ve created for tracking.\nMy experience so far, is that sometimes it might be a bit difficult, as git-review is a collaborative environment so different opinions are being shared with different approaches and some of them are \u0026rsquo;easier\u0026rsquo; and some others \u0026lsquo;pickier\u0026rsquo; like having an \u0026rsquo;extra space\u0026rsquo;, etc.\nOf course, all the code is checked by some automation engines when submitted, which validates that the code still builds, no formatting errors, etc but many of them can be executed locally by using tox, which allows to perform part of the tests like:\ntox -e pep8 tox -e py27 tox -e coverage To respectively, validate pep8 formatting (line length, spaces around operators, docstrings formatting, etc) and to run another set of tests like the ones you define.\nAfter each set of changes performed to apply the feedback received, ensure to:\n# Add the modified files to a commit git add $files_modified # Create the commit with the changes git commit -m \u0026#34;whatever\u0026#34; # This will show you the last two commits, so you can leave the first one and # on the beginning of the second one, # replace \u0026#39;pick\u0026#39; for \u0026#39;f\u0026#39; so the changes are merged with first one without # keeping the commit message git rebase -i HEAD~2 # Fix the commit message if needed (like fixing formatting, # set dependant commits, or bugs it closes, etc) git commit --amend # Submit changes again for review git-review Also, keep in mind that apart from submitting the code change is important to submit automated validation tests, which can be executed with tox -e py27 to view that the functions return the values we expect even if the input data is out of what it should be, or like coverage, to validate that the code is covered (check on tox.ini what is defined).\nAnd last but not least, expect to have lot of comments on more serious changes like changes to stable libs, as lot of reviewers will come to ensure that everything looks good and might even discuss it on the regular meetings to ensure, that a change is a good fit for the product in the proposed approach. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2016/07/21/contributing-to-openstack/","summary":"\u003cp\u003eContributing to an OpenSource project might take some time at the beginning, the good thing with OpenStack is that there are lot of guides on how to start and collaborate.\u003c/p\u003e\n\u003cp\u003eWhat I did is to look for a bug in the project tagged as \u003ca href=\"https://bugs.launchpad.net/openstack/\u0026#43;bugs?field.tag=low-hanging-fruit\u0026amp;orderby=status\u0026amp;start=0\"\u003elow-hanging-fruit\u003c/a\u003e, this allows to browse a large list of bugs that are classified as \u003ccode\u003eeasy\u003c/code\u003e, so they are the best place for new starters to get familiar with the workflow.\u003c/p\u003e","title":"Contributing to OpenStack"},{"content":"As always, I don\u0026rsquo;t usually find myself keen to write about things I do, until I later realize they might be helpful for others, and that\u0026rsquo;s why in the past I decided to switch the place I was putting the information about why did to Github and also, take benefit of practicing markdown for writing the entries.\nAt that time, I moved my old blog posts to markdown to be used in conjunction with Jekyll and to use OctoPress as the engine rendering the contents into a static website. The setup and migration was not difficult, but still require to use some ruby, while I was more familiar with Python.\nSince some time ago, I was checking other platforms, following the same approach of rendering markdown files and sticker to Pelican, it\u0026rsquo;s included in Fedora repositories (python-pelican). Pelican offers a similar behavior, having also a server for allowing you to quickly test the new settings (plugins, themes, etc) and to publish the resulting website to a hosting provider.\nAs I did with Jekyll+OctoPress, I\u0026rsquo;m still using github.io for it, and I\u0026rsquo;m in the process of adapting some changes like additional plugins, theme tweaks and consider to develop one of my own. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2016/06/03/new-blog-rendering-engine-pelican/","summary":"\u003cp\u003eAs always, I don\u0026rsquo;t usually find myself keen to write about things I do, until I later realize they might be helpful for others, and that\u0026rsquo;s why in the past I decided to switch the place I was putting the information about why did to Github and also, take benefit of practicing markdown for writing the entries.\u003c/p\u003e\n\u003cp\u003eAt that time, I moved my old blog posts to markdown to be used in conjunction with Jekyll and to use OctoPress as the engine rendering the contents into a static website. The setup and migration was not difficult, but still require to use some ruby, while I was more familiar with Python.\u003c/p\u003e","title":"New blog rendering engine: Pelican"},{"content":"Who am I? My name is Pablo and I\u0026rsquo;ve been working with Open Source since before I started university.\nPart of my evolution with Open Source is described in this article Linux.\nSince 2006 I work for Red Hat, where I started as a Dedicated Enterprise Engineer (Onsite Consultant) for a country-wide retail company with headquarters in Valencia.\nAfter two more years of visiting the biggest customers in the country, I moved to the Technical Account Manager role, where I was in charge of handling the relationship with some assigned Strategic Customers.\nShortly after my start as TAM, I started working with Cloud customers, providing support to their OpenStack deployment as Senior Cloud Technical Account Manager.\nSince September 2016, I moved to a new role within the Customer Experience and Engagement organization, starting as Senior Software Maintenance Engineer - OpenStack (SEG: Support Engineering Group) where I do collaborate with my colleagues handling the OpenStack support cases created in EMEA region and the global ones on Follow-The-Sun.\nSince April 1st 2018, I got promoted to Principal Software Maintenance Engineer - OpenStack, where I did still worked with the same \u0026lsquo;stuff\u0026rsquo;.\nSince September 1st 2018, I moved to the Solutions Engineering team, working on Kubernetes/OpenShift Federation and Quay registry among other topics, Community Ecosystem for KubeVirt and Metal³ and later, including Telco 5G deployments on OpenShift Installer-Provisioned Infrastructure on bare-metal and later Management Integration using Advanced Cluster Management (ACM) on OpenShift to explore the requirements of Telco Edge architectures.\nIn April 1st 2022, I got promoted to Principal Software Engineer while working the Management Integration Team.\nI\u0026rsquo;m also contributing on regular basis to Citellus/Risu project to help automating diagnosis on known issues at system configuration, package sets, etc.\nYou can check my updated profile at LinkedIn and my current RH Certifications Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/about/","summary":"\u003ch3 id=\"who-am-i\"\u003eWho am I?\u003c/h3\u003e\n\u003cp\u003eMy name is Pablo and I\u0026rsquo;ve been working with Open Source since before I started university.\u003c/p\u003e\n\u003cp\u003ePart of my evolution with Open Source is described in this article \u003ca href=\"/es/blog/2008/06/03/mi-evoluci%C3%B3n-con-linux/\"\u003eLinux\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSince 2006 I work for Red Hat, where I started as a Dedicated Enterprise Engineer (Onsite Consultant) for a country-wide retail company with headquarters in Valencia.\u003c/p\u003e\n\u003cp\u003eAfter two more years of visiting the biggest customers in the country, I moved to the Technical Account Manager role, where I was in charge of handling the relationship with some assigned Strategic Customers.\u003c/p\u003e","title":"About"},{"content":"Since some time ago, email filter management was not scaling for me as I was using server-side filtering, I had to deal with the web-based interface which was missing some elements like drag\u0026amp;drop reordering of rules, cloning, etc.\nAs I was already using offlineimap to sync from the remote mail server to my system into a maildir folder, I had almost all the elements I needed.\nAfter searching for several options imapfilter seemed to be a perfect fit, so I started with a small set of rules and start integration with my email process.\nOn my first attempts, I setup a pre-sync hook on offlineimap by using as well as the postsync hook I already had:\npresynchook = time imapfilter postsynchook = ~/.mutt/postsync-offlineimap.sh Initial attempts were not good at all, applying filters on the remote IMAP server was very time consuming and my actual 1 minute delay after finishing one check was becoming a real 10-15 minute interval between checks because of the IMAP filtering and this was not scaling as I was putting new rules.\nAfter some tries, and as I already had all the email synced offline, moved filtering to be locally instead of server-side, but as imapfilter requires an IMAP server, I tricked dovecot into using the local folder to be offered via IMAP:\nprotocols = imap mail_location = maildir:~/.maildir/FOLDER/:INBOX=~/.maildir/FOLDER/.INBOX/ auth_debug_passwords=yes This also required to change my foldernames to use \u0026ldquo;.\u0026rdquo; in front of them, so I needed to change mutt configuration too for this:\nset mask=\u0026#34;.*\u0026#34; and my mailfolders script:\nset mbox_type=Maildir set folder=\u0026#34;~/.maildir/FOLDER\u0026#34; set spoolfile=\u0026#34;~/.maildir/FOLDER/.INBOX\u0026#34; #mailboxes `echo -n \u0026#34;+ \u0026#34;; find ~/.cache/notmuch/mutt/results ~/.maildir/FOLDER -type d -not -name \u0026#39;cur\u0026#39; -not -name \u0026#39;new\u0026#39; -not -name \u0026#39;tmp\u0026#39; -not -name \u0026#39;.notmuch\u0026#39; -not -name \u0026#39;xapian\u0026#39; -not -name \u0026#39;FOLDER\u0026#39; -printf \u0026#34;+\u0026#39;%f\u0026#39; \u0026#34;` mailboxes `find ~/.maildir/FOLDER -type d -name cur -printf \u0026#39;%h \u0026#39;|tr \u0026#34; \u0026#34; \u0026#34;\\n\u0026#34;|grep -v \u0026#34;^/home/iranzo/.maildir/FOLDER$\u0026#34;|sort|xargs echo` #Store reply on current folder folder-hook . \u0026#39;set record=\u0026#34;^\u0026#34;\u0026#39; After this, I could start using imapfilter and start working on my set of rules\u0026hellip; but first problem appeared, apparently I started having some duplicated email as I was cancelling and rerunning the script while debugging so a new tool was also introduced to dedup my IMAP folder named IMAPdedup with a small script:\n#!/bin/bash ( for folder in $(python ~/.bin/imapdedup.py -s localhost -u iranzo -w \u0026#39;$PASSWORD\u0026#39; -m -c -v -l) do python ~/.bin/imapdedup.py -s localhost -u iranzo -w \u0026#39;$PASSWORD\u0026#39; -m -c \u0026#34;$folder\u0026#34; done ) 2\u0026gt;\u0026amp;1|grep \u0026#34;will be marked as deleted\u0026#34; This script was taking care of listing all email folders on \u0026rsquo;localhost\u0026rsquo; with my username and password (can be scripted or use external tools to gather it) and dedup email after each sync (in my postsync-offlinemap.sh as well as lbdq script for fetching new addresses, notmuch and running imapfilter after syncing (to catch the limited filtering I do sever-side)\nI still do some server-side filtering (4 rules), to get on a \u0026ldquo;Pending sort\u0026rdquo; folder all email which is either:\nNew support cases remain at INBOX All emails from case updates, bugzilla, etc to _pending All emails containing \u0026rsquo;list\u0026rsquo; or \u0026lsquo;bounces\u0026rsquo; in from to _pending All emails not containing me directly on CC or To, to _pending This more or less ensures a clean INBOX with most important things still there, and easier rule handling for email sorting.\nSo, after some tests, this is at the moment a simplified version of my filtering file:\n--------------- -- Options -- --------------- options.timeout = 30 options.subscribe = true options.create = false function offlineimap (key) local status local value status, value = pipe_from(\u0026#39;grep -A2 ACCOUNT ~/.offlineimaprc | grep -v ^#|grep \u0026#39;.. key ..\u0026#39;|cut -d= -f2\u0026#39;)C value = string.gsub(value, \u0026#39; \u0026#39;, \u0026#39;\u0026#39;) value = string.gsub(value, \u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) return value end ---------------- -- Accounts -- ---------------- -- Connects to \u0026#34;imap1.mail.server\u0026#34;, as user \u0026#34;user1\u0026#34; with \u0026#34;secret1\u0026#34; as -- password. EXAMPLE = IMAP { server = \u0026#39;localhost\u0026#39;, username = \u0026#39;iranzo\u0026#39;, password = \u0026#39;$PASSWORD\u0026#39;, port = 143 } -- My email myuser = \u0026#39;ranzo\u0026#39; function mine(messages) email=messages:contain_cc(myuser)+messages:contain_to(myuser)+messages:contain_from(myuser) return email end function filter(messages,email,destination) messages:contain_from(email):move_messages(destination) messages:contain_to(email):move_messages(destination) messages:contain_cc(email):move_messages(destination) messages:contain_field(\u0026#39;sender\u0026#39;, email):move_messages(destination) end function deleteold(messages,days) todelete=messages:is_older(days)-mine(messages) todelete:move_messages(EXAMPLE[\u0026#39;Trash\u0026#39;]) end -- Define the msgs we\u0026#39;re going to work on -- Move sent messages to INBOX to later sorting sent = EXAMPLE.Sent:select_all() sent:move_messages(EXAMPLE[\u0026#39;INBOX\u0026#39;]) inbox = EXAMPLE[\u0026#39;INBOX\u0026#39;]:select_all() pending = EXAMPLE[\u0026#39;INBOX/_pending\u0026#39;]:select_all() todos = pending + inbox -- Mark as read messages sent from my user todos:contain_from(myuser):is_recent():mark_seen() -- Delete google calendar forwards todos:contain_to(\u0026#39;piranzo@gapps.example.com\u0026#39;):delete_messages() -- Move all spam messages to Junk folder spam = todos:contain_field(\u0026#39;X-Spam-Score\u0026#39;,\u0026#39;*****\u0026#39;) spam:move_messages(EXAMPLE[\u0026#39;Junk\u0026#39;]) -- Move Jive notifications filter(todos,\u0026#39;jive-notify@example.com\u0026#39;,EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/_jive\u0026#39;]) -- Filter EXAMPLEN filter(todos,\u0026#39;dev-null@rhn.example.com\u0026#39;,EXAMPLE[\u0026#39;Trash\u0026#39;]) -- Filter PNT filter(todos:contain_subject(\u0026#39;[PNT] \u0026#39;),\u0026#39;noreply@example.com\u0026#39;,EXAMPLE[\u0026#39;Trash\u0026#39;]) -- Filter CPG (Customer Private Group) filter(todos:contain_subject(\u0026#39;Red Hat - Group \u0026#39;),\u0026#39;noreply@example.com\u0026#39;,EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Other/CPG\u0026#39;]) -- Remove month start reminders todos:contain_subject(\u0026#39;mailing list memberships reminder\u0026#39;):delete_messages() -- Delete messages about New accounts created (RHN) usercreated=todos:contain_subject(\u0026#39;New Red Hat user account created\u0026#39;)*todos:contain_from(\u0026#39;noreply@example.com\u0026#39;) usercreated:delete_messages() -- Search messages from CPG\u0026#39;s cpg = EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Other/CPG\u0026#39;]:select_all() cpg:contain_subject(\u0026#39;Cust1\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust1/CPG\u0026#39;]) cpg:contain_subject(\u0026#39;Cust2\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust2/CPG\u0026#39;]) cpg:contain_subject(\u0026#39;Cust3\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust3/CPG\u0026#39;]) cpg:contain_subject(\u0026#39;Cust4\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust4/CPG\u0026#39;]) -- Move bugzilla messages filter(todos:contain_subject(\u0026#39;] New:\u0026#39;),\u0026#39;bugzilla@example.com\u0026#39;,EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/_bugzilla/new\u0026#39;]) filter(todos,\u0026#39;bugzilla@example.com\u0026#39;,EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/_bugzilla\u0026#39;]) -- Move all support messages to Other for later processing filter(todos:contain_subject(\u0026#39;(NEW) (\u0026#39;),\u0026#39;support@example.com\u0026#39;,EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/_new\u0026#39;]) filter(todos:contain_subject(\u0026#39;Case \u0026#39;),\u0026#39;support@example.com\u0026#39;,EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Other/cases\u0026#39;]) EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/_new\u0026#39;]:is_seen():move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Other/cases\u0026#39;]) support = EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Other/cases\u0026#39;]:select_all() -- Restart the search only for messages in Other to also process if we have new rules support:contain_subject(\u0026#39;is about to breach its SLA\u0026#39;):delete_messages() support:contain_subject(\u0026#39;has breached its SLA\u0026#39;):delete_messages() support:contain_subject(\u0026#39; has had no activity in \u0026#39;):delete_messages() -- Here the process is customer after customer and mark as read messages from non-prio customers support:contain_body(\u0026#39;Cust1\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust1/cases\u0026#39;]) support:contain_body(\u0026#39;Cust2\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust2/cases\u0026#39;]) support:contain_body(\u0026#39;Cust3\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust3/cases\u0026#39;]) support:contain_body(\u0026#39;Cust4\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust4/cases\u0026#39;]) -- For customer swith common matching names, use header field support:contain_field(\u0026#39;X-SFDC-X-Account-Number\u0026#39;, \u0026#39;XXXX\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust5/cases\u0026#39;]) support:contain_body(\u0026#39;Customer : COMMONNAME\u0026#39;):move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust6/cases\u0026#39;]) -- Non prio customers (mark updates as read) cust7 = support:contain_body(\u0026#39;WATCHINGCUST\u0026#39;) + support:contain_body(\u0026#39;Cust7\u0026#39;) cust7:mark_seen() cust7:move_messages(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust7/cases\u0026#39;]) -- Filter other messages by domain filter(todos,\u0026#39;todos.es\u0026#39;, EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust8\u0026#39;]) -- Process all remaining messages in INBOX + all read messages in pending-sort for mailing lists and move to lists folder filter(todos,\u0026#39;list\u0026#39;, EXAMPLE[\u0026#39;INBOX/Lists\u0026#39;]) filter(todos,\u0026#39;bounces\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists\u0026#39;]) -- Add EXAMPLE lists, inbox and _pending and Fedora default bin for reprocessing in case a new list has been added lists = todos + EXAMPLE[\u0026#39;INBOX/Lists\u0026#39;]:select_all() + EXAMPLE[\u0026#39;INBOX/Lists/Fedora\u0026#39;]:select_all() -- Mailing lists -- EXAMPLE filter(lists,\u0026#39;outages-list\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/EXAMPLE/general/outage\u0026#39;]) filter(lists,\u0026#39;announce-list\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/EXAMPLE/general/announce\u0026#39;]) -- Fedora filter(lists,\u0026#39;kickstart-list\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/Fedora/kickstart\u0026#39;]) filter(lists,\u0026#39;ambassadors@lists.fedoraproject.org\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/Fedora/Ambassador\u0026#39;]) filter(lists,\u0026#39;infrastructure@lists.fedoraproject.org\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/Fedora/infra\u0026#39;]) filter(lists,\u0026#39;announce@lists.fedoraproject.org\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/Fedora/announce\u0026#39;]) filter(lists,\u0026#39;lists.fedoraproject.org\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/Fedora\u0026#39;]) -- OSP filter(lists,\u0026#39;openstack@lists.openstack.org\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/OpenStack\u0026#39;]) filter(lists,\u0026#39;openstack-es@lists.openstack.org\u0026#39;,EXAMPLE[\u0026#39;INBOX/Lists/OpenStack/es\u0026#39;]) -- Filter my messages not filtered back to INBOX mios=pending:contain_from(myuser) mios:move_messages(EXAMPLE[\u0026#39;INBOX\u0026#39;]) -- move messages we\u0026#39;re in BCC to INBOX for manual sorting hidden = pending - mine(pending) hidden:move_messages(EXAMPLE[\u0026#39;INBOX\u0026#39;]) -- Start processing of messages older than: maxage=60 -- Delete old messages from mailing lists deleteold(EXAMPLE[\u0026#39;INBOX/Lists/EXAMPLE/general/media\u0026#39;],maxage) deleteold(EXAMPLE[\u0026#39;INBOX/Lists/EXAMPLE/general/outage\u0026#39;],maxage) -- delete old cases maxage=180 -- for each in $(cat .imapfilter/config.lua|grep -i cases|tr \u0026#34; ,()\u0026#34; \u0026#34;\\n\u0026#34;|grep cases|sort|uniq|grep -v \u0026#34;:\u0026#34; );do echo \u0026#34;deleteold($each,maxage)\u0026#34;;done deleteold(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust1/cases\u0026#39;],maxage) deleteold(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust2/cases\u0026#39;],maxage) deleteold(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Cust3/cases\u0026#39;],maxage) deleteold(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/Other/cases\u0026#39;],maxage) deleteold(EXAMPLE[\u0026#39;INBOX/EXAMPLE/Customers/_bugzilla\u0026#39;],maxage) -- Empty trash every 7 days maxage=7 deleteold(EXAMPLE[\u0026#39;Trash\u0026#39;],maxage) As this is applied filtering twice, offlineimap might be uploading part of your changes already, making it faster to next syncs, and shuffle some of your emails while it runs.\nThe point of adding the already filtered set to be filtered again (CPG, cases, etc) is that if a new customer is considered to be filter on a folder of its own, the messages will be picked up and moved accordingly automatically ;-)\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2015/08/28/filtering-email-with-imapfilter/","summary":"\u003cp\u003eSince some time ago, email filter management was not scaling for me as I was using server-side filtering, I had to deal with the web-based interface which was missing some elements like drag\u0026amp;drop reordering of rules, cloning, etc.\u003c/p\u003e\n\u003cp\u003eAs I was already using offlineimap to sync from the remote mail server to my system into a maildir folder, I had almost all the elements I needed.\u003c/p\u003e\n\u003cp\u003eAfter searching for several options \u003ca href=\"https://github.com/lefcha/imapfilter\"\u003eimapfilter\u003c/a\u003e seemed to be a perfect fit, so I started with a small set of rules and start integration with my email process.\u003c/p\u003e","title":"Filtering email with imapfilter"},{"content":"Since some time ago, I\u0026rsquo;ve been mostly dealing with OpenStack, requiring different releases to test for different tests, etc.\nVirtualization, as provided by KVM requires some CPU flags to get accelerated operations, vmx and svm depending on your processor architecture, but, of course, this is only provided on bare-metal.\nIn order to get more flexibility at the expense of performance, nestedvt allows to expose those flags to the VM\u0026rsquo;s running at the hypervisor so you can run another level of VM\u0026rsquo;s inside those VM\u0026rsquo;s (this starts to sound like the movie Inception).\nThe problem, so far is that this required changes on the kernel and drivers to make it work, and was lacking lot of stability, so this is something NOT SUPPORTED FOR PRODUCTION USE but which makes perfect sense for demo environments, labs, etc, allowing you to maximize the use of your hardware for better flexibility but at the cost of performance.\nAs I was using RHEV for managing my home-lab I hit the first issue, my hypervisors (HP Proliant G7 N54L) where using RHEL-6 as operating system, and the support for nested was not very good, but luckily, RHEV-M 3.5 includes support for hypervisors running on RHEL-7, enabling to use latest features included in kernel, networking stack, etc.\nFirst step, was to redeploy the servers, wasn\u0026rsquo;t that hard, but required some extra steps as I had another unsupported approach (servers were sharing local storage over NFS for providing Storage Domains to environment, HIGHLY UNSUPPORTED), so I moved them from NFS to iSCSI provided by an external server and with the help of the kickstart I use for other systems, I started the process.\nOnce the two servers were migrated, the last one, finished moving VM\u0026rsquo;s from NFS to iSCSI and needed to be put on maintenance and enable the other two (as a safety measure, RHEL-6 and RHEL-7 hosts cannot coexist on the same cluster in RHEV).\nFrom here, just needed to enable nestedvt on the environment.\nNestedVT \u0026lsquo;just\u0026rsquo; requires to expose the svm or vmx flag to the VM running directly from the bare-metal host, and we need to do that for every VM we start. On normal system with libvirt, we can just edit the XML for the VM definition and define the CPU like this:\n\u0026lt;cpu mode=\u0026#39;custom\u0026#39; match=\u0026#39;exact\u0026#39;\u0026gt; \u0026lt;model fallback=\u0026#39;allow\u0026#39;\u0026gt;Opteron_G3\u0026lt;/model\u0026gt; \u0026lt;feature policy=\u0026#39;require\u0026#39; name=\u0026#39;svm\u0026#39;/\u0026gt; \u0026lt;/cpu\u0026gt; For RHEV, however, we don\u0026rsquo;t have an XML we can edit, as it is created dynamically with the contents of the database for the VM (disks, NICS, name, etc), but we\u0026rsquo;ve the VDSM-Hooks mechanism for doing this.\nHooks in vdsm are a powerful and dangerous tool, as they can modify in-flight the XML used to create the VM, and allow lot of features to be implemented.\nIn the past, for example, those hooks could be used to provide DirectLUN support to RHEV, or fixed BIOS Serial Number for VM\u0026rsquo;s where the product was still lacking the official feature, and in this case, we\u0026rsquo;ll use them to provide the CPU flags we need.\nAs you can imagine, this is something that has lot of interested people behind, and we can find upstream a repository with VDSM-Hooks.\nIn this case, the one that we\u0026rsquo;re needing is \u0026rsquo;nestedvt\u0026rsquo;, so we can proceed to install it on our hosts like:\nwget http://mirrors.ibiblio.org/ovirt/pub/ovirt-3.4/rpm/el7/noarch/vdsm-hook-nestedvt-4.14.17-0.el7.noarch.rpm rpm -Uvh vdsm-hook-nestedvt-4.14.17-0.el7.noarch.rpm You\u0026rsquo;ll need to put a host in maintenance and activate for VDSM to refresh the hooks installed and start new VM so we have the hook injecting the XML.\nAfter it boots, egrep 'svm|vmx' /proc/pcuinfo should show the flags there.\nBut wait\u0026hellip;\nRHEV also includes a security feature that makes it impossible for a VM to spy on the communications meant to other VM\u0026rsquo;s that makes it impossible to simulate other MAC\u0026rsquo;s within it, and this is performed via libvirt filters on the interfaces.\nTo come to our rescue, another hook comes to play in, this time macspoof which allows to disable this security measure for a VM so it can execute virtualization within.\nFirst, let\u0026rsquo;s repeat the procedure and install the hook on all of our hypervisors:\nwget http://mirrors.ibiblio.org/ovirt/pub/ovirt-3.4/rpm/el7/noarch/vdsm-hook-macspoof-4.14.17-0.el7.noarch.rpm rpm -Uvh vdsm-hook-macspoof-4.14.17-0.el7.noarch.rpm This will enable the hook in the system, but we also need to make the RHEV-M Engine aware of it, so we need to define a new Custom Property for VM\u0026rsquo;s:\nengine-config -s \u0026#34;UserDefinedVMProperties=macspoof=(true|false)\u0026#34; This will ask us for the compatibility version (we\u0026rsquo;ll choose 3.5) and enable a new true/false property for VM\u0026rsquo;s that require this security measure lifted. We\u0026rsquo;re doing of course this approach instead of disabling it for everyone to limit it\u0026rsquo;s use to just the VM\u0026rsquo;s needing it, not losing all the benefits on security provided.\nAs a side note, macspoof plugin is available in official repositories for RHEL7 hypervisor, so you can use this instead of oVirt\u0026rsquo;s repository one.\nNow when we create a new VM, for example to use with OpenStack, we can go to custom properties for this vm, select macspoof and set a value of \u0026rsquo;true\u0026rsquo; and once the VM is started will be able to see the processor extensions for virtualization and at the same time, the VM\u0026rsquo;s created within, will be able to communicate with the outside world.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2015/07/17/rhev-m-with-nested-vm-for-osp/","summary":"\u003cp\u003eSince some time ago, I\u0026rsquo;ve been mostly dealing with OpenStack, requiring different releases to test for different tests, etc.\u003c/p\u003e\n\u003cp\u003eVirtualization, as provided by KVM requires some CPU flags to get accelerated operations, \u003ccode\u003evmx\u003c/code\u003e and \u003ccode\u003esvm\u003c/code\u003e depending on your processor architecture, but, of course, this is only provided on bare-metal.\u003c/p\u003e\n\u003cp\u003eIn order to get more flexibility at the expense of performance, \u003ccode\u003enestedvt\u003c/code\u003e allows to expose those flags to the VM\u0026rsquo;s running at the hypervisor so you can run another level of VM\u0026rsquo;s inside those VM\u0026rsquo;s (this starts to sound like the movie \u003ca href=\"http://www.imdb.com/title/tt1375666/\"\u003eInception\u003c/a\u003e).\u003c/p\u003e","title":"RHEV-M with nested VM for OSP"},{"content":"Hi,\nTelegram.org recently announced the support for writing bots for their platform, by providing details at https://core.telegram.org/bots.\nI was missing for a long time the ability to get a count on karma like we\u0026rsquo;ve on IRC servers, so I started with it.\nMy first try is published at github repo in https://github.com/iranzo/stampython.\nAt the moment it just uses the polling interface to check the new messages received on the channels the bot is in, and later processes them and send the relevant replies via messages.\nAlso, some other commands are missing like the ones on redken that we use on IRC, but at least, basic functionality is there and is usable.\nEnjoy! (and if you do, you can Buy Me a Coffee ) Pablo\nBTW: the bot is not allowed to join channels (@stampy_bot) so it remains in a controlled environment until the code is made more robust, but I\u0026rsquo;m thinking about having a second public instance on Openshift.redhat.com for wider audience. You can invite the public instance by inviting @redken_bot\n","permalink":"https://iranzo.io/blog/2015/06/26/writing-a-telegram.org-bot-in-python/","summary":"\u003cp\u003eHi,\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://telegram.org\"\u003eTelegram.org\u003c/a\u003e recently announced the support for writing bots for their platform, by providing details at \u003ca href=\"https://core.telegram.org/bots\"\u003ehttps://core.telegram.org/bots\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI was missing for a long time the ability to get a count on karma like we\u0026rsquo;ve on\nIRC servers, so I started with it.\u003c/p\u003e\n\u003cp\u003eMy first try is published at github repo in \u003ca href=\"https://github.com/iranzo/stampython\"\u003ehttps://github.com/iranzo/stampython\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAt the moment it just uses the polling interface to check the new messages received on the channels the bot is in, and later processes them and send the relevant replies via messages.\u003c/p\u003e","title":"Writing a Telegram.org bot in Python"},{"content":"Hi,\nSome time ago, and after discussing with a colleague, I had a look on Intel\u0026rsquo;s AMT, and this week I did a demo for another colleague as a cheap-replacement for having power fencing capabilities on commodity hardware.\nAMT provides a server-like Out of band management like iLO, iDRAC, RSB etc and it\u0026rsquo;s included in i3 with vPro processors/chipsets of some equipment.\nI did the test on a Lenovo X200/201 system I had as old laptop.\nThe steps used for configuring it, require to:\nfirst enable the support in the BIOS, usually named \u0026lsquo;Intel AMT\u0026rsquo; or \u0026lsquo;Intel Active Management Technology\u0026rsquo;. After this step it was possible to use the command to enter the special AMT firmware Intel(R) Management Engine which on this laptop is enabled with CTRL-P. If this is the first time you enable it, you\u0026rsquo;ll require to change the default admin password to something secure, usually mixed upper-lower case, symbol and numbers. For this example we\u0026rsquo;ll be using Qwer123$ as password. Explore the settings, enable it and validate network settings. I\u0026rsquo;ve enabled DHCP on both LAN and Wireless for IPv4 and IPv6, and enabled KVM redirection Once finished, save changes and exit from firmware screen and let the system boot. From another host, you can perform the remaining configuration steps, from now on, the \u0026rsquo;target\u0026rsquo; system will be intercepting packets sent to specific port via the network cards and redirect to AMT firmware instead of going to target host. This is something important to note, the packets are only intercepting when coming from OUTSIDE the host so we\u0026rsquo;ll use a second computer to access it.\nYou can use a browser pointing to target system\u0026rsquo;s IP at port 16992, for example: http://target:16992\nFrom that web interface and once logging with admin and the password set Qwer123$ we can continue doing some configuration, like the power states to control (for example, this laptop could be remotely powered when it was with the charger connected even if laptop was powered off).\nNow, for doing the \u0026lsquo;command-line\u0026rsquo; part, we will need to install one package on our system and rum some scripts.\n# First we\u0026#39;ll install amtterm wsmancli dnf -y install amtterm wsmancli # This will provide the two commands we\u0026#39;ll later use, wsman for configuration and amttool for power control # We need to define the host to use and password as well as the password we\u0026#39;ll use for console redirection (via VNC) AMT_PASSWORD=\u0026#39;Qwer123$\u0026#39; AMT_HOST=target VNC_PASSWORD=\u0026#39;Qwer123$\u0026#39; # we can define those vars (specially AMT_PASSWORD) in our .profile or .bash_profile in order to avoid typing them everytime # set the vnc password (must be 8 characters MAX) wsman put http://intel.com/wbem/wscim/1/ips-schema/1/IPS_KVMRedirectionSettingData -h ${AMT_HOST} -P 16992 -u admin -p ${AMT_PASSWORD} -k RFBPassword=${VNC_PASSWORD} # enable KVM redirection to port 5900 (this will also intercept 5900 port for console redirection, so make it sure you\u0026#39;ll not need it later) wsman put http://intel.com/wbem/wscim/1/ips-schema/1/IPS_KVMRedirectionSettingData -h ${AMT_HOST} -P 16992 -u admin -p ${AMT_PASSWORD} -k Is5900PortEnabled=true # disable opt-in policy (do not ask user for console access) wsman put http://intel.com/wbem/wscim/1/ips-schema/1/IPS_KVMRedirectionSettingData -h ${AMT_HOST} -P 16992 -u admin -p ${AMT_PASSWORD} -k OptInPolicy=false # disable session timeout (do not timeout sessions) wsman put http://intel.com/wbem/wscim/1/ips-schema/1/IPS_KVMRedirectionSettingData -h ${AMT_HOST} -P 16992 -u admin -p ${AMT_PASSWORD} -k SessionTimeout=0 # enable KVM (enable keyboard/video/monitor redirection) wsman invoke -a RequestStateChange http://schemas.dmtf.org/wbem/wscim/1/cim-schema/2/CIM_KVMRedirectionSAP -h ${AMT_HOST} -P 16992 -u admin -p ${AMT_PASSWORD} -k RequestedState=2 # OPTIONAL: view settings (validate all the settings) wsman get http://intel.com/wbem/wscim/1/ips-schema/1/IPS_KVMRedirectionSettingData -h ${AMT_HOST} -P 16992 -u admin -p ${AMT_PASSWORD} After this step, we should be able to use vinagre target to access the KVM redirection and remotely control our system.\nFor example, to control power of host you can use:\n# Check host status: amttool $AMT_HOST info # Power up a powered-off host: amttool $AMT_HOST powerup # Power down a powered-on host: amttool $AMT_HOST powerdown Check man amttool for other commands like reset, powercycle.\nIMPORTANT: note that some power state changes can only be performed based on previous status, you can check with info the available ones and current status of system.\nAs a bonus, there\u0026rsquo;s a RFE1 for requesting this tool to be incorporated as power fencing mechanism in fence-agents once \u0026lsquo;amtterm\u0026rsquo; is included in RHEL, in the meantime it\u0026rsquo;s already available in Fedora, and when it comes to RHEL, hopefully could also be used as fence agent for Clusters and RHEV.\nEnjoy! (and if you do, you can Buy Me a Coffee ) Request for Enhancement: a bugzilla request oriented not to fix a bug, but to incorporate new functionality/software into a product.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iranzo.io/blog/2015/05/01/intel-amt-on-linux-for-remote-control/fencing/","summary":"\u003cp\u003eHi,\u003c/p\u003e\n\u003cp\u003eSome time ago, and after discussing with a colleague, I had a look on Intel\u0026rsquo;s \u003ca href=\"http://en.wikipedia.org/wiki/Intel_Active_Management_Technology\"\u003eAMT\u003c/a\u003e, and this week I\ndid a demo for another colleague as a cheap-replacement for having power fencing capabilities on commodity hardware.\u003c/p\u003e\n\u003cp\u003eAMT provides a server-like Out of band management like iLO, iDRAC, RSB etc and it\u0026rsquo;s included in i3 with vPro processors/chipsets of some equipment.\u003c/p\u003e\n\u003cp\u003eI did the test on a Lenovo X200/201 system I had as old laptop.\u003c/p\u003e","title":"Intel AMT on Linux for remote control/fencing"},{"content":"I had my old blog based on SPIP, and I wanted to keep all the posts together, to make it easier to migrate in the future.\nInitially, I migrated my posts from blogger, where there\u0026rsquo;s an option to export the contents and some plugins to allow easier importing to markdown files (to be used by OctoPress), those were the recent posts, so part of the job was already done there.\nNext step, was to migrate old posts on my SPIP site.\nSPIP, being not as popular as other solutions, might lack plugins for importing the data, but has a nice feature: it allows to provide full article contents via RSS.\nSo:\nI entered into my site private area /ecrire/ Entered to the administration section and under Content, I temporarily changed the syndication settings to provide full articles instead of just summary. Then, I visited the url for my user, but on the RSS generator template: spip.php?page=backend\u0026amp;id_rubrique=6, and saved it as file.xml At this point I needed some software for automating the initial conversion, so I went to python\u0026rsquo;s feedparser libraries to perform this with a bit of coding:\nurl = \u0026#34;/path/to/your/xml/file.xml\u0026#34; import codecs import feedparser feed = feedparser.parse(url) for item in feed[\u0026#34;items\u0026#34;]: filename = ( item[\u0026#34;date\u0026#34;][0:10] + \u0026#34;-\u0026#34; + item[\u0026#34;link\u0026#34;][23:] ) # remove the first 23 chars from article url http+domain print(filename) with codecs.open(filename, \u0026#34;w\u0026#34;, \u0026#34;utf-8\u0026#34;) as f: f.write(\u0026#34;---\\n\u0026#34;) f.write(\u0026#34;layout: post\\n\u0026#34;) for elem in [\u0026#34;title\u0026#34;, \u0026#34;date\u0026#34;]: f.write(\u0026#34;%s: %s\\n\u0026#34; % (elem, item[elem])) f.write(\u0026#34;---\\n\u0026#34;) f.write(item[\u0026#34;content\u0026#34;][0].value) After each iteration, a new file was created using the old http link to the article (which already had stripped problematic characters).\nJust moving those files to source/_posts allows me to re-publish them on a different site, and later work the conversion to markdown by using pandoc and some manual tuning. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2015/04/01/migrate-spip-rss-post-feed-to-html/","summary":"\u003cp\u003eI had my old blog based on \u003ca href=\"http://www.spip.net\"\u003eSPIP\u003c/a\u003e, and I wanted to keep all the posts together, to make it easier to migrate in the future.\u003c/p\u003e\n\u003cp\u003eInitially, I migrated my posts from blogger, where there\u0026rsquo;s an option to export the contents and some plugins to allow easier importing to markdown files (to be used by OctoPress), those were the recent posts, so part of the job was already done there.\u003c/p\u003e\n\u003cp\u003eNext step, was to migrate old posts on my SPIP site.\u003c/p\u003e","title":"Migrate SPIP-RSS post feed to HTML"},{"content":"Installing Linux on a RAID has lot of advantages, from using RAID1 to enjoy protection against drive failures or RAID0 to combine the size of several drives to create bigger space for files with all the smaller disks we have.\nThere are several RAID level definitions and may have different uses depending on our needs and hardware availability.\nFor this, I focused on using raid1 for the system disks (for greater redundancy/protection against failures) and raid0 (for combining several disks to make bigger space available for non important data)..\nWhy or why not use a RAID via software Pros There\u0026rsquo;s no proprietary data on the disks that could require this specific controller in case the hardware fails. Can be performed on any system, disk combination, etc Cons The use of dedicated hardware RAID cards allows to offload the CPU intensive tasks for raid calculation, etc to the dedicated processor, freeing internal CPU for system/user usage. Dedicated cards may have fancier features that require no support from the operating system as are all implemented by the card itself and presented to the OS as a standard drive. Performing the setup As I was installing on a HP Microserver G8 recently, I had to first disable the advanced mode for the included controller, so it behaved like a standard SATA one, once done, I was able to boot from my OS image (in this case EL7 ISO).\nOnce the ISO is booted in rescue mode, I could switch to the second console with ALT-F2 so I could start executing commands on the shell.\nFirst step is to setup partitioning, in this case I did two partitions, first one for holding /boot and the second one for setting up the LVM physical volume where the other Logical Volumes will be defined later.\nI\u0026rsquo;ve elected this setup over others because mdadm allows transparent support for booting (grub supports booting form it) and easy to manage setup.\nFor partitions, remember to allocate at least 500mb for /boot and as much as needed for your SO, for example, if only base OS is expected to have RAID protection, having a 20Gb partition will be enough, leaving the remaining disk to be used for a RAID0 device for allocating non-critical files.\nFor both partitions, set type with fdisk to fd: Linux RAID autodetect, and setup the two drives we\u0026rsquo;ll use for initial setup using the same values, for example:\nfdisk /dev/sda n # for new partition p # for primary \u0026lt;ENTER\u0026gt; # for first sector +500M # for size t # for type fd # for Linux RAID autodetect n # new partition p # primary \u0026lt;ENTER\u0026gt; +20G #for size t #for type 2 # for select 2nd partition fd # for Linux RAID autodetect # n for new partition p # for primary \u0026lt;ENTER\u0026gt; # for first sector \u0026lt;ENTER\u0026gt; # for remaining disk t # for type 3 # for third partition fd # for Linux RAID Autodetect w # for Writing changes And repeat that for /dev/sdb\nAt this point, we\u0026rsquo;ll have both sda and sdb with the same partitions defined: sd{a,b}1 with 500Mb for /boot and sd{a,b}2 with 20Gb for LVM and the remaining disk for RAID0 LVM.\nNow, it\u0026rsquo;s time to create the raid device on top, for simplicity, I tend to use md0 for /boot, so let\u0026rsquo;s start with it.\nCreating the raid devices with Multiple Devices mdadm Let\u0026rsquo;s create the raid devices for each system, starting with /boot:\nmdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1 mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sda2 /dev/sdb2 mdadm --create /dev/md2 --level=0 --raid-devices=2 /dev/sda3 /dev/sdb3 Now, check the status of the raid device creation by issuing:\ncat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md0 : active raid1 sda1[0] sdb1[1] 534760 blocks level 1, 64k chunk, algorithm 2 [2/2] [UU] [==\u0026gt;..................] recovery = 12.6% (37043392/292945152) finish=127.5min speed=33440K/sec md1 : active raid1 sda2[0] sdb2[1] 20534760 blocks level 1, 64k chunk, algorithm 2 [2/2] [UU] [=====\u0026gt;...............] recovery = 25.9% (37043392/692945152) finish=627.5min speed=13440K/sec ... When it finishes, all the devices will appear as synced, and we can start the installation of the operating system.\nWhat I did, after this point, is to reboot the install media, so I could use anaconda installer to select manually the filesystems, creating /boot on /dev/md0, then the Physical Volume on /dev/md1 for the operating system.\nSelect the manual partitioning during the installation to define above devices as their intended usage, and once it has been installed, create the additional Physical volume on /dev/md2 and define the intended mount points, etc.\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2015/03/28/install-rhel7/centos/fedora-on-a-software-raid-device/","summary":"\u003cp\u003eInstalling Linux on a RAID has lot of advantages, from using RAID1 to enjoy protection against drive failures or RAID0 to combine the size of several drives to create bigger space for files with all the smaller disks we have.\u003c/p\u003e\n\u003cp\u003eThere are several \u003ca href=\"http://en.wikipedia.org/wiki/RAID\"\u003eRAID\u003c/a\u003e level definitions and may have different uses depending on our needs and hardware availability.\u003c/p\u003e\n\u003cp\u003eFor this, I focused on using raid1 for the system disks (for greater redundancy/protection against failures) and raid0 (for combining several disks to make bigger space available for non important data)..\u003c/p\u003e","title":"Install RHEL7/Centos/Fedora on a software raid device"},{"content":"After testing for some days Jekyll and github.io for blog posting, I was missing some features of other CMS, so I started doing some search on how to automate many other topics while keeping simplicity on blog posting.\nOctoPress Makes this extra step so you can still focus on your contents and of course have a nice template as starting point with integrations for some social plugins, etc.\nSetup is well done if you follow the provided steps, without jumping anything, in my case, I moved my old pages (plain jekyll + poole) to OctoPress.\nOn Fedora and as my unprivileged user, I did:\nInstall RVM Define in profile configuration to have RVM use 1.9.3 ruby as required by OctoPress Install other required libraries for Setting up OctoPress Configure OctoPress Set OctoPress for github And from there, the generate, preview and deploy basics for blogging One of the interesting things it that it uses two branches on git, master and source, where master is the one that github publishes (your live environment) and source is the actual code for your blog, templates, posts, etc that are later generated, previewed and deployed from above steps.\nI\u0026rsquo;ll be testing it for a while to see how it works, but so far, so good. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2015/03/24/octopress-for-jekyll-blogging/","summary":"\u003cp\u003eAfter testing for some days Jekyll and github.io for blog posting, I was missing some features of other CMS, so I started doing some search on how to automate many other topics while keeping simplicity on blog posting.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://octopress.org\"\u003eOctoPress\u003c/a\u003e Makes this extra step so you can still focus on your contents and of course have a nice template as starting point with integrations for some social plugins, etc.\u003c/p\u003e\n\u003cp\u003eSetup is well done if you follow the provided steps, without jumping anything, in my case, I moved my old pages (plain \u003ccode\u003ejekyll + poole\u003c/code\u003e) to OctoPress.\u003c/p\u003e","title":"OctoPress for Jekyll blogging"},{"content":"Some podcasts are available via RSS feeds, so you can get notified of new episodes, so the best way I\u0026rsquo;ve found so far to automate this procedure is to use the utility flexget.\nFlexget can download a RSS feed and get the .torrent files associated to them and store locally, which makes a perfect fit for later using Transmission\u0026rsquo;s watch folder, to automatically add them to your download queue.\nIn order to do so, install flexget either via pip (pip install flexget) or using a package for your distribution and create a configuration file similar to this:\ncat ~/.flexget/config.yml tasks: download-rss: rss: http://URL/TO/YOUR/PODCAST/FEED all_series: yes only_new: yes download: /media/watch/ At each invocation of flexget execute it will access the RSS feed, search for new files and store the relevant .torrent files on the folder /media/watch from where transmission will pick up the new files and add them to your downloading queue for automatic download. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2015/03/16/podcasts-with-flexget-and-transmission/","summary":"\u003cp\u003eSome podcasts are available via RSS feeds, so you can get notified of new episodes, so the best way I\u0026rsquo;ve found so far to automate this procedure is to use the utility \u003ccode\u003eflexget\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFlexget can download a RSS feed and get the \u003ccode\u003e.torrent\u003c/code\u003e files associated to them and store locally, which makes a perfect fit for later using Transmission\u0026rsquo;s \u003ccode\u003ewatch\u003c/code\u003e folder, to automatically add them to your download queue.\u003c/p\u003e\n\u003cp\u003eIn order to do so, install \u003ccode\u003eflexget\u003c/code\u003e either via pip (\u003ccode\u003epip install flexget\u003c/code\u003e) or using a package for your distribution and create a configuration file similar to this:\u003c/p\u003e","title":"Podcasts with flexget and transmission"},{"content":"For my work I\u0026rsquo;ve been using markdown for a while, it allows to use some formatting on the documents created (mainly for knowledge base and solutions) without too much hassle for the formatting.\nOn the other side I was willing to improve the ability to make it easier to post new entries to blog without having to wait too much time, and of course, be able to prepare them offline and then push them live.\nUsing SPIP allowed me to focus on text without caring too much about formatting, the CMS is not hard to administer, maybe not as popular as WordPress, Joomla lately, but definitely, good for the work it\u0026rsquo;s intended to do, and not like traditional webpages.\nUsing Blogger, formatting was also not a hard part, and being a good platform improved the availability of tools, but still required online access, and the Firefox extensions for it, were still not convincing me for daily usage.\nYesterday, while I was reading some information about github, I got back to their \u0026lsquo;hosting\u0026rsquo; solution (github.io) and like the idea of pushing the code via git (I love the offline commit and later push of all changes), and started reading about Jekyll and Poole.\nJekyll and Poole enhance the creation of webpage posts using Markdown with some automation for automatic generation of links, reusability of common elements, etc which creates a set of static pages, fast to serve, upload, etc.\nI\u0026rsquo;ve also used Jekyll-Import to gather my old posts at blogger and I will still convert some of them to markdown and try to get back the auto-code coloring I had in place (using external JavaScript library).\nWell, let\u0026rsquo;s see how this goes and will try to implement some template to website not to use the standard one soon.\nPost Datum: This was being written while offline at the airport on a trip to Helsinki ;-) Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2015/03/16/jekyll-and-markdown/","summary":"\u003cp\u003eFor my work I\u0026rsquo;ve been using markdown for a while, it allows to use some formatting on the documents created (mainly for knowledge base and solutions) without too much hassle for the formatting.\u003c/p\u003e\n\u003cp\u003eOn the other side I was willing to improve the ability to make it easier to post new entries to blog without having to wait too much time, and of course, be able to prepare them offline and then push them live.\u003c/p\u003e","title":"Jekyll and Markdown"},{"content":"RHEV/oVirt API allows faster and simple development of scripts and utilities ranging from gathering of information to VM/host, etc manipulation.\nFor example, a simple script for connecting to API and list VM\u0026rsquo;s could be:\nimport sys import getopt import optparse import os import time from ovirtsdk.api import API from ovirtsdk.xml import params from random import choice baseurl = \u0026#34;https://localhost:8443\u0026#34; api = API(url=baseurl, username=\u0026#34;admin@internal\u0026#34;, password=\u0026#34;redhat\u0026#34;, insecure=True) for vm in api.vms.list(): print(vm.name) The .list() method works pretty well, but beware, it limits collections to 100 elements for performance reasons, so in those cases, we\u0026rsquo;ll need to check how many results do we have, and paginate by passing an extra argument to our \u0026ldquo;.list()\u0026rdquo; invocation, for example:\nfor vm in api.vms.list(query=\u0026#34;page 1\u0026#34;): print(vm) Furthermore, we can check the number of results by using:\nlen(api.vms.list(query=\u0026#34;page 1\u0026#34;)) And playing together, we could set a list that returns all results by running:\nvms = [] page = 0 length = 100 while length \u0026gt; 0: page = page + 1 query = \u0026#34;%s page %s\u0026#34; % (oquery, page) tanda = api.vms.list(query=query) length = len(tanda) for vm in tanda: vms.append(vm) We can also make funny things like migrate VM\u0026rsquo;s to another host by just running:\nvm.migrate() It\u0026rsquo;s expected for RHEV 3.1 to have a developer guide (now in Beta) at https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Virtualization/3.1-Beta/html-single/Developer_Guide/index.html\nCheck it for more examples of use and put the Virtualization to work for you! Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2012/10/24/rhev/ovirt-api-with-python/","summary":"\u003cp\u003e\u003ccode\u003eRHEV\u003c/code\u003e/\u003ccode\u003eoVirt\u003c/code\u003e API allows faster and simple development of scripts and utilities ranging from gathering of information to VM/host, etc manipulation.\u003c/p\u003e\n\u003cp\u003eFor example, a simple script for connecting to API and list VM\u0026rsquo;s could be:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e sys\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e getopt\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e optparse\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e os\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e time\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ovirtsdk.api \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e API\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ovirtsdk.xml \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e params\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e random \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e choice\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebaseurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://localhost:8443\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eapi \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e API(url\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ebaseurl, username\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;admin@internal\u0026#34;\u003c/span\u003e, password\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;redhat\u0026#34;\u003c/span\u003e, insecure\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e vm \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e api\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003evms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elist():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(vm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ename)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe \u003ccode\u003e.list()\u003c/code\u003e method works pretty well, but beware, it limits collections to 100 elements for performance reasons, so in those cases, we\u0026rsquo;ll need to check how many results do we have, and paginate by passing an extra argument to our \u0026ldquo;.list()\u0026rdquo; invocation, for example:\u003c/p\u003e","title":"RHEV/oVirt API with Python"},{"content":"This document is based on the original recipe and pictures that I gave to my colleague Miguel Pérez Colino which he assembled in an internal company document and that he kindly translated for increased reach.\nIngredients A bag of vegetables for paella (garrofón, Valencian white bean; ferraura y bajoqueta, Valencian green beans with pod) Rice bomba 🛒#ad (7 handfuls) Chicken and Rabbit (700 and 400 grams) Grated tomato (The equivalent to 5 spoons) Sweet Red Paprika Powder (Pimentón Rojo 🛒#ad) Spanish Saffron 🛒#ad (300mg in threads) or E102 dye (small spoon just to the border) Olive oil 🛒#ad to stir-fry (not much, so it doesn\u0026rsquo;t get oily) Rosemary 🛒#ad Salt Water (from Valencia) Preparation A garlic clove, to be grated, must be peeled and put together with tomato. They must be kept together until they are stir-fried.\nChicken must be sliced into small pieces.\nCover the floor with old magazines or newspaper to avoid stains.\nIn parallel, prepare the pot with water to get it boiling and we start to prepare the chicken.\nPot The vegetables must be boiled with water (if they are frozen) or added directly to Paella.\nThe pot must be filled with abundant water, a bit of salt and , if possible some Rosemary, so it enhances the taste later on.\nPaella Chicken must be stir-fried until it is well done.\nSome space is required in the centre for the tomato, do it with care as it may sputter.\nUsing some different space the Sweet Paprika powder must be slightly fried with care, as it shouldn\u0026rsquo;t get burned, and later sauted and mixed with tomato.\nThe boiling water from pot, together with the vegetables, must be added up to the border of the paella, adding more spare water if it\u0026rsquo;s not enough.\nAfter 30 minutes boiling, when the broth has reached the level of the rivets, it must be tasted for salt, taking care that it is a bit salty, and the rice must be added.\nThe 7 handfuls of rice, for 4 people, must be added to the paella (another option is to make a groove with it, so that it goes 2 cm above the broth), then the saffron/dye must be added and evenly distributed around the paella with meat, etc.\nIt must be kept on strong fire until the rice is half cooked and rises above the broth. (about 7 minutes)\nThen the fire must be lowered for 5 minutes more, taking care on the amount of broth that, if it becomes scarce will make us lower the fire even more. Right in this moment, a bit of Rosemary can be added to the Paella.\nThe last 3-4 minutes may end with medium to low fire depending on the amount of broth remaining.\nOnce finished, rice must be tasted, fire must be switched off, and the paella must rest for some minutes with a piece of cloth or newspapers above it if the rice was still a bit \u0026lsquo;hard\u0026rsquo;.\nYou can also become creative with the presentation once served:\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2012/07/08/paella-howto/","summary":"\u003cp\u003eThis document is based on the original recipe and pictures that I gave to my colleague \u003ca href=\"https://twitter.com/mmmmmmpc\"\u003eMiguel Pérez Colino\u003c/a\u003e which he assembled in an internal company document and that he kindly translated for increased reach.\u003c/p\u003e\n\u003ch1 id=\"ingredients\"\u003eIngredients\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eA bag of vegetables for paella (\u003ccode\u003egarrofón\u003c/code\u003e, Valencian white bean; \u003ccode\u003eferraura\u003c/code\u003e y \u003ccode\u003ebajoqueta\u003c/code\u003e,\nValencian green beans with pod)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.es/dp/B00986HSH0?tag=redken-21\"\u003eRice \u003ccode\u003ebomba\u003c/code\u003e 🛒#ad\u003c/a\u003e (7 handfuls)\u003c/li\u003e\n\u003cli\u003eChicken and Rabbit (700 and 400 grams)\u003c/li\u003e\n\u003cli\u003eGrated tomato (The equivalent to 5 spoons)\u003c/li\u003e\n\u003cli\u003eSweet Red Paprika Powder (\u003ca href=\"https://www.amazon.es/dp/B07FZLMP8N?tag=redken-21\"\u003e\u003ccode\u003ePimentón Rojo\u003c/code\u003e 🛒#ad\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.es/dp/B01N6OVPYQ?tag=redken-21\"\u003eSpanish Saffron 🛒#ad\u003c/a\u003e (300mg in threads) or \u003ca href=\"https://www.amazon.es/dp/B01HIVII4I?tag=redken-21\"\u003eE102 dye\u003c/a\u003e (small spoon just to the border)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.es/dp/B0781Z7TD4?tag=redken-21\"\u003eOlive oil 🛒#ad\u003c/a\u003e to stir-fry (not much, so it doesn\u0026rsquo;t get oily)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.es/dp/B01HN23N3S?tag=redken-21\"\u003eRosemary 🛒#ad\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSalt\u003c/li\u003e\n\u003cli\u003eWater (from Valencia)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"preparation\"\u003ePreparation\u003c/h1\u003e\n\u003cp\u003eA garlic clove, to be grated, must be peeled and put together with tomato. They must be kept together until they are stir-fried.\u003c/p\u003e","title":"Paella Howto"},{"content":"A few days ago, I was checking a friend\u0026rsquo;s system for backing up.\nIt was based on rsync at the Linux side and rsync server on a windows machine he uses for storing information generated by other software.\nThe solution with rsync worked pretty well for him until he started to put several strange characters in filenames which rendered the backup unusable to certain point.\nAfter being asked for alternative backup software that was available on several platforms, I\u0026rsquo;ve ended testing duplicity.\nAs a command line interface tool, it can run pretty well on cron jobs, and eases the management of doing full or incremental backups to several kinds of targets.\nDuplicity can also take care of tiding backup store removing older backups and automatically perform a full backup if older than x time, so you can automate the full backup management.\nBest of all is the backend support, as you can do backups over rsync, scp, ftp, file, imap, ssh, amazon, webdav, etc , so you can use that old NAS to store your backups without any special support except pretty standard protocols even on low-end models. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2011/06/27/duplicity-for-managing-backups/","summary":"\u003cp\u003eA few days ago, I was checking a friend\u0026rsquo;s system for backing up.\u003c/p\u003e\n\u003cp\u003eIt was based on \u003ccode\u003ersync\u003c/code\u003e at the Linux side and \u003ccode\u003ersync\u003c/code\u003e server on a windows machine he uses for storing information generated by other software.\u003c/p\u003e\n\u003cp\u003eThe solution with \u003ccode\u003ersync\u003c/code\u003e worked pretty well for him until he started to put several strange characters in filenames which rendered the backup unusable to certain point.\u003c/p\u003e\n\u003cp\u003eAfter being asked for alternative backup software that was available on several platforms, I\u0026rsquo;ve ended testing duplicity.\u003c/p\u003e","title":"duplicity for managing backups"},{"content":"Today I\u0026rsquo;ve decided to give it a try to F15-prealpha (expected to release tomorrow).\nThe upgrade performed in the unsupported way (getting and forcing install for newer fedora-release from a mirror then start issuing several yum upgrades) when reasonably good.\nOnly some minor dependency problems et voilà, system started fine.\nProblems so far:\nFirefox 4 has a few approved extensions, I used the \u0026ldquo;nightly tester tools\u0026rdquo; to disable version check and enable most of the ones I had with 3.6.x without problems. Don\u0026rsquo;t like gnome-shell at the moment Desktop icons disappeared\u0026hellip; Menu bar is black and found no way to configure it yet Gnome-applets have some dependency problems and can\u0026rsquo;t install or configure them, furthermore I lost my tray area on one of the computers but works fine on the other. I hope that tomorrow\u0026rsquo;s release of Alpha will fix some of them in order to warm-up for final launch (http://fedoraproject.org/wiki/Releases/15/Schedule)\u0026hellip;\nUpdate: wed 09/03/2011:\nAfter \u0026lsquo;Alpha\u0026rsquo; release:\nLike a bit more gnome-shell, but sometimes my desktop starts in compatibility (legacy) mode Active corners (upper left: show windows, favorites, search for apps, lower right: show tray) with empathy, notifications of new chats in bottom center of screen that let you answer in that area (but then if you open the window, it will \u0026lsquo;random sort\u0026rsquo; the messages you wrote/received GDM will not allow you to login (there\u0026rsquo;s an update on koji that updates accountservice that got positive karma today and will be pushed to updates repo) Under your user name in gnome-shell, there\u0026rsquo;s no \u0026lsquo;power off\u0026rsquo;, just \u0026lsquo;suspend\u0026rsquo; but that gets my desktop computer powered off in seconds, and the best of all, started again in seconds (and my computer is an Athlon 64 3Ghz with 2Gb ram from 2004) (http://www.youtube.com/watch?v=xPRh1NvOhqI) Still missing desktop icons Still missing gnome-applets :\u0026rsquo;( Update Wed 16/Mar/2011\nStill missing desktop icons /gnome-applets Sometimes gdm doesn\u0026rsquo;t show usernames, but you can enter username/password Evolution has a good integration with Google services (Contacts/Calendar/Gmail) so you can use it as an offline client Some bugs opened on bugzilla, but still more than 15 days for \u0026lsquo;Beta\u0026rsquo;, and a very good ratio of fixes :) Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2011/03/07/firsts-steps-with-fedora-15-pre-alpha/","summary":"\u003cp\u003eToday I\u0026rsquo;ve decided to give it a try to F15-prealpha (expected to release tomorrow).\u003c/p\u003e\n\u003cp\u003eThe upgrade performed in the unsupported way (getting and forcing install for newer fedora-release from a mirror then start issuing several yum upgrades) when reasonably good.\u003c/p\u003e\n\u003cp\u003eOnly some minor dependency problems \u003ccode\u003eet voilà\u003c/code\u003e, system started fine.\u003c/p\u003e\n\u003cp\u003eProblems so far:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirefox 4 has a few approved extensions, I used the \u0026ldquo;nightly tester tools\u0026rdquo; to disable version check and enable most of the ones I had with 3.6.x without problems.\u003c/li\u003e\n\u003cli\u003eDon\u0026rsquo;t like gnome-shell at the moment\u003c/li\u003e\n\u003cli\u003eDesktop icons disappeared\u0026hellip;\u003c/li\u003e\n\u003cli\u003eMenu bar is black and found no way to configure it yet\u003c/li\u003e\n\u003cli\u003eGnome-applets have some dependency problems and can\u0026rsquo;t install or configure them, furthermore I lost my tray area on one of the computers but works fine on the other.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI hope that tomorrow\u0026rsquo;s release of Alpha will fix some of them in order to warm-up for final launch (\u003ca href=\"http://fedoraproject.org/wiki/Releases/15/Schedule\"\u003ehttp://fedoraproject.org/wiki/Releases/15/Schedule\u003c/a\u003e)\u0026hellip;\u003c/p\u003e","title":"Firsts steps with Fedora 15 pre-alpha"},{"content":"Introduction A standard install media, (let\u0026rsquo;s talk about a DVD for easier start) has several files/folders at his root, but most important are:\nisolinux (where the loader lives) images (for extra files for installer to load) Packages for installation (RedHat/ for EL4, Server/Client for EL5) Usually, a distribution has, for its main binaries, more than 2 gigabytes of data, that enables one target to act as a multifunction server/workstation, but that you will not usually load on the same system. Furthermore, since the DVD creation, there have been so many updates/patches that make your installation a \u0026lsquo;outdated\u0026rsquo; install that you\u0026rsquo;ll need to upgrade to have recent patches.\nWouldn\u0026rsquo;t it be better to have one install media suited for your target systems with all available updates applied?\nPreparing everything First, we\u0026rsquo;ll need to copy all of our DVD media to a folder in our hard drive, including those hidden files on DVD root (the ones telling installer which CD-sets are included and some other info).\nLet\u0026rsquo;s assume that we\u0026rsquo;ll work on /home/user/DVD/\nAfter we\u0026rsquo;ve copied everything from our install media, we\u0026rsquo;ll start customizing :)\nDVD background image at boot prompt We can customize DVD background image and even keyboard layout by tweaking isolinux/isolinux.cfg with all required fields (Check Syslinux Documentation to check proper syntax)\nOn Kickstart: instalaciones automatizadas para anaconda (Spanish) you can also check how to create a kickstart, so you can embed it on this DVD and configure isolinux.cfg to automatic provision a system\nIncluding updates The easiest way would be to install a system with all required package set from original DVD media, and then connect that system to an update server to fetch but not install them.\nEL4: up2date -du # yum upgrade —downloadonly EL5: yum upgrade —downloadonly After you download every single update, you\u0026rsquo;ll need to copy them to a folder like /home/user/DVD/updates/.\nWell, now let\u0026rsquo;s start the funny work:\nFor each package in updates/, you\u0026rsquo;ll need to remove old version from original folder (remember: Client/ Server/ or RedHat/RPMS ), and place in that folder the updated one\u0026hellip;\nAfter some minutes, you\u0026rsquo;ll have all updates in place\u0026hellip; and you can remove the DVD/updates/ folder as it will be empty after placing each updated RPM in the folder where the previous versions was.\nRemoving unused packages Well, after having everything in place, we\u0026rsquo;ll start removing unused files. Usually, we could check every package install status on \u0026rsquo;test\u0026rsquo; system by checking rpm, but that\u0026rsquo;s going to be a way lengthy task, so we can \u0026lsquo;automate\u0026rsquo; it a bit by doing:\nIf you have ssh password-less connection between your systems (BUILD and TARGET): On BUILD system:\n#!bash for package in *.rpm do NAME=`rpm -q —queryformat \u0026#39;%NAME\u0026#39; $package` ssh TARGET \u0026#34;rpm -q $NAME \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || echo rm $package\u0026#34; |tee things-to-do done If you don\u0026rsquo;t have ssh password-less setup (using private/public key authentication or Kerberos), you can do something similar this way: On BUILD system:\n#!bash for package in *.rpm do NAME=`rpm -q —queryformat \u0026#39;%NAME\u0026#39; $package` echo \u0026#34;$package:$NAME\u0026#34; \u0026gt; packages-on-DVD done Then copy that file on your TARGET system and running:\nOn TARGET system:\n#!bash for package in `cat packages-on-DVD` do QUERY=`echo $package|cut -d \u0026#34;:\u0026#34; -f 2` FILE=`echo $package|cut -d \u0026#34;:\u0026#34; -f 1` rpm -q —queryformat \u0026#39;%NAME\u0026#39; $QUERY \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || echo rm $FILE|tee things-to-do done After you finish, you\u0026rsquo;ll have a file named things-to-do, in which you\u0026rsquo;ll see commands like rm packagename-version.rpm\nIf you\u0026rsquo;re confident about it\u0026rsquo;s contents, you can run sh things-to-do and have all \u0026rsquo;not installed on TARGET\u0026rsquo; packages removed from your DVD folder.\nAdding extra software In the same way we added updates, we can also add new software to be deployed along base system like monitoring utilities, custom software, hardware drivers, etc, just add packages to desired folders before going through next steps.\nRecreating metadata After all our adds and removals, we need to tell installer that we changed packages, and update it\u0026rsquo;s dependencies, install order, etc.\nEL4 This one is trickier, but it is still possible in a not so hard way, first of all, we need to update some metadata files (hdlist) and Package order for installation, it can be difficult if we add extra packages, as we\u0026rsquo;ll have special care:\nGenerate first version of hdlists:\n#!bash export PYTHONPATH=/usr/lib/anaconda-runtime:/usr/lib/anaconda /usr/lib/anaconda-runtime/genhdlist —withnumbers /home/user/DVD/ /usr/lib/anaconda-runtime/pkgorder /home/user/DVD/ i386 |tee /home/user/order.txt Review order.txt to check all packages added by hand to check correct or include missing packages and then continue with next commands:\n#!bash export PYTHONPATH=/usr/lib/anaconda-runtime:/usr/lib/anaconda /usr/lib/anaconda-runtime/genhdlist —withnumbers /home/user/DVD/ —fileorder /home/user/order.txt EL5 Using createrepo we\u0026rsquo;ll recreate metadata, but we\u0026rsquo;ve to keep care and use comps.xml to provide \u0026lsquo;group\u0026rsquo; information to installer, so we\u0026rsquo;ll need to run:\n#!bash createrepo -g /home/DVD/Server/repodata/groupsfile.xml /home/DVD/Server/ Finishing At this step you\u0026rsquo;ll have a DVD structure on your hard drive, and just need to get an ISO to burn and test:\n#!bash mkisofs -v -r -N -L -d -D -J -V NAME -b isolinux/isolinux.bin -c isolinux/boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -x lost+found -m .svn -o MyCustomISO.iso /home/user/DVD/ Now, it\u0026rsquo;s time to burn MyCustomISO.iso and give it a try ;-)\nPost Datum: While testing is just better to keep using rewritable media until you want to get a \u0026lsquo;release\u0026rsquo;\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2010/01/23/customize-rhel/centos-installation-media-el4/el5-/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eA standard install media, (let\u0026rsquo;s talk about a DVD for easier start) has several files/folders at his root, but most important are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eisolinux\u003c/code\u003e (where the loader lives)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimages\u003c/code\u003e (for extra files for installer to load)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ePackages\u003c/code\u003e for installation (\u003ccode\u003eRedHat/\u003c/code\u003e for EL4, \u003ccode\u003eServer\u003c/code\u003e/\u003ccode\u003eClient\u003c/code\u003e for EL5)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsually, a distribution has, for its main binaries, more than 2 gigabytes of data, that enables one target to act as a multifunction server/workstation, but that you will not usually load on the same system. Furthermore, since the DVD creation, there have been so many updates/patches that make your installation a \u0026lsquo;outdated\u0026rsquo; install that you\u0026rsquo;ll need to upgrade to have recent patches.\u003c/p\u003e","title":"Customize RHEL/CentOS installation media (EL4/EL5+)"},{"content":"MiniDLNA provides an OpenSource DLNA server software that can index and present specific folders on your computer to DLNA clients on your network.\nProject at sourceforge is distributed as CVS code that you need to checkout and compile for it to work on your computer.\nI\u0026rsquo;ve setup a spec file that will allow you to create an rpm that has been tested on Red Hat Enterprise Linux 5.3 machine x86 for easing adoption among users.\n#!spec %define dist .el%(rpm -q --queryformat=\u0026#39;%{VERSION}\u0026#39; redhat-release 2\u0026gt; /dev/null | tr -cd \u0026#39;[:digit:]\u0026#39;) Summary: DLNA compatible server Name: MiniDLNA Version: 1.0.14 Release: 6 License: GPL Group: System Environment/Utilities Source: %{name}-%{version}.tar.gz BuildRoot: /var/tmp/%{name}-buildroot Vendor: MiniDLNA SF group https://sourceforge.net/projects/minidlna/ Packager: Pablo Iranzo Gómez (Pablo.Iranzo@uv.es) BuildRequires: flac-devel, libvorbis-devel, libexif-devel, sqlite-devel, uuid-devel,ffmpeg-devel,libid3tag-devel, libjpeg-devel, e2fsprogs-devel, cvs Requires: redhat-lsb %description MiniDLNA is a DLNA UPnP AV compatible server for being used by other DLNA capable devices %prep [ -d %{name} ] \u0026amp;\u0026amp; rm -Rfv %{name} mkdir %{name} cd %{name} cvs -q -d:pserver:anonymous@minidlna.cvs.sourceforge.net:/cvsroot/minidlna login cvs -z3 -d:pserver:anonymous@minidlna.cvs.sourceforge.net:/cvsroot/minidlna co -P minidlna %build cd %{name}/minidlna sh genconfig.sh make cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; initscript #!/bin/sh # chkconfig: 345 99 10 # description: Startup/shutdown script for MiniDLNA daemon # # \\$Id: minidlna.init.d.script,v 1.2 2009/07/02 00:33:15 jmaggard Exp \\$ # MiniUPnP project # author: Thomas Bernard # website: http://miniupnp.free.fr/ or http://miniupnp.tuxfamily.org/ # Modified for RHEL Compatibility by Pablo Iranzo Gómez (Pablo.Iranzo@uv.es), http://Alufis35.uv.es/~iranzo/ MINIDLNA=/usr/sbin/minidlna ARGS=\u0026#39;-f /etc/minidlna.conf\u0026#39; test -f \\$MINIDLNA || exit 0 . /lib/lsb/init-functions case \u0026#34;\\$1\u0026#34; in start) MSG=\u0026#34;Starting minidlna\u0026#34; start_daemon \\$MINIDLNA $ARGS $LSBNAMES \u0026amp;\u0026amp; log_success_msg \\$MSG || log_failure_msg \\$MSG ;; stop) MSG=\u0026#34;Stopping minidlna\u0026#34; killproc \\$MINIDLNA \u0026amp;\u0026amp; log_success_msg \\$MSG || log_failure_msg \\$MSG ;; restart|reload|force-reload) \\$0 stop \\$0 start ;; *) log_action_msg \u0026#34;Usage: /etc/init.d/minidlna {start|stop|restart|reload|force-reload}\u0026#34; exit 2 ;; esac exit 0 EOF %install cd %{name}/minidlna %{__install} -D -m0755 minidlna %{buildroot}/usr/sbin/minidlna %{__install} -D -m0644 minidlna.conf %{buildroot}/etc/minidlna.conf %{__install} -D -m0755 initscript %{buildroot}/etc/rc.d/init.d/minidlna %clean rm -rf $RPM_BUILD_ROOT %files %defattr(-,root,root) %config /etc/minidlna.conf /usr/sbin/minidlna /etc/rc.d/init.d/minidlna %post chkconfig --add minidlna %preun service minidlna stop chkconfig --del minidlna %changelog * Wed Jul 1 2009 Pablo Iranzo Gómez (Pablo.Iranzo@uv.es) - Initial version Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2009/07/10/minidlna-spec-rpm/","summary":"\u003cp\u003eMiniDLNA provides an OpenSource DLNA server software that can index and present specific folders on your computer to DLNA clients on your network.\u003c/p\u003e\n\u003cp\u003eProject at sourceforge is distributed as CVS code that you need to checkout and compile for it to work on your computer.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve setup a spec file that will allow you to create an rpm that has been tested on Red Hat Enterprise Linux 5.3 machine x86 for easing adoption among users.\u003c/p\u003e","title":"MiniDLNA SPEC \u0026 RPM"},{"content":"I\u0026rsquo;ve been using a Xen guest under RHEL 5.2 to hold this Webserver, and because of failures, I choose to keep a copy of the full disk image on another machine.\nHaving to transfer the full disk in the network means stop the server (Xen guest), rsync the image on disk (wait 40 minutes), then start guest again.\nAfter doing the initial image transfer, it would be easier to just sync updated files, but\u0026hellip; how to loop mount a full disk?\nIn my case, the HDD image contained a partition for /boot and a partition for a LVM pv.\nFirst, I needed to check Number of cylinders in virtual disk inside Xen Guest. Using fdisk I could check that number, for example, 777.\nOn the remote system, the one with the full image transferred previously I could then do:\n#!bash fdisk /var/lib/xen/images/GUEST.img -C 777 -l -u # and will yield something like: 255 heads, 63 sectors/track, 777 cylinders, 0 sectores en total Unidades = sectores de 1 * 512 = 512 bytes Disposit. Inicio Comienzo Fin Bloques Id Sistema /var/lib/xen/images/GUEST.img1 * 63 208844 104391 83 Linux /var/lib/xen/images/GUEST.img2 208845 12482504 6136830 8e Linux LVM In this case, as I want to access my LVM volume, so I need to convert the partition start to a size, so:\n#!bash START=512*208845 = 106928640 and then thanks to losetup:\n#!bash libre=`losetup -f` #Get a free loop device losetup -o 106928640 $libre /var/lib/xen/images/GUEST.img #setup the device for the 2nd partition pvscan #scan for LVM\u0026#39;s pv vgscan # same for VG lvscan # same for LV\u0026#39;s lvchange -a y /dev/GUESTvg #activate LV\u0026#39;s on \u0026#39;GUESTvg\u0026#39; VG and mount our drives doing mount /dev/GUESTvg/LVMunit where desired ;)\nAt this point I can just run:\n#!bash rsync -avur —perms —progress —delete remoteserver:/ /mnt/DISKIMAGE And get a \u0026lsquo;working\u0026rsquo; copy of the remote machine but just copying changed elements. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2008/07/18/mount-disk-images/","summary":"\u003cp\u003eI\u0026rsquo;ve been using a Xen guest under RHEL 5.2 to hold this Webserver, and because of failures, I choose to keep a copy of the full disk image on another machine.\u003c/p\u003e\n\u003cp\u003eHaving to transfer the full disk in the network means stop the server (Xen guest),\n\u003ccode\u003ersync\u003c/code\u003e the image on disk (wait 40 minutes), then start guest again.\u003c/p\u003e\n\u003cp\u003eAfter doing the initial image transfer, it would be easier to just sync updated files, but\u0026hellip; how to loop mount a full disk?\u003c/p\u003e","title":"Mount disk images"},{"content":"Introduction LVM are the initials for Logical Volume Manager, a powerful tool present in actual Linux systems inspired in the implementation available in other systems like AIX and HP-UX.\nLVM introduces a separation between system structure and elements like disks, partitions, filesystems to which we are used to.\nLVM has three levels:\nPhysical volumes Volume Groups Logical volumes One of the benefits of LVM over traditional systems is that LVM introduces an abstraction layer which improves the limitation of a disk, allowing us to have a filesystem to span over several disks, resizing thus making a more efficient usage of storage.\nVolumes make use of PE1, which are the units (there\u0026rsquo;s a relationship set during creation) used for measuring size and future size changes of the volumes.\nLVM structure is as shown:\nPhysical volumes (PV) A physical volume is a disk or a slice of it that we will designate for inclusion in a volume group.\nPhysical volumes can be places in a partition (for example if they have to coexist with another operating systems), or well span over the whole disk, even over md devices2.\nVolume groups (VG) Volume groups are defined grouping one or more physical volumes and are, as if they where physical disks which take their size from the different physical volumes associated to them.\nLogical volumes (LV) Logical volumes are created inside a volume group and are the equivalent to partitions in other systems, is the part of LVM that we format with a filesystem and we join afterwards to our system for being used for storage.\nCommands All commands related with LVM use a similar naming, which starts with the element they affect to:\npv(change, display, remove, create, move, resize, scan) for physical volumes. vg(convert, extend, reduce, scan, create, import, remove, split, change, display, merge, rename, export) for volume groups lv(change, display, convert, extend, remove, rename, scan, create, reduce, resize) for logical volumes System preparation for LVM Hard drive partitioning Before using LVM we need to designate some devices (full drives or partitions (type 8e on fdisk))\nAfter doing so, we need to execute partprobe to refresh the partition layouts in running kernel with the defined schema.\nIn our example, we\u0026rsquo;ll have two hard drives: hda and sda.\nIn hda we have one partition, hda1 in which we store /boot (partition for kernel and initrd, as they cannot be on a LVM volume), and the rest is in a partition hda2 which grows until the full size of the drive.\nIn sda we have the whole disk available for being used as LVM, so we will define a partition sda1 for this use.\nCreation of physical volumes (PV) Physical volumes are the places in which we put the structure for volume groups, it\u0026rsquo;s creation is as easy as running:\n#!bash pvcreate /dev/hda2 pvcreate /dev/sda1 If we execute pvscan, we can check the listing of physical volumes defined on the system, as well as the kind of metadata (lvm or lvm2) and it\u0026rsquo;s size, and a briefing on total, used and available.\nFor having a detailed state, we can execute pvdisplay, which will show more information like size, PE's available, etc.\nCreation of volume groups (VG) Volume groups are like drawers, which placed over physical volumes, that define the grouping for logical volumes, making more clear the structure.\nFor creating the volume group we will execute:\n#!bash vgcreate Prueba /dev/sda1 This command will create a volume group named Prueba over the physical volume at /dev/sda1\nFor checking that the action went ok, was we can execute vgscan for getting a listing of defined volume groups.\nCreation of Logical volumes (LV) Logical volumes are the equivalent to partitions, is the place over which we will put a filesystem and thus, the data we want to store.\nLogical volumes are defined inside a volume group in the following way:\n#!bash lvcreate Prueba -n Inicial -L 2G If we execute lvscan, we will have a listing of all the defined volume groups and it\u0026rsquo;s size and between them, Inicial, inside of Prueba and with a 2Gb size\nCreation of a filesystem Before using the logical volume we\u0026rsquo;ll need to prepare it for data, and creating a filesystem, this time, the command is identical to the one we use over a physical disk, but specifying the logical volume, for example:\n#!bash mkfs.ext3 /dev/Prueba/Inicial It\u0026rsquo;s advisable to make use of a filesystem we can resize, as, one the improvements of LVM is the ability to resize logical units and, requires, as consequence that the filesystem works as a \u0026ldquo;container\u0026rdquo; able to grow or shrink the same way.\nNow, we can mount the filesystem:\n#!bash mount /dev/Prueba/Inicial /mnt Resizing an LVM \u0026ldquo;drive\u0026rdquo; EXT3, allows resizing of volumes, so they can grow while being used, but for reducing size we need to stop its usage, and even in that case, it allows the interesting opportunity to create small filesystems, fitted for our initial usage and then keep them growing based on our needs without stop running.\nAs an example, we\u0026rsquo;re going to extend our Inicial filesystem, increasing it in 250Mb, we\u0026rsquo;ll execute:\n#!bash pvscan #(where we will be told about physical volumes and free space) # In case of need, we can extend our volume group with: pvcreate /dev/disconuevo vgextend Prueba /dev/disconuevo # From now on, the vg will have the space ready for being used by lv and we will make it grow doing: lvextend -L +250M /dev/Prueba/Inicial ext2online /dev/mapper/Prueba-Inicial After it finishes, the filesystem at /mnt will have an extra 250Mb available.\nWe can make the filesystem grow to a total size, for example, make it grow until 4Gb, to do so, we will do:\n#!bash lvextend -L 4G /dev/Prueba/Inicial ext2online /dev/mapper/Prueba-Inicial Starting in Fedora Core 6 (FC6), the utility ext2online doesn\u0026rsquo;t exist, as it has been integrated in resize2fs, so we will execute instead: resize2fs -p /dev/Prueba-Inicial [final size].\nATTENTION: This is a dangerous step, we can lose data\nIf we want to reduce the size of a logical volume, first, we\u0026rsquo;ll need to know how much space is used by the filesystem and then, umount it:\n#!bash umount /dev/mapper/Prueba-Inicial #Next step would be reduce the filesystem: resize2fs /dev/mapper/Prueba-Inicial [new size] I\u0026rsquo;m recommending reduce the filesystem under the final size we want to achieve, so we have a security margin. This size must ALWAYS be higher that the usage3 of the volume.\nWhen the process has ended, we can reduce the volume running:\nlvextend -L -2G /dev/mapper/Prueba-Inicial\nAnd now we will make the filesystem grow to regain the \u0026ldquo;security margin\u0026rdquo; we left with: resize2fs /dev/mapper/Prueba-Inicial.\nGraphical tool Red Hat or Fedora have a graphical tool system-config-lvm which allows to manage logical volumes in our system in the following way.\nIn the next screenshot we can see the physical volumes no assigned to a volume group, and with the options provided by the tool, we can add to an existing volume group or well, create a new volume group:\nHere we can create a new logical volume inside the volume group, we can specify the volume name, the kind of volume as well as size and filesystem:\nHere we can see the volume group \u0026ldquo;Test\u0026rdquo; and the logical and physical view created (as we see, Test is made of four physical volumes: sdb1,sdb2,sdb3,sdb4)\nWhen we select the free space of the logical volume, the following is shown: number of extents, physical location for each physical volume.\nPhysical Extents\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMultiple Devices: is a software technology that allows creation of several disk grouping levels: linear, raid0, raid1, raid5. Devices will identify themselves to the system as /dev/md*0,1,2,3,etc*, and it\u0026rsquo;s status (defined md's, sync status, etc) in the file /proc/mdstat\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe one that we wrote down in the first step, before it was unmounted. Enjoy! (and if you do, you can Buy Me a Coffee ) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iranzo.io/blog/2008/04/20/logical-volume-manager-lvm/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eLVM are the initials for Logical Volume Manager, a powerful tool present in actual Linux systems inspired in the implementation available in other systems like AIX and HP-UX.\u003c/p\u003e\n\u003cp\u003eLVM introduces a separation between system structure and elements like disks, partitions, filesystems to which we are used to.\u003c/p\u003e\n\u003cp\u003eLVM has three levels:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePhysical volumes\u003c/li\u003e\n\u003cli\u003eVolume Groups\u003c/li\u003e\n\u003cli\u003eLogical volumes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOne of the benefits of LVM over traditional systems is that LVM introduces an abstraction layer which improves the limitation of a disk, allowing us to have a filesystem to span over several disks, resizing thus making a more efficient usage of storage.\u003c/p\u003e","title":"Logical volume manager (LVM)"},{"content":"Introduction SELinux is an implementation of MAC (Mandatory Access Controls) over LSM (Linux Security Modules) in Linux Kernel.\nSELinux, originally developed by N.S.A. (National Security Agency) allows applications to be confined by the kernel.\nInside that \u0026ldquo;confined area\u0026rdquo;, much more grained than a standard chroot (system where basic executables are copied to another folder in order to have a small subsystem isolated from real system. The drawback is that a small subsystem could have enough utilities to reveal private information from our internal network),in which we can allow only certain operations, for example: adding information to a file, read from a directory but not writing, even just for one file in a standard directory, etc\u0026hellip;\nPolicies Each policy has different applications, and restrictions to the running system. The most extended ones are:\nTargeted Strict MLS (Multi Level Security http://www.cs.stthomas.edu/faculty/resmith/r/mls/index.html A policy is a set of \u0026ldquo;rules\u0026rdquo; that apply to any object of a system, users, software, files, network, etc. Those rules apply to security contexts, where a \u0026ldquo;context\u0026rdquo; for an object is composed of:\nuser:role:type:level:category\nBased on this information, SELinux allows or denies access to any other object in system, and for doing so, it uses rules that allow process access permissions to objects.\nA strict policy would require a very careful setup of system permissions, security contexts, etc, and there are several projects in order to accomplish it.\nThere are several sites on the Internet which provide a locked-down environment for testing (there\u0026rsquo;s one to play at http://www.coker.com.au/selinux/play.html). Those sites make extensive usage of strict policies as well as numerous restrictions to establish user roles, limits, etc.\nA system running in MLS/Strict would be difficult to use as due to security restrictions required by the policy, no windowing environment (there are some projects to achieve this,but using very simple windowing environments) would be available to avoid copying of information between windows (as this could avoid security levels).\nThe Fedora/Red Hat/CentOS approach Those distributions decided to focus on just some services. Instead of using a system-wide restrictive policy, that would make system unusable, their choice was to \u0026ldquo;target\u0026rdquo; services that are subject to attacks, so, for example, there\u0026rsquo;s a policy for Apache that defines exactly what files can read, what can write, what are log files, what should access from disk, etc.\nThis policy is named \u0026ldquo;targeted\u0026rdquo; and for services defined within, it restricts what they can do, and the rest of the system runs as unconfined, as if there was no SELinux enabled at all.\nThis policy balances the security provided by SELinux for critical services, while keeping system usable. Services without rules continue working as on a system with SELinux disabled.\nLearning by example: Apache Working with SELinux For working with SELinux we have several tools available, most of them, are old friends: ls, ps, id, etc.\nMost of those tools have been expanded to use SELinux and have extra parameters, for example, in our example:\nls -lZ /usr/sbin/httpd\\* rwxr-xr-x root root system_u:object_r:httpd_exec_t /usr/sbin/httpd rwxr-xr-x root root system_u:object_r:httpd_exec_t /usr/sbin/httpd.worker The \u0026ldquo;-Z\u0026rdquo; tells ls to show SELinux attributes.\nWhere does system stores them First versions of SELinux, used external files, but in order to get included into upstream Kernel, a more standard approach was required, and finally, it got implemented using system \u0026ldquo;Extended Attributes\u0026rdquo;.\nSo, as a consequence, SELinux requires filesystems with xattr support in order to work.\nIn the listing before, apache is listed as system user, object role and httpd_exec type.\nIf we do a ps auxZ|grep httpd\nroot:system_r:httpd_t apache 2923 0.0 0.4 10424 2076 ? S 00:58 0:00 /usr/sbin/httpd We see that process httpd is being executed as root, system role, and httpd type.\nHey, how could that be possible? executable was httpd_exec, but process is httpd_t\u0026hellip;\nWell, SELinux uses process transitions, in this case, httpd_exec_t, when executed, translates to httpd_t and from now on, would be the type regulating what process can or cannot do on our system.\nHow do we start httpd daemon We use a script at /etc/init.d/httpd which:\nls -lZ /etc/init.d/httpd rwxr-xr-x root root system_u:object_r:initrc_exec_t /etc/init.d/httpd Is yet another type!!!\nSELinux defines another kind of \u0026ldquo;domain transition\u0026rdquo; which allows that process started by \u0026ldquo;root\u0026rdquo; or by init transition to final \u0026ldquo;httpd_t\u0026rdquo;, all those rules need to be defined in a module for SELinux.\nAre we supposed to do everything like this on our own Not really ;-). There are several macros that automate this (for example, from /usr/share/selinux/devel/example.te), we can see:\ndomain_type(myapp_t) domain_entry_file(myapp_t, myapp_exec_t) That automatically setup a domain type myapp_t, and a transition from myapp_exec_t to myapp_t when that executable is loaded for running it on our system.\nHow does apache loads its config files ls -lZd /etc/httpd/ drwxr-xr-x root root system_u:object_r:httpd_config_t /etc/httpd/ As Apache will need to use that directory, we need to put in our custom template that permission, so we will write on our template:\nallow httpd_t httpd_config_t:dir r_dir_perms; allow httpd_t httpd_config_t:file r_file_perms; This will give permission to access \u0026quot;dir\u0026quot;ectories with read-only permissions as well as read-only permission to \u0026ldquo;file\u0026quot;s with that type.\nWhy do this? Well. it\u0026rsquo;s a question about security\u0026hellip; ¿why should Apache write to configuration files? SELinux allows us to confine apache in just the minimum required permissions to work without any chance to harm our system.\nIn a chroot environment, an attacker could get into a minimal system which could give access to our internal network, SELinux will limit this :-)\nHow do we setup correct SELinux rules to allow something blocked Well, our installed system has a graphical SELinux troubleshooter that will pop up when there are any kind of problems with rules.\nThere is an command-line utility named \u0026ldquo;audit2allow\u0026rdquo; that will told us what we need to enable in order to get that problem fixed, for example:\nWhat if we try to run httpd listening on port 27 audit2allow -a #============= httpd_t ============== allow httpd_t reserved_port_t:tcp_socket name_bind; Well, this will be the \u0026ldquo;fix\u0026rdquo;, but this is a \u0026ldquo;dirty\u0026rdquo; one, as will allow Apache to hook on any reserved port.\nSELinux provides semanage that allows to define several behaviors, for example, the ports available to use by httpd_t:\nsemanage port -l|grep http http_port_t tcp 80, 443, 488, 8008, 8009, 8443 Well, http_t uses http_port_t that enables Apache to hook on those ports, if we need to make apache to listen on 27, we\u0026rsquo;ll need to execute:\nsemanage port -a -t http_port_t -p tcp 27\nThis will add port 27 using TCP to http_port_t type, and this will make Apache work ;-)\nsemanage also helps into defining clearance levels for users, and map user logins to security levels, so we can have users that get a specific clearance, that even root will not be able to access, so those files would be \u0026ldquo;private\u0026rdquo;.\nsemanage login -l Login Name SELinux User MLS/MCS Range __default__ user_u s0 root root SystemLow-SystemHigh In this case, any user gets mapped to user_u and s0 level, root instead, gets root user and s0.c0 to s0.c255 level\nWe can test with policies and disabling them or not using setenforce. Set enforce allows to switch SELinux enforcing behaviour without rebooting system, so we can freely test our rules without wasting time on reboots.\nWrite a simple policy Well, in order to write a simple policy, we need the selinux-devel package on our system, and then:\ncp /usr/share/selinux/devel/Makefile /root cd /root Now we will need to edit a template and put something like this:\npolicy_module(hello-world,1.0.0) type myapp_t; type myapp_exec_t; domain_type(myapp_t) domain_entry_file(myapp_t, myapp_exec_t) type myapp_log_t; logging_log_file(myapp_log_t) type myapp_tmp_t; files_tmp_file(myapp_tmp_t) allow myapp_t myapp_log_t:file ra_file_perms; allow myapp_t myapp_tmp_t:file manage_file_perms; files_tmp_filetrans(myapp_t,myapp_tmp_t,file) This was the sample template placed at /usr/share/selinux/devel/example.te, using it together with example.fc:\n/usr/sbin/myapp — gen_context(system_u:object_r:myapp_exec_t,s0) Will provide a complete policy for our app:\nThis defines that /usr/sbin/myapp will have the context system_u:object_r:myapp_exec_t and s0. Depending on the policy we\u0026rsquo;re running, the gen_context will create adequate context for that file.\nIn order to get it loaded, we just need to do make load, and module will be compiled (they use m4 macro language), and then loaded.\nHow do we verify that a module has been loaded With semodule -l all loaded modules will be shown, remember that some policies allow booleans to enable or disable certain aspects of them. We can check all defined booleans for currently loaded modules with getsebool -a and we can set them with setsebool , using \u0026ldquo;-P\u0026rdquo; in order to set that value as default after reboots.\nWe can switch a value using togglesebool value but this will not set it as default for next reboot.\nI hope this can help you as a small introduction to SELinux and it\u0026rsquo;s features :)\nPD: News and updates about SELinux at SELinux News Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2008/01/04/security-enhanced-linux-selinux/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eSELinux is an implementation of MAC (Mandatory Access Controls) over LSM (Linux Security Modules) in Linux Kernel.\u003c/p\u003e\n\u003cp\u003eSELinux, originally developed by N.S.A. (National Security Agency) allows applications to be confined by the kernel.\u003c/p\u003e\n\u003cp\u003eInside that \u0026ldquo;confined area\u0026rdquo;, much more grained than a standard \u003ccode\u003echroot\u003c/code\u003e (system where basic executables are copied to another folder in order to have a small subsystem isolated from real system. The drawback is that a small subsystem could have enough utilities to reveal private information from our internal network),in which we can allow only certain operations, for example: adding information to a file, read from a directory but not writing, even just for one file in a standard directory,\netc\u0026hellip;\u003c/p\u003e","title":"Security Enhanced Linux (SELinux)"},{"content":"Introduction JigDo (JIGsaw DOwnload) is a small utility that can assemble a CD/DVD image from it\u0026rsquo;s internal files.\nFor example, Debian has been using it for years for distributing the entire distribution: you downloaded a .jigdo file, and then, using the utility jigdo-lite (package jigdo-file on Debian like and RPM based1. This way, you only downloaded small files from servers, preventing line failures, spreading load between several servers, etc.\nFurthermore, if you already had some files (for example if you started at version X and have been downloading and keeping all files until X.Y, jigdo, can use those updated files to compare them against the .jigdo file and avoid downloading duplicated files\u0026hellip;\n¿How it works? A JigDo download contains two parts, one, the .jigdo file which contains the files part of the ISO2 image and a template file automatically generated when creating the .jigdo using jigdo-file.\nWhen specified jigdo-lite file.jigdo will ask for previous locations (folders) that could contain required packages, and will use the URL\u0026rsquo;s described in .jigdo file to download any missing package and reassemble an ISO image with same MD5sum of original, but saving on bandwidth and avoiding re-downloads from network failures (just small file compared to a full DVD ISO)\n¿How do I create a .jigdo and a .template file? For example, if we download CentOS 5 DVD ISO from mirror.centos.org and then mount it loopback and put on a folder (for example for providing our own mirror, or kickstart-able tree) we can do:\n(Supposing that ISO is at /var/www/CentOS/isos/ and that our exploded directory and file tree at /var/www/CentOS/tree/, we will do:\n#!bash jigdo-file mt -i /var/www/CentOS/isos/CentOS-5.0-i386-bin-DVD/CentOS-5.0-i386-bin-DVD.iso -j /var/www/CentOS/Centos5-DVD.jigdo -t /var/www/CentOS/Centos5-DVD.template —uri Centosmirrors=[http://mirror.centos.org/centos-5/5/os/i386/](http://mirror.centos.org/centos-5/5/os/i386/) /var/www/CentOS/tree/ After some processing and ISO\u0026lt;-\u0026gt;tree verification we will have two new files.\nUsing a text editor we can modify our .jigdo file and set the URL for our source packages (for example: http://mirror.centos.org/centos-5/5/os/i386)\nAfter that, if we provide those files to anyone, he/she could enjoy faster and better downloads of our ISO\u0026rsquo;s that will get reconstructed automatically on target system\n¿How do I get an ISO from a .jigdo file? Easy:\njigdo-lite http://SOMESERVER/PATH/file.jigdo\nJigDo will download the descriptor, search it for the template file, download it and begin creation of ISO by downloading packages not found on optional local folder (to avoid re-download of packages).\nEnjoy! (and if you do, you can Buy Me a Coffee ) Attached are the sample .jigdo and .template that I created for this document, right now it allows you to get a Centos 5 DVD using individual RPM files.\nYou can test it using jigdo-lite http://alufis35.uv.es/deploy/Centos5-DVD.jigdo\nOr: jigdo-lite http://alufis35.uv.es/deploy/CentOS51-DVD.jigdo\nFrom Dag\u0026rsquo;s Repo\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt can also be used to deliver other kind of files\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iranzo.io/blog/2007/11/15/creating-jigsaw-download-jigdo-files-for-downloading-isos/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eJigDo (\u003ccode\u003eJIGsaw DOwnload\u003c/code\u003e) is a small utility that can assemble a CD/DVD image from it\u0026rsquo;s internal files.\u003c/p\u003e\n\u003cp\u003eFor example, \u003ca href=\"http://www.debian.org/\"\u003eDebian\u003c/a\u003e has been using it for years for distributing the entire distribution: you downloaded a\n\u003ccode\u003e.jigdo\u003c/code\u003e file, and then, using the utility \u003ccode\u003ejigdo-lite\u003c/code\u003e (package\n\u003ccode\u003ejigdo-file\u003c/code\u003e on Debian like and RPM based\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. This way, you only downloaded small files from servers, preventing line failures, spreading load between several servers, etc.\u003c/p\u003e\n\u003cp\u003eFurthermore, if you already had some files (for example if you started at version X and have been downloading and keeping\nall files until X.Y,\n\u003ccode\u003ejigdo\u003c/code\u003e, can use those updated files to compare them against the \u003ccode\u003e.jigdo\u003c/code\u003e file and avoid downloading duplicated files\u0026hellip;\u003c/p\u003e","title":"Creating Jigsaw Download (JigDo) files for downloading ISO's"},{"content":"Introduction I\u0026rsquo;m working for a \u0026ldquo;very concerned about security\u0026rdquo; firm, that makes mandatory using VPN for accessing their network, and internal services:\nIMAPS SMTP Intranet Forums Online training, etc As it should, we provided services for a client, also very concerned about security, thus not allowing internet access despite of using two squid proxies with a network appliance filtering protocols, scripts, viruses and malware. They only allow FTP, HTTP and HTTPS.\nWhat to do when you need to access your business services from within the client network?\nIngredients SSH an intermediate computer outside both networks (at home, for example\u0026hellip;) a machine which you can reach by SSH on your business a squid server Extras SSH Proxy Command by Shun-ichi GOTO Preparation First of all, we need to be able to exit from our client network, and the only way is to use HTTP, HTTPS or FTP\u0026hellip; and if we use HTTP packet filtering, would block access, so we only have the SSL choice, as it is cyphered, and doesn\u0026rsquo;t tolerate \u0026ldquo;Man-in-the-middle\u0026rdquo; attacks, and allows us to get internet traffic trough it.\nSSH Proxy command SSH Proxy command, is a excellent piece of code, distributed in C in only one file that we will get compiled with gcc command.c -o connect\nWith \u0026ldquo;connect\u0026rdquo; we will get a connection, for example SSH trough squid. The way to configure it is just editing .ssh/config file and make it look kind of sort like this:\nHost home.com KeepAlive yes ProxyCommand connect -H proxy.client.com:3128 %h %p Now, when we execute ssh home.com 22 a SSH connection will be made using the squid proxy.\nFirst problem arises, finding that we have a very smart client, and blocks every connection getting trough squid ending in privileged-ports despite of ftp(21),http(80) or https(443)\u0026hellip;\nAs we have full-control of our computer at home, we can make SSH listen to 22 and 2222 adding a line Listen 2222 in /etc/ssh/sshd_config.\nWell, after this step, we have exterior connectivity, and we can make use of a good utility that SSH provides us: \u0026ldquo;tunnels\u0026rdquo; that will pass inside the SSH connection, so let\u0026rsquo;s use one text editor and begin modifying .ssh/config to add something like:\nHost casa.com KeepAlive yes ProxyCommand connect -H proxy.cliente.com:3128 %h 2222 LocalForward 10993 127.0.0.1:10993 LocalForward 1025 127.0.0.1:1025 LocalForward 2225 127.0.0.1:2225 compression yes` In this way, after establishing connection (trough the proxy) we will also establish three tunnels that will link 10993, 1025 and 2225 ports with 10993, 1025 and 2225 from our house\u0026rsquo;s computer.\nThis step could have been avoided connecting directly to our business computer (but as they are also very concerned about security (filtering, one time passwords (s/key), etc), they aren\u0026rsquo;t willing to change is security policies listening on extra port)\u0026hellip;\nWe will need to repeat this configuration at home, this time, without using SSH Proxy Command, as we have full internet access from home.\nLet\u0026rsquo;s edit .ssh/config from our house\u0026rsquo;s machine, and we will add the following configuration:\nHost business.com compression yes KeepAlive yes LocalForward 10993 business.com:993 LocalForward 1025 business.com:25 LocalForward 2225 business.com:3128 Now, if we connect using SSH from home, we will redirect another set of ports\u0026hellip;\nLaunch script:\n#!bash echo \u0026#34;Launching SSH tunnel\u0026#34; ssh -fNC home.com # Runs connection trough http proxy, and exits, leaving ssh echo \u0026#34;Starting ssh tunnels from home\u0026#34; ssh -t home.com ssh -ftNC business.com #redirects tty, allowing us to enter one-time-password, and because it will not forks to background, process will appear as blocked at this point for a while killtunnel script:\n#!bash #(Requires improvements like identifying started processes for not killing other not opened by \u0026#34;launch\u0026#34; script. echo \u0026#34;Script connected, press enter for disconnecting\u0026#34; read echo \u0026#34;Killing remote session\u0026#34; ssh -f casa.com killall -9 ssh echo \u0026#34;Killing local session\u0026#34; killall -9 ssh` Now, we can run at our command line lanzar and automatically all tunnels will be created for accessing IMAPS, SMTP and our business internal Proxy all by just using 10993, 1025 \u0026amp; 2225 of our local machine (all within a only-web internet access network ;) )\nSpecial thanks to a colleague, JMP for original script for using SSH tunnels that I adapted for this manual. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2006/11/01/ssh-stunnel-and-a-proxy-double-stunnel-bypass/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eI\u0026rsquo;m working for a \u0026ldquo;very concerned about security\u0026rdquo; firm, that makes mandatory using VPN for accessing their network, and internal services:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIMAPS\u003c/li\u003e\n\u003cli\u003eSMTP\u003c/li\u003e\n\u003cli\u003eIntranet\u003c/li\u003e\n\u003cli\u003eForums\u003c/li\u003e\n\u003cli\u003eOnline training, etc\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs it should, we provided services for a client, also very concerned about security, thus not allowing internet access despite of using two squid proxies with a network appliance filtering protocols, scripts, viruses and malware. They only allow FTP, HTTP and HTTPS.\u003c/p\u003e","title":"SSH, stunnel and a proxy - double stunnel bypass"},{"content":"Prior to using the following info for creating own-made packages, let\u0026rsquo;s test if everything is working fine.\nI\u0026rsquo;ve created a NSIS script that writes into registry in a key called HKLMSOFTWAREOCS and puts a key named \u0026ldquo;cert\u0026rdquo; with value creilla.\nIf we create a package with action \u0026ldquo;LAUNCH\u0026rdquo;, and attach the regcert.zip with command to execute regcert.exe, all clients with functional package deployment, will add that key to registry, so we can check, using OCS registry query function for a key named \u0026ldquo;cert\u0026rdquo; into: HKLM SOFTWAREOCS.\nAfter some time, affected and with working software deployment (Read OCS Inventory Package deployment to learn how to set up it properly), we could know in which ones it\u0026rsquo;s working and in which ones it doesn\u0026rsquo;t work, just doing a search for computers with a special value in that registry key. (I\u0026rsquo;ve also uploaded the precompiled version I use for your testing purposes).\nSoftware ------------------------------------------------------------------- ----------------------------------------------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- **Software** Command switches **Result** [OCS Inventory](http://ocsinventory.sf.net/) /S /SERVER:serverip Installs new OCS version on top of older one, this is only recommended if package deployment was setup successfully, if not, use NSIS script referred at OCS Inventory Package Deployment [VNC](http://www.realvnc.com/) /SP- /VERYSILENT Installs VNC without any user intervention [PowerOff](http://users.pandora.be/jbosman/poweroff/poweroff.htm) Just copy to desired folder Allows to shutdown, reboot hibernate, etc remotely or from command line [EICAR](http://www.eicar.org/) Copy and run from any folder This file is detected by antivirus as a virus, but it isn\u0026#39;t harmful (read [http://www.eicar.org](http://www.eicar.org/)), if execution fails... target computer has a working antivirus engine running [Firefox](http://www.mozilla.com/) -ms -ira Installs Mozilla Firefox silently (with 1.5 it show uncompressing dialog, but with 2b2 it\u0026#39;s totally invisible) [Java](http://www.java.com/) /S /v/qn\u0026#34; WEBSTARTICON=0 REBOOT=Suppress MOZILLA=1 IEXPLORER=1\u0026#34; Installs Java Runtime Environment (tested on 1.5.0.0.6) silently, totally invisible [Acrobat Reader](http://www.adobe.com/) msiexec.exe /i acrobat.msi /qb-! Silent install, must first uncompress installation and copy extracted files to a folder, zip them, and upload to OCS (you can extract Acrobat components doing: AdbeRdr80.exe -nos_s -nos_ne -nos_oC:temp) [Flash Player](http://www.adobe.com/) msiexec /i fp9.msi /qn Silent install using windows installer interface (it\u0026#39;s ok for other programs distributed in msi files ------------------------------------------------------------------- ----------------------------------------------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Registry Hive Path Value Utility ------ -------------------------------------------------------------------------------- -------------- --------------------------------------------------------------------------- HKLM SOFTWAREMicrosoftWindowsCurrentVersionRun * Know which programs are executed at startup HKCU SOFTWAREMicrosoftWindowsCurrentVersionRun * Know which programs are executed at user login HKLM SOFTWARENetwork AssociatesTVDShared ComponentsVirusScan Engine4.0.xx szVirDefDate Know Mcafee Virus Definitions date HKLM SoftwareSymantecSharedDefs * Shows different norton products definitions HKLM SoftwareMicrosoftWindowsCurrentVersion ProductKey Windows 9x Install key HKLM SoftwareMicrosoftWindowsCurrentVersionRunServices * Services in the machine HKLM SoftwareMicrosoftWindowsCurrentVersionRunServices * Windows 9x Services HKLM SoftwareOCS cert Allows us to check if package deployment is working using regcert example Packages Action File Command line Description -------- ------ ------------------------------------------------------------ ----------------------------------------------------------------------------------------------- RUN \u0026#34;%PROGRAMFILES%RealVNCVNC4winvnc4.exe\u0026#34; -connect HOST Launches a new VNC client to HOST, allowing to watch Desktop as HOST initiated the connection RUN netsh firewall add portopening TCP 5900 VNC Opens 5900 port on TCP in Windows Firewall (XP SP2) (for using with VNC) Please, feel free to contribute with your Registry Keys or command line commands to improve this guide.\nUsing win-get (an apt-get clone for Win32), you can use this big list of supported applications for installation doing win-get sinstall APPNAME Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2006/07/29/ocs-deployment-tips-and-tricks/","summary":"\u003cp\u003ePrior to using the following info for creating own-made packages, let\u0026rsquo;s test if everything is working fine.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve created a NSIS script that writes into registry in a key called \u003ccode\u003eHKLMSOFTWAREOCS\u003c/code\u003e and puts a key named \u0026ldquo;cert\u0026rdquo; with value \u003ccode\u003ecreilla\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf we create a package with action \u0026ldquo;LAUNCH\u0026rdquo;, and attach the \u003ccode\u003eregcert.zip\u003c/code\u003e with command to execute \u003ccode\u003eregcert.exe\u003c/code\u003e, all clients with functional package deployment, will add that key to registry, so we can check, using OCS registry query function for a key named \u0026ldquo;cert\u0026rdquo; into: \u003ccode\u003eHKLM\u003c/code\u003e \u003ccode\u003eSOFTWAREOCS\u003c/code\u003e.\u003c/p\u003e","title":"OCS Deployment Tips and tricks"},{"content":"Introduction OCS Inventory is an excellent piece of GPL Software for getting info from hardware components, and software installed on computers running Windows or UNIX-like operating systems (there are also some unofficial clients for running on other platforms).\nSince New Generation (OCS-NG), a new architecture was implemented: server were contacted using standard TCP/IP connection (Previous versions, used an MDB file to store information, and required a SMB share to inventory computers ), allowing remote computers to connect as well as providing a new web interface for computer administration, and inventory query.\nWithin new features, a new small client (about 64kb), was able to contact inventory server, and download from it the full set of required files (OcsAgent.exe) which was expanded to %SYSTEMDRIVE%ocs-ng, then executed, an inventory sent to server.\nThis approach had a little problem, if you had a small outgoing connection, serving a 700 Kb file was painful\u0026hellip; but at least, despite some minor revision changes, machines were able to upgrade automatically (In many other times, you had to manually redeploy or force with /DEPLOY:#VERNUMBER# the new deployment) when new version was deployed on the server.\nWith RC3, agent for installation uses near double that size, about 1.5 megabytes, but the ability for after first installed, use external servers with a bigger upload bandwidth, eases installation of new versions in computers.\nRelease Candidate 3, and 1.0 OCS-NG RC3 came with important architectural changes, including several major and minor improvements, being these the more important ones:\nNow the Windows client works as a service (for the first time it includes a Windows agent) RC3 includes a component for software/files deployment/distribution The new tool is called package deployment. This feature it is managed using the also improved admin web interface.\nOCS-NG could be setup on different machines hosting each service:\nAn Inventory-receiving machine An Admin web interface A site with information about packages A site with package fragments for deployment or, as I did in my setup, use a Debian Linux machine for doing the four tasks, but I plan to relay the fourth task to other machines, when packages are bigger than expected.\nSetting it up and running I\u0026rsquo;ll assume that you\u0026rsquo;ve running Apache, PHP, and were able to set up OCS using the bundled instructions, so you only have to enable new features for using package deployment.\nFirst of all, we need SSL support in Apache.\nPackage deployment infrastructure, is too much powerful, so it requires SSL access to validate server before trying to download from it, so\u0026hellip; we\u0026rsquo;ll need some SSL certificates for use with our server.\nI like http://www.cacert.org services: they sign your certificates, and provide one certificate aiming to be used with many FOSS projects, because it\u0026rsquo;s free instead of paid certificates like the ones from Thawte or Verisign.\nGetting SSL Certificates First of all, we need to create a private key and a CSR (Certificate Signing Request) which we will send to CACERT for signing (please, note that if you don\u0026rsquo;t have a domain name, will make it impossible to use OCS Package Deployment if your IP is also dynamic, so if that is your case (as was mine too), open an account at No-IP and create an URL-redirect to your machine, you\u0026rsquo;ll have to install an update client, but this will allow you to use certificates) it.\nHaving OpenSSL installed, we will execute (please, double check that questions, specially CN exactly matches ServerName and hostname, for it to work properly after) the following commands:\n#!bash openssl genrsa -out server.key 1024 openssl req -new -key server.key -out server.csr First one, will create a private key called \u0026ldquo;server.key\u0026rdquo;, second one, will create a CSR which we will paste at https://www.cacert.org/account.php?id=10 to get our server certificate signed.\nFor being able to use http://www.cacert.org services, we\u0026rsquo;ll have to create an account and add a domain to it, this is verified sending an email to an account like webmaster, root or so, clicking on the supplied link, will entitle to work in representation of that domain.\nAfter that, Cacert.org will show you a certificate for your server that you\u0026rsquo;ll have to copy to a file named server.crt.\nLet\u0026rsquo;s then download CACERT\u0026rsquo;s root certificate to cacert.pem\nConfiguring apache for using that certificates Next, we\u0026rsquo;ll have to tell apache, to use this certificate for SSL support, in my case, I configured:\n#!apache /etc/apache2/conf.d/ssl: SSLProtocol all SSLOptions +StdEnvVars SSLCipherSuite ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP:+eNULL SSLCACertificateFile /etc/apache2/ssl/cacert.pem SSLCertificateFile /etc/apache2/ssl/server.crt SSLCertificateKeyFile /etc/apache2/ssl/server.key So, I had to put server.crt, server.key and cacert.pem in /etc/apache2/ssl/\nNext one, was to configure a new site that requires SSL to work:\n#!apache /etc/apache2/sites-enabled/001-default: ServerName yourserver.no-ip.org NameVirtualHost \\*:443 ErrorLog /var/log/apache2/errssl.log # Possible values include: debug, info, notice, warn, error, crit, # alert, emerg. LogLevel warn CustomLog /var/log/apache2/ssl.log combined ServerSignature On DocumentRoot /var/www SSLEngine on SSLOptions +StdEnvVars SetEnvIf User-Agent \u0026#34;.\\*MSIE.\\*\u0026#34; nokeepalive ssl-unclean-shutdown After that\u0026hellip; we have to reload Apache configuration and try to connect to https://yourserver.no-ip.org to check if everything is ok.\nWell, if this works, we had the first and harder step done ;)\nCreating a package There are three types of packages: RUN, STORE and LAUNCH.\nEach of them has different behavior, one runs a command, the other downloads a file and stores it on a folder, and the other does a combined thing: downloads a file, unzips it, and then runs a command.\nFor Package creating, we must have write access to Apache\u0026rsquo;s DocumentRoot/download folder, and after creation, copy contents of \u0026ldquo;/download\u0026rdquo; to \u0026ldquo;/\u0026rdquo;, or as I did, gave write access to \u0026ldquo;/var/www\u0026rdquo;, and create (ln -s . download) a symbolic link for download.\nSo\u0026hellip; let\u0026rsquo;s create a first package:\nWe must login into OCS Web interface, and select (first menu option on first yellow icon) package creation We must assign a name for the package, Platform, Protocol and Priority (priority will allow us to decide package execution order in the client, the lower number, the higher priority) If we\u0026rsquo;re going to upload files, we must ZIP it BEFORE, so OCS will unzip on client machine, and then run commands We choose an action, and then, a path (we can use system variables like %SYSTEMDRIVE%, %TEMP%, %USERPROFILE%, %PROGRAMFILES%, etc) to store the file, or command to run We can choose if we want the user to be warned about package execution, and even to allow user to delay execution (useful for service pack deployments, etc) Next step, will allow us to specify fragments (pieces in which the package will be split for allowing better deployment, making use of re-downloading for only failed fragments, etc), as well as checksum for data validity\nYour package, will be created then on /var/www/download/#pkgid#.\nActivating a package Once a package has been defined, we have an \u0026ldquo;info\u0026rdquo; file, describing package actions, and package fragments, we can have them together or split it between different servers, and we will have to specify where is located each piece, before using it on our machines. That process is called \u0026ldquo;Activation\u0026rdquo;.\nWhen we select that option, we have to specify the pkgid, so we use the second menu entry in the package deployment icon, and we\u0026rsquo;ll get a list of packages ready for activation, and then, we select \u0026ldquo;Activate\u0026rdquo; from the one we\u0026rsquo;re interested in.\nOn package activation, we will be asked for two SERVERS (thanks to the development team, specially to Pascal who helped in determining that we require to specify server name, not URL) one with https (for downloading info file) and one with http for downloading fragments (if any).\nAfter sending server names, OCS will check availability of \u0026ldquo;info\u0026rdquo; and fragment files (if any (On RUN packages, there is nothing to download prior to running the commands) and then, package will be activated and ready for next step.\nAffecting a package In this step, we can select a computer in the main view, or do a search using specific criteria, and as a result, apply a package on listed computers.\nWe can affect a package to several computers at once, just to one, and even, have different packages affect to the same computer\u0026hellip;.\nOCS will connect, and execute actions defined in priority order\u0026hellip;\nHow to get client side Package working Packages from client side, are as easy to setup, as having a working OCS Agent Service installed and a file called cacert.pem, which we got from the SSL Creation step\u0026hellip; having them in the OCS Agent folder, and a package affected to a computer, will make computer to download, and do the actions specified. ¿What are the pro\u0026rsquo;s and con\u0026rsquo;s of this method?\nWhen installing OCSAgent, using: OCSAgentSetup.exe /S /SERVER:yourserver.no-ip.org, we have no cacert.pem file copied, so we must copy it by hand, or, as I did, use an install script which does this in one step.\nI\u0026rsquo;ve used NSIS to create a script for doing this:\nFirst, we\u0026rsquo;ll have to create a folder and put in it:\nOcsAgentSetup.exe cacert.pem NSIS Script service.ini (in my case, for accelerating first inventory creation) The file service.ini could look like this:\n#!ini [OCS_SERVICE] TTO_WAIT=10 PROLOG_FREQ=1 OLD_PROLOG_FREQ=1 Miscellaneous= /S /SERVER:yourserver.no-ip.org And NSIS script with:\n#!bat ; Script edited using HM NIS Edit Script Wizard. ; Creator Pablo Iranzo Gómez (Pablo.Iranzo@uv.es) ; Homepage: http://Alufis35.uv.es/~iranzo/ ; License: http://creativecommons.org/licenses/by-sa/2.5/ ; HM NIS Edit Wizard helper defines !define PRODUCT_NAME \u0026#34;OCS\u0026#34; !define PRODUCT_VERSION \u0026#34;RC3\u0026#34; !define PRODUCT_PUBLISHER \u0026#34;Pablo Iranzo Gomez (Pablo.Iranzo@uv.es)\u0026#34; !define PRODUCT_WEB_SITE \u0026#34;http://Alufis35.uv.es/~iranzo/\u0026#34; SetCompressor zlib Name \u0026#34;${PRODUCT_NAME} ${PRODUCT_VERSION}\u0026#34; OutFile \u0026#34;ocs-inst.exe\u0026#34; InstallDir \u0026#34;$TEMP\u0026#34; Icon \u0026#34;${NSISDIR}ContribGraphicsIconsmodern-install.ico\u0026#34; SilentInstall silent InstallDirRegKey HKLM \u0026#34;${PRODUCT_DIR_REGKEY}\u0026#34; \u0026#34;\u0026#34; Section \u0026#34;Principal\u0026#34; SEC01 SetOverwrite on SetOutPath \u0026#34;$TEMP\u0026#34; File \u0026#34;OcsAgentSetup.exe\u0026#34; Exec \u0026#34;$TEMPOcsAgentSetup.exe /S /SERVER:yourserver.no-ip.org\u0026#34; Exec \u0026#34;$PROGRAMFILESOCS Inventory AgentOCSService.exe -stop\u0026#34; SetOutPath \u0026#34;$PROGRAMFILESOCS Inventory Agent\u0026#34; File \u0026#34;cacert.pem\u0026#34; File \u0026#34;service.ini\u0026#34; Exec \u0026#34;$PROGRAMFILESOCS Inventory AgentOCSService.exe -start\u0026#34; Exec \u0026#34;$PROGRAMFILESOCS Inventory AgentOCSInventory.exe /SERVER:yourserver.no-ip.org /DEBUG\u0026#34; SectionEnd Section -Post SectionEnd` This script, when compiled, will create a ocs-inst.exe file, with all files needed to be packed in it, when executed, will:\nsilently run extract OCSAgentSetup install it using silent install then, stop service output cacert.pem certificate in OCS Service folder replace service.ini for faster inventory, and then, start service Force DEBUG and hand-started inventory This will leave us with a working OCS Agent Setup, with a valid certificate for authenticating against our deployment server, and ready for creating more packages.\nI hope this document is good for you to test this excellent software.\nThanks (again) to the OCS Developing team (specially to Pascal Danek) for creating such a nice program, and helping in diagnosing problems found and setup procedures for correctly using it.\nThanks to Pablo Chamorro for reviewing this article too ;)\nHave a look at OCS Deployment Tips and tricks to get ideas on how to use package deployment Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2006/07/27/ocs-inventory-package-deployment/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eOCS Inventory is an excellent piece of GPL Software for getting info from hardware components, and software installed on computers running Windows or UNIX-like operating systems (there are also some unofficial clients for running on other platforms).\u003c/p\u003e\n\u003cp\u003eSince New Generation (\u003ccode\u003eOCS-NG\u003c/code\u003e), a new architecture was implemented: server were contacted using standard TCP/IP connection (Previous versions, used an MDB file to store information, and required a SMB share to inventory computers ), allowing remote computers to connect as well as providing a new web interface for computer administration, and inventory query.\u003c/p\u003e","title":"OCS Inventory Package deployment"},{"content":"BIOS (Basic Input Output System) from PC\u0026rsquo;s report errors in the hardware at boot with a code of beeps, that can help diagnose the issue in case there\u0026rsquo;s no image being displayed.\nError code reported by BIOS during POST (Power On Self Test):\nFatal errors # of beeps Meaning 1 DRAM refresh error 2 640Kb base RAM error 4 system timer error 5 CPU error 6 Keyboard Gate A20 error 7 Virtual mode exception error 9 BIOS-ROM checksum error Non-Fatal errors # of beeps Meaning 3 Memory test failure conventional and extended 8 Monitor error with tracing vertical and horizontal Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2004/04/12/error-depending-of-number-of-beeps-in-bios/","summary":"\u003cp\u003eBIOS (Basic Input Output System) from PC\u0026rsquo;s report errors in the hardware at boot with a code of beeps, that can help diagnose the issue in case there\u0026rsquo;s no image being displayed.\u003c/p\u003e\n\u003cp\u003eError code reported by BIOS during POST (Power On Self Test):\u003c/p\u003e\n\u003ch2 id=\"fatal-errors\"\u003eFatal errors\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e# of beeps\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003eDRAM refresh error\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e640Kb base RAM error\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e4\u003c/td\u003e\n          \u003ctd\u003esystem timer error\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e5\u003c/td\u003e\n          \u003ctd\u003eCPU error\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e6\u003c/td\u003e\n          \u003ctd\u003eKeyboard Gate A20 error\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e7\u003c/td\u003e\n          \u003ctd\u003eVirtual mode exception error\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e9\u003c/td\u003e\n          \u003ctd\u003eBIOS-ROM checksum error\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"non-fatal-errors\"\u003eNon-Fatal errors\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e# of beeps\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003eMemory test failure conventional and extended\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e8\u003c/td\u003e\n          \u003ctd\u003eMonitor error with tracing vertical and horizontal\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\nEnjoy! (and if you do, you can\n\u003ca href=\"https://ko-fi.com/I2I4KDA0V\" target=\"_blank\"\u003e\n  \u003cimg src=\"https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png\" height='36' style='border:0px;height:36px;vertical-align:middle;display:inline-block;' border=\"0\"\u003e\n\u003c/img\u003e\nBuy Me a Coffee\n\u003c/a\u003e\n)\n\u003c/p\u003e","title":"Error depending of number of beeps in BIOS"},{"content":"Olivetti Echos Family (Tested on my P75, but obtained from the Internet) (Running now at 166 MHz)\nJumper Settings Processor Speed 1100 75 MHz 0000 90 MHz 0100 100 MHz 1110 100 MHz 0010 120 MHz 1111 120 MHz 0110 133 MHz 0011 150 MHz 1101 150 MHz 0001 166 MHz 0111 166 MHz Switches 1,2:\nSwitch 1 Switch 2 Bus OFF OFF 60 OFF ON 66.6 ON OFF 20 ON ON 50 Switch 3,4:\nSwitch 3 Switch 4 Multiplier OFF OFF 1.5x OFF ON 3x ON OFF 2x ON ON 2.5x You\u0026rsquo;ll find those jumpers under the sound card, remove the ring from the outside part of the notebook, then remove the plastic pieces that keep keyboard in its position, then remove a screw that is retaining an aluminum piece (acting as disipator for the microprocessor) . Then, you\u0026rsquo;ll see the sound card piece in the left side of the notebook, lift it up and you\u0026rsquo;ll see a four switch piece\u0026hellip; configure it as showed in the table before and you\u0026rsquo;ll set the new processor speed.\nI don\u0026rsquo;t know if this will damage your computer, so I give you no warranty, if you proceed, you\u0026rsquo;re doing it at your own risk, all that I can say is that this worked for me. I\u0026rsquo;ve just replaced P75 for a P120 processor, I did this without any \u0026ldquo;over-clocking\u0026rdquo; intention, just using real processor.\n¡Good Luck!\nMine stopped working in August, the screen gets white and only Fn-F11 and Fn-F12 seems to work, hard disk is working properly, CPU too, so it seems a problem with the mainboard. I\u0026rsquo;ve found several cases on Internet regarding this problem, and some describe it as an oscillator problem that makes it boot sometimes and other no.\nFor me, it\u0026rsquo;s always not working, I try from time to time because I keep hope on its awakening\u0026hellip;\nUPDATE: 1st October 2006, I\u0026rsquo;ve tried to boot it up for trying to update an USR2450 AP card I use with LinuxAP , and it booted! it said that there was a problem with the RTC clock, but after configuring bios properly, I have it on\u0026hellip; when I\u0026rsquo;m finished with updating the PCMCIA card (newer notebook has no PCMCIA slot) I\u0026rsquo;ll try to shutdown and re-test\u0026hellip;\nSECOND UPDATE: After moving the notebook for connecting a parallel cable from my zip drive, computer shutdown, and now, has the video problem again\u0026hellip; bad luck :'(\nI\u0026rsquo;ve the following hardware if someone interested in buying it:\nOlivetti Echos P75 system:\nMotherboard DSTN Screen Spanish Keyboard Sound card Floppy drive Used battery Power adapter THIRD UPDATE: 17th November 2007: At How to create a digital frame says that the problem with the blinking cursor is related with battery being exhausted (as a visitor pointed too). Will try :) Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2004/04/06/jumpers-olivetti-echos-p75/","summary":"\u003cp\u003eOlivetti Echos Family (Tested on my P75, but obtained from the Internet)\n(Running now at 166 MHz)\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003eJumper Settings\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003eProcessor Speed\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e1100\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e75 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e0000\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e90 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e0100\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e100 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e1110\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e100 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e0010\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e120 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e1111\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e120 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e0110\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e133 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e0011\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e150 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e1101\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e150 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e0001\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e166 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e0111\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e166 MHz\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSwitches 1,2:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003eSwitch 1\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003eSwitch 2\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003eBus\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003ccode\u003eOFF\u003c/code\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003ccode\u003eOFF\u003c/code\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e60\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003ccode\u003eOFF\u003c/code\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003ccode\u003eON\u003c/code\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e66.6\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003ccode\u003eON\u003c/code\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003ccode\u003eOFF\u003c/code\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e20\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003ccode\u003eON\u003c/code\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003ccode\u003eON\u003c/code\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e50\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSwitch 3,4:\u003c/p\u003e","title":"Jumpers Olivetti Echos P75"},{"content":"General Purpose The idea on writing VPNS and the structure it follows was the problem that we had into easily configure many VPN\u0026rsquo;s for use with the wireless project interconnection (Valencia Wireless http://www.valenciawireless.org).\nWhen having no chance to join networks using just wireless links, we needed to provide some kind of transparent link from one WiFi node to another. To do so, we decided to use the vpnd daemon to establish links between our networks for allowing transparent traffic flow.\nThe vpnd it\u0026rsquo;s great because it\u0026rsquo;s very easy to configure and works fine, but has a little big problem\u0026hellip; it doesn\u0026rsquo;t allow to use DNS names for establishing VPN\u0026rsquo;s, it just allows to use static IP\u0026rsquo;s. Probably that wouldn\u0026rsquo;t be a problem for big business, universities, etc but as wireless projects are common between particulars, and at least in Spain we usually have dynamic IP\u0026rsquo;s, it made our tunnels to be \u0026ldquo;alive\u0026rdquo; until either our provider or the remote provider decides to change the IP\u0026hellip;\nAll of us were using Dynamic DNS Services from different providers (I use No-IP (http://www.no-ip.org), so if you decide to sign it with them, please use the following link No-Ip http://www.no-ip.com), other people at Valencia Wireless uses DynDNS http://www.dyndns.org, if in doubt, search for Dynamic DNS to get an idea on what you can use\u0026hellip;\nIn brief\u0026hellip; we had DNS names that get dynamically resolved to our current IP\u0026rsquo;s, but vpnd was unable to deal with those names\u0026hellip; so I wrote VPNS :-D\n(please, if you use it, send me an email to let me know how much people is using it\u0026hellip; thanks)\nRequirements VPNS, it\u0026rsquo;s just a bunch of configuration files and three scripts that requires the presence of:\nPerl (for the scripts) Bash or compatible (for the startup script) vpnd (for the VPN establishing) Ping (for resolving DNS names to IP\u0026rsquo;s) CAUTION\nAs this is just a gziped-tar file by the moment, you\u0026rsquo;ll have to check that the required software is installed before trying to use it.\nThis script has been created for a Debian GNU/Linux distribution, but as far as I can remember it\u0026rsquo;s compatible with the schema used on Red Hat, SuSE, etc\u0026hellip; but check before\nThe way it works\u0026hellip; As vpnd requires one key file (vpnd.key), and a configuration file (vpnd.conf) for each connection, the idea was to split different files for each node.\nWe\u0026rsquo;ll require then one key file and one configuration file for each node to be able to spawn a vpnd daemon with those data.\nAs each vpnd will require both static origin and target IP\u0026rsquo;s there would be some things that would be static in the configuration (other parameters) and others that would need to get rewritten (target, and start addresses).\nConfiguration files The configuration get\u0026rsquo;s deployed on two directories: /etc/vpnd/ for the local host configuration (your host) and /etc/vpnd/hosts/* for the remote hosts configuration.\nThere are also some rc?.d directories with symbolic links to VPNS to start/stop the daemon at different runlevels, it\u0026rsquo;s a good idea to copy them too, but as the main purpose of this script is to periodically test it, it should be included at crontab (as well as rc?.d to initial startup).\n4.1 Local Configuration Those files specify the configuration for the local host that will be used by all the other hosts.\n/etc/vpnd/master.resolv : In this file you\u0026rsquo;ll need to put the Dynamic DNS name of your machine (in my case \u0026ldquo;lacreu.no-ip.org\u0026rdquo;), so the update script will get your IP from it to be able to establish tunnels both if your IP changes or just if remote IP changes. /etc/vpnd/master.ip : This file is created automatically with the results of resolving your Dynamic DNS to an IP to allow comparison between your last IP and your current to check if you need to relaunch VPN\u0026rsquo;s (remember that the worst case is both local and remote IP\u0026rsquo;s getting changed). /etc/vpnd/master.restart : This file is created when local IP has changed, specifying a total vpnd restart for all VPN\u0026rsquo;s 4.2 Host configuration (/etc/vpnd/hosts/$HOST) Each file in this subdirectory specifies a VPN to be launched and defines the name that the configuration files would have appended.\nWe\u0026rsquo;ll create a folder for each remote host with a descriptive name that would be use by the scripts.\nCAUTION\nFor creating the files you\u0026rsquo;ll have to use \u0026ldquo;echo \u0026ldquo;client\u0026rdquo; \u0026gt; mode\u0026rdquo; or similar (for port, resolv, mode, etc), there have been some problems with files not created this way (and my perl knowledge doesn\u0026rsquo;t allow me to do anything better).\nInto this subdirectory there would be the following config files:\n4.2.1 Static files vpnd.key : Cipher key that would be used for the connection with the remote host. It gets created by vpnd or copied from remote host. mode : Mode that we use to get connected to remote host (server or client) port : Port at which the connection gets established resolv : Dynamic DNS resolver name for the target host (used for resolving it\u0026rsquo;s IP) master : Master configuration file for HOST (the config that it\u0026rsquo;s static) 4.2.2 Dynamic files vpnd.conf : Configuration file for HOST, it gets created automatically when running /etc/vpnd/update.pl script restart : This file gets created by the \u0026ldquo;compare.pl\u0026rdquo; script to indicate that VPNS should restart this host VPN. This file is created if the IP has changed since last launch 4.2.3 Scripts /etc/vpnd/update.pl : Script to merge configuration files for host, resolve IP and then output a vpnd.conf file for that host /etc/vpnd/compare.pl : Script to compare IP\u0026rsquo;s between recorded one and current for preparing vpnd restart /etc/init.d/vpns : Script to start, stop, restart or restart-if-needed the VPNS based on hosts definitions Recommendations As probably IP\u0026rsquo;s will change, you\u0026rsquo;ll need to put a crontab sentence for checking of updated IP\u0026rsquo;s, to do so, put /etc/init.d/vpns restart-if-needed in your crontab for allowing the tunnels to be recreated at every change.\nSample crontab line would be:\n#!bash 0-59/5 * * * * root /etc/init.d/vpns restart-if-needed This way, every five minutes, the script would be run and will restart VPNS that needed to do so.\nAt every run, compare.pl will check new IP addresses, and will mark the service to be restarted, it will create master.ip and \u0026ldquo;host/ip\u0026rdquo; file.\nupdate.pl will dump a vpnd.conf file containing the merge of \u0026ldquo;host/master\u0026rdquo; and the Dynamic DNS for both local host and remote host configured as \u0026ldquo;host/mode\u0026rdquo; says using the \u0026ldquo;host/port\u0026rdquo; port to establish connection and vpnd.key file in each \u0026ldquo;host/\u0026rdquo; folder.\nChangelog Version 0.43 Added a check (chop $mode) with the mode definition to fix problems in systems that always configured mode as server (default check, because the if sentence at update.pl searched for \u0026ldquo;client\u0026rdquo; and sometimes it was not well compared)\nThanks to Hawkmoon\nVersion 0.42 Change in the compare.pl script for the case in which the changed ip was the local one to restart all VPN daemons.\nBefore this version, when local IP changed, only the first-checked VPN will be restarted as there was no file specifying a total restart.\nVersion 0.41 Little changes in configuration files an scripts to make it easier to set up, please, check your configuration files to use new behaviour\npidfile now gets automatically defined to match VPNS kill procedure, so it shouldn\u0026rsquo;t be in \u0026ldquo;master\u0026rdquo; file. Version 0.4 Now, added localhost IP resolution to check if just origin or end (or both) changed their IP, so if either one of them changes, the VPN get\u0026rsquo;s marked as restart-needed.\nVersion 0.3 Each host is now restarted only if needed. (added VPNS restart-if-needed command) for not having to stop/relaunch every VPN at every time.\nAdded compare.pl script to compare IP\u0026rsquo;s and to let inform the VPNS that needs to restart it.\nVersion 0.2 Configuration files moved from /etc/vpnd/ to /etc/vpnd/hosts/*, so each host has it\u0026rsquo;s configuration files into one subdirectory, allowing to tidy the things a bit. Old files were named vpnd-HOST.conf, vpnd-HOST.key, HOST.resolv,etc. They got renamed and moved to the actual location.\nVersion 0.1 First version: at every restart of VPNS new configuration files were created for each host, and then the vpnd were restarted with the configuration files\nSample configuration files Here you will see a sample files used at my host, the other files not dumped here are created automatically by the scripts or you must create them manually (for example the vpnd.key).\n7.1 Local 7.1.1 master.resolv merak.no-ip.org\n7.2 Remote Host 7.2.1 mode server\n7.2.2 port 2010\n7.2.3 resolv oceano.dyndns.org\n7.2.4 master key value local 172.16.97.78 remote 172.16.97.77 autoroute keepalive 10 noanswer 3 keyfile vpnd.key` randomdev /dev/urandom mtu 1600 7.3 Sample run for Oceano (resulting vpnd.conf) key value local 172.16.97.78 remote 172.16.97.77 autoroute keepalive 10 noanswer 3 keyfile vpnd.key pidfile /var/run/vpnd:hawkmoon.pid randomdev /dev/urandom mtu 1600 client 81.202.20.245 2010 server 81.202.117.88 2010 mode server FAQ 8.1 Problems 8.1.1 Gentoo (Thanks to Hawkmoon) It seems that the script as provided within this package doesn\u0026rsquo;t work fine with Gentoo, please, specify the full path to the vpnd.key file in each \u0026ldquo;master\u0026rdquo; configuration file, and be sure to edit \u0026ldquo;VPNS\u0026rdquo; to point to your \u0026ldquo;vpnd\u0026rdquo; executable.\nCredits Copyright (c) 2003 Pablo Iranzo Gómez (Pablo.Iranzo@uv.es) https://Alufis35.uv.es/~iranzo/\nYou\u0026rsquo;re given permission to copy, distribute and/or modify this document under the terms of the GNU General Public License Version 2 or higher published by the Free Software Foundation\nPlease, if you use this program, email me to know where it gets to and if it\u0026rsquo;s used.\nAbout This document has been created using the LyX Editor and compiled with under Debian GNU/Linux, and then converted to the format you\u0026rsquo;re viewing.\nFile translated from TeX by T~T~H, version 3.67. On 9 Mar 2007, 21:50. Enjoy! (and if you do, you can Buy Me a Coffee ) ","permalink":"https://iranzo.io/blog/2003/03/05/vpns-multiple-vpns-launcher/","summary":"\u003ch2 id=\"general-purpose\"\u003eGeneral Purpose\u003c/h2\u003e\n\u003cp\u003eThe idea on writing VPNS and the structure it follows was the problem that\nwe had into easily configure many VPN\u0026rsquo;s for use with the wireless project\ninterconnection (Valencia Wireless\n\u003ca href=\"http://www.valenciawireless.org/\"\u003e\u003ccode\u003ehttp://www.valenciawireless.org\u003c/code\u003e\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eWhen having no chance to join networks using just wireless links, we needed\nto provide some kind of transparent link from one WiFi node to another. To\ndo so, we decided to use the vpnd daemon to establish links between our\nnetworks for allowing transparent traffic flow.\u003c/p\u003e","title":"VPNS - Multiple VPN's Launcher"},{"content":"Those are some pics I took during December in 2002 around Camelle and Costa da Morte area in North of Spain after the Prestige oil spill. ","permalink":"https://iranzo.io/blog/2002/02/25/camelle-costa-da-morte-2002/","summary":"\u003cp\u003eThose are some pics I took during December in 2002 around Camelle and Costa da Morte area in North of Spain after the Prestige oil spill.\n\n\n\n\u003cdiv class=\"gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition\" itemscope itemtype=\"http://schema.org/ImageGallery\"\u003e\n\t  \n\n\u003clink rel=\"stylesheet\" href=/css/hugo-easy-gallery.css /\u003e\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/WpXrXUFt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/WpXrXUFt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/WpXrXUF.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/CkKAAQvt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/CkKAAQvt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/CkKAAQv.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/x8mgooYt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/x8mgooYt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/x8mgooY.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/ZCGqEIkt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/ZCGqEIkt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/ZCGqEIk.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/z5A2NHDt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/z5A2NHDt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/z5A2NHD.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/YpB4CPVt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/YpB4CPVt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/YpB4CPV.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/Rbex877t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/Rbex877t.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/Rbex877.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/Q9ck1fRt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/Q9ck1fRt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/Q9ck1fR.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/Qiat3F8t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/Qiat3F8t.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/Qiat3F8.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/HJpjYMzt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/HJpjYMzt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/HJpjYMz.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/XT6aEBZt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/XT6aEBZt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/XT6aEBZ.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/pHpeouIt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/pHpeouIt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/pHpeouI.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/nautu1rt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/nautu1rt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/nautu1r.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/9qh0sXkt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/9qh0sXkt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/9qh0sXk.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/YztdWjUt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/YztdWjUt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/YztdWjU.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/fOfiPQ3t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/fOfiPQ3t.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/fOfiPQ3.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/aF8j6ORt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/aF8j6ORt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/aF8j6OR.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/IkcQ70Ft.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/IkcQ70Ft.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/IkcQ70F.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/GkwY1bet.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/GkwY1bet.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/GkwY1be.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/AXbSkIht.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/AXbSkIht.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/AXbSkIh.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/zshBJdgt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/zshBJdgt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/zshBJdg.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/WBTA2yAt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/WBTA2yAt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/WBTA2yA.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/t5ATjuvt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/t5ATjuvt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/t5ATjuv.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/G2k4NrBt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/G2k4NrBt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/G2k4NrB.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/MjQI4gat.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/MjQI4gat.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/MjQI4ga.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/rAu1TW3t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/rAu1TW3t.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/rAu1TW3.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/fcvxI4kt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/fcvxI4kt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/fcvxI4k.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/E3rzZrPt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/E3rzZrPt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/E3rzZrP.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/whXjW0Pt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/whXjW0Pt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/whXjW0P.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/R1iMY3Kt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/R1iMY3Kt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/R1iMY3K.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/0VyhRzEt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/0VyhRzEt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/0VyhRzE.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/87YaYR4t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/87YaYR4t.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/87YaYR4.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/rWGEonOt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/rWGEonOt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/rWGEonO.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/8AOrgdgt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/8AOrgdgt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/8AOrgdg.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/XwjQArWt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/XwjQArWt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/XwjQArW.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/NOeptEJt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/NOeptEJt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/NOeptEJ.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/LlYEUqht.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/LlYEUqht.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/LlYEUqh.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/mrO6MOdt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/mrO6MOdt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/mrO6MOd.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/imAxGH1t.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/imAxGH1t.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/imAxGH1.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003cdiv class=\"box\" \u003e\n  \u003cfigure  itemprop=\"associatedMedia\" itemscope itemtype=\"http://schema.org/ImageObject\"\u003e\n    \u003cdiv class=\"img\" style=\"background-image: url('https://i.imgur.com/nToZQ8Wt.jpg');\"\u003e\n      \u003cimg itemprop=\"thumbnail\" src=\"https://i.imgur.com/nToZQ8Wt.jpg\" /\u003e\n    \u003c/div\u003e\n    \u003ca href=\"https://i.imgur.com/nToZQ8W.jpg.jpg\" itemprop=\"contentUrl\"\u003e\u003c/a\u003e\n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\u003c/div\u003e\n\u003c/p\u003e","title":"Camelle - Costa da Morte 2002"}]