<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>KMM 1.1 Scale testing | Pablo Iranzo Gómez blog</title>
<meta name=keywords content="KMM,Scale,Kernel Module Management"><meta name=description content="
    Attention
    First published at https://cloud.redhat.com/blog/scale-testing-kernel-module-management


Introduction
Kernel Module Management (KMM) Operator manages, builds, signs and deploys out-of-tree kernel modules and device plugins on OpenShift Container Platform clusters.
KMM adds, for the HUB/Spoke scenario, a new ManagedClusterModule which describes an out-of-tree kernel module and its associated device plugin. You can use ManagedClusterModule resources to configure how to load the module, define ModuleLoader images for kernel versions, and include instructions for building and signing modules for specific kernel versions."><meta name=author content="Pablo Iranzo Gómez, Enrique Belarte"><link rel=canonical href=https://iranzo.io/blog/2023/06/21/kmm-1.1-scale-testing/><meta name=google-site-verification content="Bk4Z5ucHLyPXqlZlj5LzANpYBBSvxqBW4E8i-Kwf-bQ"><meta name=yandex-verification content="993ede96cdfbee95"><link crossorigin=anonymous href=../../../../../assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://iranzo.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://iranzo.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://iranzo.io/favicon-32x32.png><link rel=apple-touch-icon href=https://iranzo.io/apple-touch-icon.png><link rel=mask-icon href=https://iranzo.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://iranzo.io/blog/2023/06/21/kmm-1.1-scale-testing/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZL9P943TP1"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZL9P943TP1")}</script><meta property="og:url" content="https://iranzo.io/blog/2023/06/21/kmm-1.1-scale-testing/"><meta property="og:site_name" content="Pablo Iranzo Gómez blog"><meta property="og:title" content="KMM 1.1 Scale testing"><meta property="og:description" content=" Attention
First published at https://cloud.redhat.com/blog/scale-testing-kernel-module-management
Introduction Kernel Module Management (KMM) Operator manages, builds, signs and deploys out-of-tree kernel modules and device plugins on OpenShift Container Platform clusters.
KMM adds, for the HUB/Spoke scenario, a new ManagedClusterModule which describes an out-of-tree kernel module and its associated device plugin. You can use ManagedClusterModule resources to configure how to load the module, define ModuleLoader images for kernel versions, and include instructions for building and signing modules for specific kernel versions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-06-21T00:00:00+02:00"><meta property="article:modified_time" content="2024-04-09T15:05:24+00:00"><meta property="article:tag" content="KMM"><meta property="article:tag" content="Scale"><meta property="article:tag" content="Kernel Module Management"><meta property="og:image" content="https://iranzo.io/mugshot.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://iranzo.io/mugshot.png"><meta name=twitter:title content="KMM 1.1 Scale testing"><meta name=twitter:description content="
    Attention
    First published at https://cloud.redhat.com/blog/scale-testing-kernel-module-management


Introduction
Kernel Module Management (KMM) Operator manages, builds, signs and deploys out-of-tree kernel modules and device plugins on OpenShift Container Platform clusters.
KMM adds, for the HUB/Spoke scenario, a new ManagedClusterModule which describes an out-of-tree kernel module and its associated device plugin. You can use ManagedClusterModule resources to configure how to load the module, define ModuleLoader images for kernel versions, and include instructions for building and signing modules for specific kernel versions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://iranzo.io/blog/"},{"@type":"ListItem","position":2,"name":"KMM 1.1 Scale testing","item":"https://iranzo.io/blog/2023/06/21/kmm-1.1-scale-testing/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"KMM 1.1 Scale testing","name":"KMM 1.1 Scale testing","description":" Attention\nFirst published at https://cloud.redhat.com/blog/scale-testing-kernel-module-management\nIntroduction Kernel Module Management (KMM) Operator manages, builds, signs and deploys out-of-tree kernel modules and device plugins on OpenShift Container Platform clusters.\nKMM adds, for the HUB/Spoke scenario, a new ManagedClusterModule which describes an out-of-tree kernel module and its associated device plugin. You can use ManagedClusterModule resources to configure how to load the module, define ModuleLoader images for kernel versions, and include instructions for building and signing modules for specific kernel versions.\n","keywords":["KMM","Scale","Kernel Module Management"],"articleBody":" Attention\nFirst published at https://cloud.redhat.com/blog/scale-testing-kernel-module-management\nIntroduction Kernel Module Management (KMM) Operator manages, builds, signs and deploys out-of-tree kernel modules and device plugins on OpenShift Container Platform clusters.\nKMM adds, for the HUB/Spoke scenario, a new ManagedClusterModule which describes an out-of-tree kernel module and its associated device plugin. You can use ManagedClusterModule resources to configure how to load the module, define ModuleLoader images for kernel versions, and include instructions for building and signing modules for specific kernel versions.\nKMM is designed to accommodate multiple kernel versions at once for any kernel module, allowing for seamless node upgrades and reduced application downtime.\nFor more information about it, refer to the product documentation available at Kernel Module Management Operator | Specialized hardware and driver enablement | OpenShift Container Platform 4.12\nDon’t forget that also KMM is a community project at kubernetes-sigs/kernel-module-management and that it can be tested on upstream Kubernetes and has a slack community channel at Kubernetes Slack.\nVocabulary In order to make it easier, we’re going to use some acronyms and products along this text, find here the most common ones:\nTerm Definition KMM Kernel Module Management ACM Advance Cluster Management OpenShift Red Hat’s Kubernetes-based product HUB Central management Cluster that via ACM manages some Spokes Spoke Cluster managed via ACM from a management cluster referred to as Hub SNO Single-Node OpenShift CRD Custom Resource Definition EDGE Relevant to Telco 5G and other use cases, refers to systems that are placed close to the end user making use of the services to get better performance OOT Out-of-tree, referred to Kernel Module The test goal One of the new features coming in KMM 1.1 is the ability to work in Hub-Spoke architectures, by leveraging Advanced Cluster Management capabilities, deploying KMM in this hub-spoke architecture is like a walk in the park.\nThis kind of setup is very common at the EDGE, where data centers have a more resourceful set of servers, while EDGE devices are more resource-constrained, and everything that can be saved, can be used to provide a better experience to the closest users.\nIn this mode, KMM is able to build new kernel drivers for specific releases on the hub cluster, and then, deliver the built images to the spokes clusters, which, using fewer resources can still benefit from hardware enablement which automatically gets updated for each kernel when a newer OpenShift version image gets released.\nThe KMM team wanted to perform these tests and grab metrics on the behavior in a large-scale environment that allowed them to evaluate the action points on the HUB-Spoke scenario.\nThe goal for this test is to reach 1000 nodes deployed with the KMM module and monitor the actual resource utilization for the whole process.\nTest environment We reached out to the ScaleLab team at https://scalelab.redhat.com/ to explain our use case and to request a loan of machines we could use for our testing.\nThe team provided 69 systems in total, all equal, and with the following specifications:\nDell R650 with 512Gb of RAM, 3TB NVME, 21.8Tb SSD + 1 447GB SSD, powered by the Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz processor reported as 112 CPU’s (dual processor, 56 cores, 112 threads). In order to set up the environment, we used 4 hosts for base infrastructure, designed to become:\n1 Bastion for deploying other hosts and interacting with the environment (Ansible playbook execution, mirror registry, etc.). 3 nodes for creating an OpenShift Baremetal Compact cluster that will hold the hub cluster. The remaining 65 hosts were configured as hypervisors for running virtual machines with KVM to be used as Single Node OpenShift (SNO) clusters.\nNode Hardware CPU RAM(GiB) Disk(GiB) Count Bastion Dell R650 112 512 446 SSD / 2x1.8TB SSD / 2.9TB NVMe 1 Hub BM Cluster Dell R650 112 512 446 SSD / 2x1.8TB SSD / 2.9TB NVMe 3 SNO HV Dell R650 112 512 446 SSD / 2x1.8TB SSD / 2.9TB NVMe 65 SNO VM Libvirt KVM 8 18 120 1755 (max) It might sound easy to achieve, but in reality, there are several things to take into account in order to set up the whole environment:\nConfigure all the relevant hosts and get access to them using Ansible. Enable the required packages and services for virtualization. Define the network cards for the VM’s to be interconnected and able to reach the hub. Have proper sizing and placing of the VM’s so that the faster hard drives are used for holding the VM disks and avoid extra workload because of VM density causing disk pressure on the same drive. Most of this logic is already present in the scripts at https://github.com/redhat-performance/jetlag/ repository, which is pretty tied to the SCALELAB environment that we were going to use for this setup to prepare and configure the relevant resources for this task.\nIn total we got 27 VMs per host, getting to a total of 1755 potential SNOs. Note the word potential here… we’re setting up the infrastructure to mimic real hardware:\nRemotely power on and off the VMs (using sushy-tools to interact with libvirt). KVM for running the VMs. Etc. Each SNO is configured with 8 VCPU and 18Gb of RAM, which is in line with the bare minimum requirements to get OpenShift deployed.\nFor example, we had to alter the limits for the KMM deployment in order to allow it to use more memory during the module build, which was initially limited to 128Mb of RAM and was not enough for the compilation of the module.\nWe were pretty lucky and after the whole process of deployment in this restricted scenario, configuration, ACM deployment, and provisioning of the clusters, only one cluster failed ‘sno01412’, out of 1755 SNOs in total.\nNote that some other spokes will be discarded in the next paragraphs.\nFor this setup, and to avoid external issues, we decided to do the installation following the disconnected IPv6 approach, that is, the bastion is running an internal registry which is later configured in all clusters via a CatalogSource:\napiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: rh-du-operators namespace: openshift-marketplace spec: displayName: disconnected-redhat-operators image: d20-h01-000-r650:5000/olm-mirror/redhat-operator-index:v4.12 publisher: Red Hat sourceType: grpc updateStrategy: registryPoll: interval: 1h This process is performed automatically as part of the scripts used to deploy the environment, but we wanted to validate and fix some hosts that failed to apply the policy that automatically was configuring it.\nIn order to prepare the setup, we also had to mirror into our registry the KMM Operator, and for the reporting, we also added the Grafana operator.\nAs a side issue, we ran out of disk space in the bastion, as we were only using the 500 GB SSD, so we’ve noted this to automatically set additional space on /var/lib/containers on the bastion using extra disks.\nAlso, as we switched from IPv4 to IPv6, we were required to perform some manual cleanup of leftovers that could have caused some of the instabilities we saw in previous attempts.\nThe Grafana instance OpenShift already includes a Grafana dashboard, together with ACM, that can show some information about the bare-metal cluster we will use for management.\nHowever, in order to be able to customize the dashboard, we are required to install a custom instance. This custom instance is what we’ll show in the next screenshots to highlight the behavior of the cluster and the numbers obtained.\nOur results As a briefing, we’ve 1754 valid possible SNOs (deployed and in operation).\nOut of those, 6 spokes were removed, as they failed in different steps (test upgrade of OpenShift, operators in a non-working state, etc).\nSo 1748, is the real number of spokes available for deploying KMM\nKMM installation For deploying KMM we used the operator, to get it added on the Hub.\nUsing ACM, we defined a policy to also add it to the managed clusters, so that’s what we’ll be seeing in the next graphs.\nFirst, the KMM Hub installation started at 11:42, and we can see some data on the following graph:\nAs we started the spokes installation in the period between 12:17 and 12:23 we can also see that after the initial setup of the hub and some activity during the spokes installation, the usage of resources is more or less steady, we can see a bump to a new level around 13:30, but this was once all the activity for deployment has finished.\nWe can compare this pattern as well with the SNO’s resources usage:\nThe SNOs were already active but without too much load in RAM or CPU, as a big part of the installation is just deploying the required components in the clusters.\nIf we focus on the average graphs to avoid the spikes of regular activity we can see some patterns:\nAnd for the spokes:\nIn general, Hub has been unaffected, and in the SNOs, we have a bit more activity in average and RAM usage, but if we check the numbers it’s around 200Mb of RAM usage in difference, so not a big deal in terms of resource consumption.\nIn terms of KMM controllers, we can see it really clearly here:\nThe number of KMM controllers increased in a really short period of time and got to the total number of operative SNOs\nModule installation Now that KMM has been installed, we need to actually deploy a module, this will cause KMM to compile it, and prepare the relevant images that the Spokes will consume.\nThe module used for this test is a really simple module called kmm-kmod which is part of the KMM project. It just prints a “Hello World” and a “Goodbye World” message to the kernel log but it is suitable for testing building and loading kernel modules with KMM like any other ‘production’ module would be.\nIn our case, we first got the module compiled and installed prior to our testing so that we can compare the workload increase when the module is already prepared in the hub and ready for distribution.\nIn this case, the container that was already working in the HUB started to be deployed and we can check the count change here:\nAround 13:30, all SNOs got the module container up and running and repeated the graphs provided earlier for the whole period, we can see that hub increased memory usage\nAnd for the spokes:\nAs we can see that once KMM was installed, a bump in the memory (under 200Mb) happened, but no appreciable change once the module was loaded.\nNote that in this situation, the hub creates the module (compilation, etc), builds the image, and then ACM does the deployment to the spokes.\nFrom the numbers, we see that 100% of the operative SNOs deployed the KMM controller and loaded the module within a really short timeframe with no noticeable impact.\nModule upgrade One of the tests we wanted to perform was to get SNOs to do a kernel upgrade, in this way, we can test KMM by doing the new module compilation itself and then, delivering the updated kernel module to the spoke clusters.\nThis, of course, has an implication… a way to get a newer kernel on the SNO nodes… is by actually upgrading OpenShift itself.\nThis, which might sound easy, means having to mirror the required images for the newer OpenShift release, apply an ICSP to all the SNOs, as well as adding the signatures for validating the new image, and of course, launch the upgrade itself.\nThe Hub started the upgrade at 16:20, moving from a starting OpenShift version 4.12.3 to the final version 4.12.6 which was previously mirrored, with the relevant signatures added, etc.\nThe upgrade took around 90 minutes in total for the Hub, and once the Hub moved to 4.12.6, it recompiled the Kernel module.\nIn our tests, we tested manually on the first SNO (sno0001), and once validated, this was launched for all the remaining and active nodes.\nThe timing was the following:\n18:22 First SNO (sno0001) and others start applying YAML for version signatures and for ICSP (just missing the upgrade itself). 18:24 script to launch all the upgrades in parallel started. 19:38: 1500 updated, 249 in progress. 19:49: 4 nodes failed the OpenShift Upgrade. 22:51: all SNOs, including the failing ones, are upgraded to 4.12.6. Out of the total count, 4 nodes didn’t come back (API not responding), a manual reboot of the SNO’s got them into API responding, and apparently good progress towards finishing the upgrade to the 4.12.6 release.\nThe upgrade itself, as it took time, caused no appreciable load on the Hub itself, but as highlighted, several SNO hosts did not perform the upgrade properly.\nFinally as just a confirmation about what we were expecting:\nThe Hub did perform a build once the first SNO required the new kernel, but the spokes did no builds at all (which is the exact use case of HUB-Spoke architecture)\nSummary Milestones Check the graphs and milestones for the whole process\n11:42 KMM installation at HUB. 12:17-12:23 KMM installation at SNOs (controller). 13:30 KMM KMOD added and deployed to all SNOs. 16:20 HUB OpenShift upgrade from 4.12.3 to 4.12.6. 17:50 Hub upgrade finished. 18:22 First SNO (sno0001) and others start applying YAML for version signatures and for ICSP (just missing the upgrade itself). 18:24 script to launch all the upgrades in parallel started. 19:38: 1500 updated, 249 in progress. 19:49: 4 nodes failed the OpenShift Upgrade. 22:51: all SNOs, including the failing ones, are upgraded to 4.12.6. Metrics about spokes\nMetric Spokes # Potential 1755 Spokes not working properly 7 Operative 1748 KMM installed 1748 KMOD deployed 1748 KMOD updated 1748 Results For KMM Operator deployment at Hub, OperatorHub via UI has been used and a Policy has been applied with oc client in order to deploy the controller at spokes:\nKMM DEPLOYMENT Deployment time CPU utilization post-deployment MEM utilization post-deployment Hub \u003c1min 1.5% 627 MB Peak 3% 200 Mb During 0.4% 200 MB After \u003c 7 min negligible 200 MB For KMM-KMOD deployment a ManagedClusterModule has been applied so the image is built on Hub and then deployed to all spokes:\nKMM KMOD Deployment Build time Deployment time CPU utilization MEM utilization Hub \u003c2min N/A 30% peak Spoke (per) N/A \u003c 1 min after build Spoke (avg. ) N/A \u003c 1min 0.08% 80Mb Spoke (total) N/A 11 mins 0.2% No appreciable change in RAM usage For the KMM-KMOD upgrade, we used the different kernel versions between RHCOS shipped on OCP 4.12.3 and 4.12.6. Both Hub and Spokes were upgraded to 4.12.6 so the new kernel version was detected by KMM and a new KMM-KMOD was built at Hub and automatically deployed to all spokes:\nNote that some values are reported as N/A for the spokes as the operation came as part of the OpenShift Upgrade itself.\nKMM KMOD Upgrade Build time Deployment time CPU utilization MEM utilization Hub \u003c2.5 mins N/A Peak of 60% (one host, one core) for a brief time during compilation Peak of 4.3Gb and stabilizing on 3.3Gb Spoke (per) N/A N/A N/A N/A Spoke (avg.) N/A N/A N/A N/A Spoke (total) N/A N/A N/A N/A Lessons learned As highlighted in the previous paragraphs this test has been affected by several infrastructure issues:\nOpenShift deployment (SNO installation) by ACM. OpenShift Upgrades (SNOs failing in the upgrade and requiring manual intervention). Also, some other difficulties played in this:\nThe infrastructure uses a dense number of VMs to simulate the clusters and this can cause: Bottlenecks on the network during the installation of SNOs. Netmask requiring adjustment when using IPv4, or using IPv6. Using IPv6 requires using a disconnected environment, which requires extra work for ‘simple’ things like: Mirroring the images. Deploying new operators (like the custom Grafana for customizing the dashboard). We’ve contributed back some of the found issues to the jetlag repository to reduce some of the problems found, but still, there were a lot of manual tasks required for setting up the environment itself.\nIn the end, the KMM test was very smooth and the results were along the expected lines: little to no impact at all on the clusters, but all the preparation work involved in the setup, and troubleshooting everything else involved, took most of the time.\nIt’s worth considering investing extra time in shaping those scripts and setups to make them more straightforward when a team is going to scale test a product and avoid the hassle of dealing with all the nits and picks from the environment itself which could be considered as infrastructure and not really a part of the test itself.\nThanks We want to give special thanks to the following who helped contribute to the success of this endeavor:\nScale Lab team Alex Krzos Edge Pillar, Partner Accelerator team ","wordCount":"2780","inLanguage":"en","image":"https://iranzo.io/mugshot.png","datePublished":"2023-06-21T00:00:00+02:00","dateModified":"2024-04-09T15:05:24.842Z","author":[{"@type":"Person","name":"Pablo Iranzo Gómez"},{"@type":"Person","name":"Enrique Belarte"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://iranzo.io/blog/2023/06/21/kmm-1.1-scale-testing/"},"publisher":{"@type":"Organization","name":"Pablo Iranzo Gómez blog","logo":{"@type":"ImageObject","url":"https://iranzo.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://iranzo.io/ accesskey=h title="Home (Alt + H)"><img src=https://iranzo.io/apple-icon-152x152.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://iranzo.io/es/ title=Spanish aria-label=Spanish>Es</a></li></ul></div></div><ul id=menu><li><a href=https://iranzo.io/ title=🏠Home><span>🏠Home</span></a></li><li><a href=https://iranzo.io/about/ title=🗒️About><span>🗒️About</span></a></li><li><a href=https://iranzo.io/redken_bot/ title=🐘redken_bot><span>🐘redken_bot</span></a></li><li><a href=https://iranzo.io/projects/ title=📐Projects><span>📐Projects</span></a></li><li><a href=https://iranzo.io/archives/ title="🗄️ Archives"><span>🗄️ Archives</span></a></li><li><a href=https://iranzo.io/categories/ title=🗺️Categories><span>🗺️Categories</span></a></li><li><a href=https://iranzo.io/tags/ title=🏷️Tags><span>🏷️Tags</span></a></li><li><a href=https://iranzo.io/search title="🔎Search (Alt + /)" accesskey=/><span>🔎Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://iranzo.io/>Home</a>&nbsp;»&nbsp;<a href=https://iranzo.io/blog/>Blogs</a></div><h1 class=post-title>KMM 1.1 Scale testing</h1><div class=post-meta><span title='2023-06-21 00:00:00 +0200 +0200'>June 21, 2023</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;Pablo Iranzo Gómez, Enrique Belarte</div><meta http-equiv=refresh content="1; url=https://cloud.redhat.com/blog/scale-testing-kernel-module-management"></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#vocabulary aria-label=Vocabulary>Vocabulary</a></li><li><a href=#the-test-goal aria-label="The test goal">The test goal</a></li><li><a href=#test-environment aria-label="Test environment">Test environment</a></li><li><a href=#the-grafana-instance aria-label="The Grafana instance">The Grafana instance</a></li><li><a href=#our-results aria-label="Our results">Our results</a><ul><li><a href=#kmm-installation aria-label="KMM installation">KMM installation</a></li><li><a href=#module-installation aria-label="Module installation">Module installation</a></li><li><a href=#module-upgrade aria-label="Module upgrade">Module upgrade</a></li></ul></li><li><a href=#summary aria-label=Summary>Summary</a><ul><li><a href=#milestones aria-label=Milestones>Milestones</a></li><li><a href=#results aria-label=Results>Results</a></li></ul></li><li><a href=#lessons-learned aria-label="Lessons learned">Lessons learned</a></li><li><a href=#thanks aria-label=Thanks>Thanks</a></li></ul></div></details></div><div class=post-content><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>First published at <a href=https://cloud.redhat.com/blog/scale-testing-kernel-module-management>https://cloud.redhat.com/blog/scale-testing-kernel-module-management</a></p></div><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Kernel Module Management (KMM) Operator manages, builds, signs and deploys out-of-tree kernel modules and device plugins on OpenShift Container Platform clusters.</p><p>KMM adds, for the HUB/Spoke scenario, a new ManagedClusterModule which describes an out-of-tree kernel module and its associated device plugin. You can use ManagedClusterModule resources to configure how to load the module, define <a href=https://kmm.sigs.k8s.io/documentation/module_loader_image/>ModuleLoader</a> images for kernel versions, and include instructions for building and signing modules for specific kernel versions.</p><p>KMM is designed to accommodate multiple kernel versions at once for any kernel module, allowing for seamless node upgrades and reduced application downtime.</p><p>For more information about it, refer to the product documentation available at <a href=https://docs.openshift.com/container-platform/4.12/hardware_enablement/kmm-kernel-module-management.html>Kernel Module Management Operator | Specialized hardware and driver enablement | OpenShift Container Platform 4.12</a></p><p>Don&rsquo;t forget that also KMM is a community project at <a href=https://github.com/kubernetes-sigs/kernel-module-management>kubernetes-sigs/kernel-module-management</a> and that it can be tested on upstream Kubernetes and has a slack community channel at <a href=https://kubernetes.slack.com/archives/C037RE58RED>Kubernetes Slack</a>.</p><h1 id=vocabulary>Vocabulary<a hidden class=anchor aria-hidden=true href=#vocabulary>#</a></h1><p>In order to make it easier, we&rsquo;re going to use some acronyms and products along this text, find here the most common ones:</p><table><thead><tr><th>Term</th><th>Definition</th></tr></thead><tbody><tr><td>KMM</td><td>Kernel Module Management</td></tr><tr><td>ACM</td><td>Advance Cluster Management</td></tr><tr><td>OpenShift</td><td>Red Hat&rsquo;s Kubernetes-based product</td></tr><tr><td>HUB</td><td>Central management Cluster that via ACM manages some Spokes</td></tr><tr><td>Spoke</td><td>Cluster managed via ACM from a management cluster referred to as Hub</td></tr><tr><td>SNO</td><td>Single-Node OpenShift</td></tr><tr><td>CRD</td><td>Custom Resource Definition</td></tr><tr><td>EDGE</td><td>Relevant to Telco 5G and other use cases, refers to systems that are placed close to the end user making use of the services to get better performance</td></tr><tr><td>OOT</td><td>Out-of-tree, referred to Kernel Module</td></tr></tbody></table><h1 id=the-test-goal>The test goal<a hidden class=anchor aria-hidden=true href=#the-test-goal>#</a></h1><p>One of the new features coming in KMM 1.1 is the ability to work in Hub-Spoke architectures, by leveraging Advanced Cluster Management capabilities, deploying KMM in this hub-spoke architecture is like a walk in the park.</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/100002010000034C00000241B0253DB93395DCA8.png></p><p>This kind of setup is very common at the EDGE, where data centers have a more resourceful set of servers, while EDGE devices are more resource-constrained, and everything that can be saved, can be used to provide a better experience to the closest users.</p><p>In this mode, KMM is able to build new kernel drivers for specific releases on the hub cluster, and then, deliver the built images to the spokes clusters, which, using fewer resources can still benefit from hardware enablement which automatically gets updated for each kernel when a newer OpenShift version image gets released.</p><p>The KMM team wanted to perform these tests and grab metrics on the behavior in a large-scale environment that allowed them to evaluate the action points on the HUB-Spoke scenario.</p><p>The goal for this test is to reach 1000 nodes deployed with the KMM module and monitor the actual resource utilization for the whole process.</p><h1 id=test-environment>Test environment<a hidden class=anchor aria-hidden=true href=#test-environment>#</a></h1><p>We reached out to the ScaleLab team at <a href=https://scalelab.redhat.com/>https://scalelab.redhat.com/</a> to explain our use case and to request a loan of machines we could use for our testing.</p><p>The team provided 69 systems in total, all equal, and with the following specifications:</p><ul><li>Dell R650 with 512Gb of RAM, 3TB NVME, 2<em>1.8Tb SSD + 1</em> 447GB SSD, powered by the Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz processor reported as 112 CPU&rsquo;s (dual processor, 56 cores, 112 threads).</li></ul><p>In order to set up the environment, we used 4 hosts for base infrastructure, designed to become:</p><ul><li>1 Bastion for deploying other hosts and interacting with the environment (Ansible playbook execution, mirror registry, etc.).</li><li>3 nodes for creating an OpenShift Baremetal Compact cluster that will hold the hub cluster.</li></ul><p>The remaining 65 hosts were configured as hypervisors for running virtual machines with KVM to be used as Single Node OpenShift (SNO) clusters.</p><table><thead><tr><th>Node</th><th>Hardware</th><th>CPU</th><th>RAM(GiB)</th><th>Disk(GiB)</th><th>Count</th></tr></thead><tbody><tr><td>Bastion</td><td>Dell R650</td><td>112</td><td>512</td><td>446 SSD / 2x1.8TB SSD / 2.9TB NVMe</td><td>1</td></tr><tr><td>Hub BM Cluster</td><td>Dell R650</td><td>112</td><td>512</td><td>446 SSD / 2x1.8TB SSD / 2.9TB NVMe</td><td>3</td></tr><tr><td>SNO HV</td><td>Dell R650</td><td>112</td><td>512</td><td>446 SSD / 2x1.8TB SSD / 2.9TB NVMe</td><td>65</td></tr><tr><td>SNO VM</td><td>Libvirt KVM</td><td>8</td><td>18</td><td>120</td><td>1755 (max)</td></tr></tbody></table><p>It might sound easy to achieve, but in reality, there are several things to take into account in order to set up the whole environment:</p><ul><li>Configure all the relevant hosts and get access to them using Ansible.</li><li>Enable the required packages and services for virtualization.</li><li>Define the network cards for the VM&rsquo;s to be interconnected and able to reach the hub.</li><li>Have proper sizing and placing of the VM&rsquo;s so that the faster hard drives are used for holding the VM disks and avoid extra workload because of VM density causing disk pressure on the same drive.</li></ul><p>Most of this logic is already present in the scripts at <a href=https://github.com/redhat-performance/jetlag/>https://github.com/redhat-performance/jetlag/</a> repository, which is pretty tied to the SCALELAB environment that we were going to use for this setup to prepare and configure the relevant resources for this task.</p><p>In total we got 27 VMs per host, getting to a total of 1755 potential SNOs. Note the word potential here… we&rsquo;re setting up the infrastructure to mimic real hardware:</p><ul><li>Remotely power on and off the VMs (using <a href=https://github.com/openstack/sushy-tools>sushy-tools</a> to interact with libvirt).</li><li>KVM for running the VMs.</li><li>Etc.</li></ul><p>Each SNO is configured with <a href=https://github.com/redhat-performance/jetlag/blob/main/ansible/roles/hv-vm-create/templates/kvm-def.xml.j2>8 VCPU and 18Gb of RAM</a>, which is in line with the bare minimum requirements to get OpenShift deployed.</p><p>For example, we had to alter the limits for the KMM deployment in order to allow it to use more memory during the module build, which was initially limited to 128Mb of RAM and was not enough for the compilation of the module.</p><p>We were pretty lucky and after the whole process of deployment in this restricted scenario, configuration, ACM deployment, and provisioning of the clusters, only one cluster failed ‘sno01412’, out of 1755 SNOs in total.</p><p>Note that some other spokes will be discarded in the next paragraphs.</p><p>For this setup, and to avoid external issues, we decided to do the installation following the disconnected IPv6 approach, that is, the bastion is running an internal registry which is later configured in all clusters via a CatalogSource:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>operators.coreos.com/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>CatalogSource</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>rh-du-operators</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-marketplace</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>displayName</span>: <span style=color:#ae81ff>disconnected-redhat-operators</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>image</span>: <span style=color:#ae81ff>d20-h01-000-r650:5000/olm-mirror/redhat-operator-index:v4.12</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>publisher</span>: <span style=color:#ae81ff>Red Hat</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>sourceType</span>: <span style=color:#ae81ff>grpc</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>updateStrategy</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>registryPoll</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>interval</span>: <span style=color:#ae81ff>1h</span>
</span></span></code></pre></div><p>This process is performed automatically as part of the scripts used to deploy the environment, but we wanted to validate and fix some hosts that failed to apply the policy that automatically was configuring it.</p><p>In order to prepare the setup, we also had to mirror into our registry the KMM Operator, and for the reporting, we also added the Grafana operator.</p><p>As a side issue, we ran out of disk space in the bastion, as we were only using the 500 GB SSD, so we&rsquo;ve noted this to automatically set additional space on /var/lib/containers on the bastion using extra disks.</p><p>Also, as we switched from IPv4 to IPv6, we were required to perform some manual cleanup of leftovers that could have caused some of the instabilities we saw in previous attempts.</p><h1 id=the-grafana-instance>The Grafana instance<a hidden class=anchor aria-hidden=true href=#the-grafana-instance>#</a></h1><p>OpenShift already includes a Grafana dashboard, together with ACM, that can show some information about the bare-metal cluster we will use for management.</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/100002010000072F000003B41B178679481C50E9.png></p><p>However, in order to be able to customize the dashboard, we are required to install a custom instance. This custom instance is what we&rsquo;ll show in the next screenshots to highlight the behavior of the cluster and the numbers obtained.</p><h1 id=our-results>Our results<a hidden class=anchor aria-hidden=true href=#our-results>#</a></h1><p>As a briefing, we&rsquo;ve 1754 valid possible SNOs (deployed and in operation).</p><p>Out of those, 6 spokes were removed, as they failed in different steps (test upgrade of OpenShift, operators in a non-working state, etc).</p><p>So 1748, is the real number of spokes available for deploying KMM</p><h2 id=kmm-installation>KMM installation<a hidden class=anchor aria-hidden=true href=#kmm-installation>#</a></h2><p>For deploying KMM we used the operator, to get it added on the Hub.</p><p>Using ACM, we defined a policy to also add it to the managed clusters, so that&rsquo;s what we&rsquo;ll be seeing in the next graphs.</p><p>First, the KMM Hub installation started at 11:42, and we can see some data on the following graph:</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/100002010000039000000121C6C0873FF8D2CBF6.png></p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/1000020100000390000001297BDAEBAF8E447190.png></p><p>As we started the spokes installation in the period between 12:17 and 12:23 we can also see that after the initial setup of the hub and some activity during the spokes installation, the usage of resources is more or less steady, we can see a bump to a new level around 13:30, but this was once all the activity for deployment has finished.</p><p>We can compare this pattern as well with the SNO&rsquo;s resources usage:</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/1000020100000390000001565907347C7C4CBB7C.png></p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/10000201000003A2000001567041AB958D7DFBBC.png></p><p>The SNOs were already active but without too much load in RAM or CPU, as a big part of the installation is just deploying the required components in the clusters.</p><p>If we focus on the average graphs to avoid the spikes of regular activity we can see some patterns:</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/10000201000003900000012337A1285C8DD5EB0A.png></p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/10000201000003980000012906353036F284D06C.png></p><p>And for the spokes:</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/1000020100000399000001290AEB14BCFD6D50C6.png></p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/10000201000003900000012186A6259F523EA553.png></p><p>In general, Hub has been unaffected, and in the SNOs, we have a bit more activity in average and RAM usage, but if we check the numbers it&rsquo;s around 200Mb of RAM usage in difference, so not a big deal in terms of resource consumption.</p><p>In terms of KMM controllers, we can see it really clearly here:</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/10000201000003900000012240AD8E1F19996B05.png></p><p>The number of KMM controllers increased in a really short period of time and got to the total number of operative SNOs</p><h2 id=module-installation>Module installation<a hidden class=anchor aria-hidden=true href=#module-installation>#</a></h2><p>Now that KMM has been installed, we need to actually deploy a module, this will cause KMM to compile it, and prepare the relevant images that the Spokes will consume.</p><p>The module used for this test is a really simple module called <a href=https://github.com/rh-ecosystem-edge/kernel-module-management/tree/main/ci/kmm-kmod>kmm-kmod</a> which is part of the KMM project. It just prints a “Hello World” and a “Goodbye World” message to the kernel log but it is suitable for testing building and loading kernel modules with KMM like any other ‘production’ module would be.</p><p>In our case, we first got the module compiled and installed prior to our testing so that we can compare the workload increase when the module is already prepared in the hub and ready for distribution.</p><p>In this case, the container that was already working in the HUB started to be deployed and we can check the count change here:</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/100002010000039A00000129E8776215C8E330D1.png></p><p>Around 13:30, all SNOs got the module container up and running and
repeated the graphs provided earlier for the whole period, we can see
that hub increased memory usage</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/1000020100000390000001297BDAEBAF8E447190.png></p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/10000201000003980000012906353036F284D06C.png></p><p>And for the spokes:</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/1000020100000399000001290AEB14BCFD6D50C6.png></p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/10000201000003900000012186A6259F523EA553.png></p><p>As we can see that once KMM was installed, a bump in the memory (under 200Mb) happened, but no appreciable change once the module was loaded.</p><p>Note that in this situation, the hub creates the module (compilation, etc), builds the image, and then ACM does the deployment to the spokes.</p><p>From the numbers, we see that 100% of the operative SNOs deployed the KMM controller and loaded the module within a really short timeframe with no noticeable impact.</p><h2 id=module-upgrade>Module upgrade<a hidden class=anchor aria-hidden=true href=#module-upgrade>#</a></h2><p>One of the tests we wanted to perform was to get SNOs to do a kernel upgrade, in this way, we can test KMM by doing the new module compilation itself and then, delivering the updated kernel module to the spoke clusters.</p><p>This, of course, has an implication… a way to get a newer kernel on the SNO nodes… is by actually upgrading OpenShift itself.</p><p>This, which might sound easy, means having to mirror the required images for the newer OpenShift release, apply an ICSP to all the SNOs, as well as adding the signatures for validating the new image, and of course, launch the upgrade itself.</p><p>The Hub started the upgrade at 16:20, moving from a starting OpenShift version 4.12.3 to the final version 4.12.6 which was previously mirrored, with the relevant signatures added, etc.</p><p>The upgrade took around 90 minutes in total for the Hub, and once the Hub moved to 4.12.6, it recompiled the Kernel module.</p><p>In our tests, we tested manually on the first SNO (sno0001), and once validated, this was launched for all the remaining and active nodes.</p><p>The timing was the following:</p><ul><li>18:22 First SNO (sno0001) and others start applying YAML for version signatures and for ICSP (just missing the upgrade itself).</li><li>18:24 script to launch all the upgrades in parallel started.</li><li>19:38: 1500 updated, 249 in progress.</li><li>19:49: 4 nodes failed the OpenShift Upgrade.</li><li>22:51: all SNOs, including the failing ones, are upgraded to 4.12.6.</li></ul><p>Out of the total count, 4 nodes didn&rsquo;t come back (API not responding), a manual reboot of the SNO&rsquo;s got them into API responding, and apparently good progress towards finishing the upgrade to the 4.12.6 release.</p><p>The upgrade itself, as it took time, caused no appreciable load on the Hub itself, but as highlighted, several SNO hosts did not perform the upgrade properly.</p><p>Finally as just a confirmation about what we were expecting:</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/100002010000073B0000014166BB9D50586F35D5.png></p><p>The Hub did perform a build once the first SNO required the new kernel, but the spokes did no builds at all (which is the exact use case of HUB-Spoke architecture)</p><h1 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h1><h2 id=milestones>Milestones<a hidden class=anchor aria-hidden=true href=#milestones>#</a></h2><p>Check the graphs and milestones for the whole process</p><p><img loading=lazy src=../../../../../blog/2023/06/21/kmm-1.1-scale-testing/images/100002010000073100000379C64605D632B48403.png></p><ul><li>11:42 KMM installation at HUB.</li><li>12:17-12:23 KMM installation at SNOs (controller).</li><li>13:30 KMM KMOD added and deployed to all SNOs.</li><li>16:20 HUB OpenShift upgrade from 4.12.3 to 4.12.6.</li><li>17:50 Hub upgrade finished.</li><li>18:22 First SNO (sno0001) and others start applying YAML for version signatures and for ICSP (just missing the upgrade itself).</li><li>18:24 script to launch all the upgrades in parallel started.</li><li>19:38: 1500 updated, 249 in progress.</li><li>19:49: 4 nodes failed the OpenShift Upgrade.</li><li>22:51: all SNOs, including the failing ones, are upgraded to 4.12.6.</li></ul><p>Metrics about spokes</p><table><thead><tr><th>Metric</th><th>Spokes #</th></tr></thead><tbody><tr><td>Potential</td><td>1755</td></tr><tr><td>Spokes not working properly</td><td>7</td></tr><tr><td>Operative</td><td>1748</td></tr><tr><td>KMM installed</td><td>1748</td></tr><tr><td>KMOD deployed</td><td>1748</td></tr><tr><td>KMOD updated</td><td>1748</td></tr></tbody></table><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p><strong>For KMM Operator deployment at Hub, OperatorHub via UI has been used and a Policy has been applied with oc client in order to deploy the controller at spokes:</strong></p><table><thead><tr><th>KMM DEPLOYMENT</th><th>Deployment time</th><th>CPU utilization post-deployment</th><th>MEM utilization post-deployment</th></tr></thead><tbody><tr><td>Hub</td><td>&lt;1min</td><td>1.5%</td><td>627 MB</td></tr><tr><td>Peak</td><td></td><td>3%</td><td>200 Mb</td></tr><tr><td>During</td><td></td><td>0.4%</td><td>200 MB</td></tr><tr><td>After</td><td>&lt; 7 min</td><td>negligible</td><td>200 MB</td></tr></tbody></table><p><strong>For KMM-KMOD deployment a ManagedClusterModule has been applied so the image is built on Hub and then deployed to all spokes:</strong></p><table><thead><tr><th>KMM KMOD Deployment</th><th>Build time</th><th>Deployment time</th><th>CPU utilization</th><th>MEM utilization</th></tr></thead><tbody><tr><td>Hub</td><td>&lt;2min</td><td>N/A</td><td>30% peak</td><td></td></tr><tr><td>Spoke (per)</td><td>N/A</td><td>&lt; 1 min after build</td><td></td><td></td></tr><tr><td>Spoke (avg.</td><td>) N/A</td><td>&lt; 1min</td><td>0.08%</td><td>80Mb</td></tr><tr><td>Spoke (total)</td><td>N/A</td><td>11 mins</td><td>0.2%</td><td>No appreciable change in RAM usage</td></tr></tbody></table><p><strong>For the KMM-KMOD upgrade, we used the <a href="https://releases-rhcos-art.apps.ocp-virt.prod.psi.redhat.com/diff.html?arch=x86_64&amp;first_release=412.86.202302091419-0&amp;first_stream=prod%2Fstreams%2F4.12&amp;second_release=412.86.202302282003-0&amp;second_stream=prod%2Fstreams%2F4.12">different kernel versions</a> between RHCOS shipped on OCP 4.12.3 and 4.12.6. Both Hub and Spokes were upgraded to 4.12.6 so the new kernel version was detected by KMM and a new KMM-KMOD was built at Hub and automatically deployed to all spokes:</strong></p><p>Note that some values are reported as N/A for the spokes as the operation came as part of the OpenShift Upgrade itself.</p><table><thead><tr><th>KMM KMOD Upgrade</th><th>Build time</th><th>Deployment time</th><th>CPU utilization</th><th>MEM utilization</th></tr></thead><tbody><tr><td>Hub</td><td>&lt;2.5 mins</td><td>N/A</td><td>Peak of 60% (one host, one core) for a brief time during compilation</td><td>Peak of 4.3Gb and stabilizing on 3.3Gb</td></tr><tr><td>Spoke (per)</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td></tr><tr><td>Spoke (avg.)</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td></tr><tr><td>Spoke (total)</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td></tr></tbody></table><h1 id=lessons-learned>Lessons learned<a hidden class=anchor aria-hidden=true href=#lessons-learned>#</a></h1><p>As highlighted in the previous paragraphs this test has been affected by several infrastructure issues:</p><ul><li>OpenShift deployment (SNO installation) by ACM.</li><li>OpenShift Upgrades (SNOs failing in the upgrade and requiring manual intervention).</li></ul><p>Also, some other difficulties played in this:</p><ul><li>The infrastructure uses a dense number of VMs to simulate the clusters and this can cause:</li><li>Bottlenecks on the network during the installation of SNOs.</li><li>Netmask requiring adjustment when using IPv4, or using IPv6.</li><li>Using IPv6 requires using a disconnected environment, which requires extra work for ‘simple&rsquo; things like:</li><li>Mirroring the images.</li><li>Deploying new operators (like the custom Grafana for customizing the dashboard).</li></ul><p>We&rsquo;ve contributed back some of the found issues to the jetlag repository to reduce some of the problems found, but still, there were a lot of manual tasks required for setting up the environment itself.</p><p>In the end, the KMM test was very smooth and the results were along the expected lines: little to no impact at all on the clusters, but all the preparation work involved in the setup, and troubleshooting everything else involved, took most of the time.</p><p>It&rsquo;s worth considering investing extra time in shaping those scripts and setups to make them more straightforward when a team is going to scale test a product and avoid the hassle of dealing with all the nits and picks from the environment itself which could be considered as infrastructure and not really a part of the test itself.</p><h1 id=thanks>Thanks<a hidden class=anchor aria-hidden=true href=#thanks>#</a></h1><p>We want to give special thanks to the following who helped contribute to the success of this endeavor:</p><ul><li>Scale Lab team</li><li>Alex Krzos</li><li>Edge Pillar, Partner Accelerator team</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://iranzo.io/tags/kmm/>KMM</a></li><li><a href=https://iranzo.io/tags/scale/>Scale</a></li><li><a href=https://iranzo.io/tags/kernel-module-management/>Kernel Module Management</a></li></ul><nav class=paginav><a class=prev href=https://iranzo.io/tips/raspbian-upgrade/><span class=title>« Prev</span><br><span>Upgrade Debian from buster to bullseye</span>
</a><a class=next href=https://iranzo.io/blog/2023/03/21/showing-calendar-events-in-telegram/><span class=title>Next »</span><br><span>Showing calendar events in Telegram</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share KMM 1.1 Scale testing on x" href="https://x.com/intent/tweet/?text=KMM%201.1%20Scale%20testing&amp;url=https%3a%2f%2firanzo.io%2fblog%2f2023%2f06%2f21%2fkmm-1.1-scale-testing%2f&amp;hashtags=KMM%2cScale%2cKernelModuleManagement"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share KMM 1.1 Scale testing on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2firanzo.io%2fblog%2f2023%2f06%2f21%2fkmm-1.1-scale-testing%2f&amp;title=KMM%201.1%20Scale%20testing&amp;summary=KMM%201.1%20Scale%20testing&amp;source=https%3a%2f%2firanzo.io%2fblog%2f2023%2f06%2f21%2fkmm-1.1-scale-testing%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share KMM 1.1 Scale testing on reddit" href="https://reddit.com/submit?url=https%3a%2f%2firanzo.io%2fblog%2f2023%2f06%2f21%2fkmm-1.1-scale-testing%2f&title=KMM%201.1%20Scale%20testing"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share KMM 1.1 Scale testing on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2firanzo.io%2fblog%2f2023%2f06%2f21%2fkmm-1.1-scale-testing%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share KMM 1.1 Scale testing on whatsapp" href="https://api.whatsapp.com/send?text=KMM%201.1%20Scale%20testing%20-%20https%3a%2f%2firanzo.io%2fblog%2f2023%2f06%2f21%2fkmm-1.1-scale-testing%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share KMM 1.1 Scale testing on telegram" href="https://telegram.me/share/url?text=KMM%201.1%20Scale%20testing&amp;url=https%3a%2f%2firanzo.io%2fblog%2f2023%2f06%2f21%2fkmm-1.1-scale-testing%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share KMM 1.1 Scale testing on ycombinator" href="https://news.ycombinator.com/submitlink?t=KMM%201.1%20Scale%20testing&u=https%3a%2f%2firanzo.io%2fblog%2f2023%2f06%2f21%2fkmm-1.1-scale-testing%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://iranzo.io/>Pablo Iranzo Gómez blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><center><small>This blog is a participant in the Amazon Associate Program, an affiliate advertising program designed to provide a means for sites to earn advertising fees by advertising and linking to Amazon.</small></center><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>