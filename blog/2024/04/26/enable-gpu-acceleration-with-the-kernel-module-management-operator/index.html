<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Enable GPU acceleration with the Kernel Module Management Operator | Pablo Iranzo G√≥mez blog</title><meta name=keywords content="KMM,OpenShift,DTK,NFD,Intel Flex,GPU,Acceleration,AI,Artificial Intelligence"><meta name=description content="Learn how to enable GPU acceleration with the Kernel Module Management (KMM) Operator so you can can run AI workloads on top of Intel Data Center GPU Flex 140."><meta name=author content="Pablo Iranzo G√≥mez"><link rel=canonical href=https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/><meta name=google-site-verification content="Bk4Z5ucHLyPXqlZlj5LzANpYBBSvxqBW4E8i-Kwf-bQ"><meta name=yandex-verification content="993ede96cdfbee95"><link crossorigin=anonymous href=../../../../../assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://iranzo.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://iranzo.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://iranzo.io/favicon-32x32.png><link rel=apple-touch-icon href=https://iranzo.io/apple-touch-icon.png><link rel=mask-icon href=https://iranzo.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZL9P943TP1"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZL9P943TP1")}</script><meta property="og:url" content="https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/"><meta property="og:site_name" content="Pablo Iranzo G√≥mez blog"><meta property="og:title" content="Enable GPU acceleration with the Kernel Module Management Operator"><meta property="og:description" content="Learn how to enable GPU acceleration with the Kernel Module Management (KMM) Operator so you can can run AI workloads on top of Intel Data Center GPU Flex 140."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-04-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-12T09:32:52+00:00"><meta property="og:image" content="https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/cover.png"><meta name=twitter:title content="Enable GPU acceleration with the Kernel Module Management Operator"><meta name=twitter:description content="Learn how to enable GPU acceleration with the Kernel Module Management (KMM) Operator so you can can run AI workloads on top of Intel Data Center GPU Flex 140."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://iranzo.io/blog/"},{"@type":"ListItem","position":2,"name":"Enable GPU acceleration with the Kernel Module Management Operator","item":"https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Enable GPU acceleration with the Kernel Module Management Operator","name":"Enable GPU acceleration with the Kernel Module Management Operator","description":"Learn how to enable GPU acceleration with the Kernel Module Management (KMM) Operator so you can can run AI workloads on top of Intel Data Center GPU Flex 140.","keywords":["KMM","OpenShift","DTK","NFD","Intel Flex","GPU","Acceleration","AI","Artificial Intelligence"],"articleBody":" Attention\nThis post was originally published on Red Hat Developer, the community to learn, code, and share faster. To read the original post, click here.\nCheck the video version for Red Hat TV\nIn this article we cover the required steps to configure Kernel Module Management Operator (KMM) and use it to deploy an out-of-tree (OOT) kernel module, as well as leveraging other related technologies to build a toolset for hardware enablement. To illustrate that process, we‚Äôll leverage the Intel Data Center GPU Flex 140.\nWhat is the Kernel Module Management Operator? The Kernel Module Management Operator manages, builds, signs, and deploys out-of-tree (OOT) kernel modules and device plug-ins on Red Hat OpenShift Container Platform clusters.\nBefore KMM, cluster admins had to manually install drivers to multiple nodes. Upgrades were painful and prone to errors from incompatible drivers. Furthermore, workloads might get scheduled to a node with broken drivers causing scheduling issues or missing hardware. KMM solves all of these problems, as we‚Äôll see.\nKMM is designed to accommodate multiple kernel versions at once for any kernel module, allowing for seamless node upgrades and reduced application downtime. For more information, refer to the Kernel Module Management Operator product documentation.\nKMM is also a community project, which you can test on upstream Kubernetes, and there is a Slack community channel.\nPrerequisites For this scenario, we‚Äôll require an already working OpenShift environment as we will use it to deploy the different tools on top. Check the documentation for instructions.\nKMM will require a registry to push images to. If you‚Äôve installed on bare metal, ensure the internal registry is enabled and configured (refer to Installing a user-provisioned cluster on bare metal).\nAdditionally, this tutorial references data available from Intel at the following locations:\nIntel Data Center GPU Driver for OpenShift Intel Technology Enabling for OpenShift Set up Node Feature Discovery Operator Node Feature Discovery (NFD) detects hardware features available on nodes and advertises those features using nodes labels, so that they can later be used as selector for scheduling decisions.\nThe NFD Operator automatically adds labels to the nodes that present some characteristics, including if the node has a GPU and which GPU it has.\nIt‚Äôs an ideal way to identify which nodes require a kernel module to be enabled for the specific node(s) and later use it to instruct KMM to build it only for those.\nWe can install it via the following YAML:\n--- apiVersion: v1 kind: Namespace metadata: name: openshift-nfd --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-nfd namespace: openshift-nfd spec: targetNamespaces: - openshift-nfd --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace Once installed, we can create a CRD as described in the NFD Operator documentation to start.\nCreate a file named nfdcr.yaml with the following contents:\napiVersion: nfd.openshift.io/v1 kind: NodeFeatureDiscovery metadata: name: nfd-instance namespace: openshift-nfd spec: operand: image: quay.io/openshift/origin-node-feature-discovery:4.14 imagePullPolicy: Always servicePort: 12000 workerConfig: configData: | Then apply it with:\noc apply -f nfdcr.yaml Additionally, create a nodefeaturerule.yaml:\napiVersion: nfd.openshift.io/v1alpha1 kind: NodeFeatureRule metadata: name: intel-dp-devices spec: rules: - name: \"intel.gpu\" labels: \"intel.feature.node.kubernetes.io/gpu\": \"true\" matchFeatures: - feature: pci.device matchExpressions: vendor: { op: In, value: [\"8086\"] } class: { op: In, value: [\"0300\", \"0380\"] } And once it has been applied, oc describe node displays the labels applied. Below is a partial output of the labels that NFD applies:\nLabels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux feature.node.kubernetes.io/cpu-cpuid.ADX=true feature.node.kubernetes.io/cpu-cpuid.AESNI=true feature.node.kubernetes.io/cpu-cpuid.AMXINT8=true feature.node.kubernetes.io/cpu-cpuid.AMXTILE=true feature.node.kubernetes.io/cpu-cpuid.AVX=true feature.node.kubernetes.io/cpu-cpuid.AVX2=true ‚Ä¶ As we can see, it starts showing some of the CPU flags among other values as labels in our system. We can then grep for Intel-specific GPU labels with:\n$ oc describe node rhvp-intel-01|grep intel.feature intel.feature.node.kubernetes.io/gpu=true ‚Ä¶where we can see that NFD has detected the GPU.\nAt this point, NFD will take care of our node‚Äôs HW detection and labeling, which can later be used as node selectors for KMM to deploy the required modules.\nAdvanced node labeling For this use case, the Intel Data Center GPU Driver for OpenShift (i915) driver is only available and tested for some kernel versions.\nUsing NFD labels, we can target specific custom kernel versions for our module deployment and enablement so that only hosts with the required kernel and the required hardware are enabled for driver activation. This ensures that only compatible drivers are installed on nodes with a supported kernel, which is what makes KMM so valuable.\nIn this case, we‚Äôll be using nodes that contain the intel.feature.node.kubernetes.io/gpu tag.\nLet‚Äôs move on to the next steps in our journey, installing KMM and building the kernel module with Driver Toolkit (DTK). DTK is a container image used as a base image where drivers can be built, as it includes kernel packages, some tools, etc.\nSet up Kernel Module Management Operator Install KMM using OperatorHub in our OpenShift Console or the following kmm.yaml:\n--- apiVersion: v1 kind: Namespace metadata: name: openshift-kmm --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: kernel-module-management namespace: openshift-kmm --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: kernel-module-management namespace: openshift-kmm spec: channel: \"stable\" installPlanApproval: Automatic name: kernel-module-management source: redhat-operators sourceNamespace: openshift-marketplace With:\noc apply -f kmm.yaml Once it‚Äôs done, we can switch to the openshift-kmm project:\noc project openshift-kmm Create a kernel module As mentioned previously, KMM can perform the compilation and installation of kernel module drivers for our hardware.\nA kernel module can:\nHave dependencies Replace an existing driver Let‚Äôs explore the use cases in the next sections of this article.\nIntegration with NFD via CRDs KMM uses a kmod (kernel module) image to define which kernel modules to load, which is an OCI image that contains the .ko files.\nIn .spec.selector, we can define which nodes should be selected and as we showcased earlier, we can target one specific label added by NFD, so that only those nodes are targeted for loading the module for the hardware installed.\nKMM dependencies Adding a module might have additional module dependencies, that is, extra modules that need to be already loaded in the kernel.\nWe can use the Custom Resource Definition (CRD) field .spec.moduleLoader.container.modprobe.modulesLoadingOrder to identify the order for module loading starting with upmost module, then the module it depends on, and so on.\nReplace an in-tree module with an out-of-tree module Similar to dependencies, sometimes the module being loaded conflicts with an already loaded kernel module.\nIn this case, we need to have KMM first remove the conflicting module via the .spec.moduleLoader.container.inTreeModuleToRemove field of the CRD. KMM will then proceed and load the newer OOT module.\nFor the Intel Data Center GPU Flex series, the intel_vsec and i915 drivers will have to be removed which will be discussed later in this article.\nConfigure the Driver Toolkit for image building The Driver Toolkit) provides a base image with required kernel development packages that are used to build specific drivers for our platform, which match the kernel version used on each node where the accelerators exist.\nBy using a specially crafted Containerfile containing a reference to DTK_AUTO as shown below:\nARG DTK_AUTO FROM ${DTK_AUTO} RUN gcc \\... | The KMM Operator will replace the required variables as well as pull and build the image for the driver.\nAnd that‚Äôs all. Easy, right?\nManage heterogeneous nodes in the cluster As we‚Äôre using labels for selecting specific nodes in our cluster, we can keep a mix of nodes with or without the hardware in our cluster. KMM will take care of loading the required modules on the matching nodes, leaving the other ones without the specific accelerator.\nIn our case we‚Äôre using intel.feature.node.kubernetes.io/gpu=true as the label to match our intended nodes, leaving other nodes without the GPU affected.\nEnable the Intel Data Center GPU Flex 140 We‚Äôre going to explore the step-by-step the process for detecting, configuring, and enabling an Intel GPU in our OpenShift environment.\nOne of our workers has the accelerator installed, as reported by lspci | egrep 'Graphic|Display:\n02:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 52) a0:00.0 Display controller: Intel Corporation Data Center GPU Flex 140 (rev 05) Let‚Äôs create a MachineConfigPool (MCP) to apply the configuration in our environment:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: intel-dgpu spec: machineConfigSelector: matchExpressions: - { key: machineconfiguration.openshift.io/role, operator: In, values: [worker, intel-dgpu, master], } nodeSelector: matchLabels: intel.feature.node.kubernetes.io/gpu: \"true\" Note\nIf you‚Äôre using single node OpenShift for this test, remember that the YAMLs must be adapted so that the configuration via MCO applies to the primary MCP; that is, using the selector machineconfiguration.openshift.io/role: master.\nUsing a machine configuration, we can define new parameters, like for example disable the built-in drivers with this YAML:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: intel-dgpu name: 100-intel-dgpu-machine-config-disable-i915 spec: config: ignition: version: 3.2.0 storage: files: - contents: source: data:,blacklist%20i915 mode: 0644 overwrite: true path: /etc/modprobe.d/blacklist-i915.conf - contents: source: data:,blacklist%20intel_vsec mode: 0644 overwrite: true path: /etc/modprobe.d/blacklist-intel-vsec.conf After this YAML is applied, we can check that there are no modules applied via oc debug node/rhvp-intel-01 (NOTE: if you see intel_vsec or i915 in the output, verify that the MachineConfig defined was correctly applied):\n$ lsmod|egrep 'i915|vsec' Now, we need to define the path to find the firmware for the module, and for this, there are two approaches; the first one is an MCO that patches the kernel command line (and will cause a reboot of the node) as configured with the following YAML:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: master name: 100-alternative-fw-path-for-master-nodes spec: config: ignition: version: 3.2.0 kernelArguments: - firmware_class.path=/var/lib/firmware And for validating, we can check on the hosts via oc debug node/rhvp-intel-01 that the new parameter has been enabled:\n$ cat /proc/cmdline BOOT_IMAGE=(hd5,gpt3)/ostree/rhcos-085fdd39288474060c9d5bd7a88fabe8d218fcc960186712834c5e4ab319cb1d/vmlinuz-5.14.0-284.52.1.el9_2.x86_64 ignition.platform.id=metal ostree=/ostree/boot.0/rhcos/085fdd39288474060c9d5bd7a88fabe8d218fcc960186712834c5e4ab319cb1d/0 ip=ens801f0:dhcp,dhcp6 root=UUID=bf0c9edf-4aab-48a8-9549-5005fff7890e rw rootflags=prjquota boot=UUID=282ee60b-3053-4c2e-8f92-612af621e245 firmware_class.path=/var/lib/firmware systemd.unified_cgroup_hierarchy=1 cgroup_no_v1=all psi=1 The other approach is to patch the configuration for the KMM operator.\nAlternatively, we can modify the configmap by setting the path for the firmware which doesn‚Äôt cause a reboot, which is useful in single node OpenShift installations:\n$ oc patch configmap kmm-operator-manager-config -n openshift-kmm --type='json' -p='[{\"op\": \"add\", \"path\": \"/data/controller_config.yaml\", \"value\": \"healthProbeBindAddress: :8081\\nmetricsBindAddress: 127.0.0.1:8080\\nleaderElection:\\n enabled: true\\n resourceID: kmm.sigs.x-k8s.io\\nwebhook:\\n disableHTTP2: true\\n port: 9443\\nworker:\\n runAsUser: 0\\n seLinuxType: spc_t\\n setFirmwareClassPath: /var/lib/firmware\"}]' If you follow this approach, the KMM pod must be deleted so that the new configuration is taken into effect as soon as the operator recreates it, so that the systems get it applied during the loading of the modules.\nIntel Data Center GPU Flex 140 kernel module The Intel GPU kernel module deployment might be a bit tricky because it‚Äôs a driver and it has more ties to the specific kernel version being used that we‚Äôll be setting in the commands for the build process, as we‚Äôll see in the Containerfile used for the build.\nUsing prebuilt drivers In this approach, we use KMM to deploy the built and certified drivers already created by Intel which provides the container for each kernel version via a CI/CD pipeline. End users can directly consume that container via this Module definition:\napiVersion: kmm.sigs.x-k8s.io/v1beta1 kind: Module metadata: name: intel-dgpu namespace: openshift-kmm spec: moduleLoader: container: modprobe: moduleName: i915 firmwarePath: /firmware kernelMappings: - regexp: '^.*\\.x86_64$' containerImage: registry.connect.redhat.com/intel/intel-data-center-gpu-driver-container:2.2.0-$KERNEL_FULL_VERSION selector: intel.feature.node.kubernetes.io/gpu: \"true\" Compiling your own driver For the sake of demonstration, here we‚Äôll be building our own driver image using in-cluster builds.\nFor building the kernel module, we can use the following Containerfile based on the upstream instructions that we‚Äôre already defining as configmap:\napiVersion: v1 kind: ConfigMap metadata: name: intel-dgpu-dockerfile-configmap namespace: openshift-kmm data: dockerfile: |- # Intel Data Center GPU driver components combinations. ARG I915_RELEASE=I915_23WW51.5_682.48_23.6.42_230425.56 ARG FIRMWARE_RELEASE=23WW49.5_682.48 # Intel Data Center GPU Driver for OpenShift version. ARG DRIVER_VERSION=2.2.0 # RHCOS Kernel version supported by the above driver version. ARG KERNEL_FULL_VERSION # Red Hat DTK image is used as builder image to build kernel driver modules. # Appropriate DTK image is provided with the OCP release, to guarantee compatibility # between the built kernel modules and the OCP version's RHCOS kernel. # DTK_AUTO is populated automatically with the appropriate DTK image by KMM operator. ARG DTK_AUTO FROM ${DTK_AUTO} as builder ARG I915_RELEASE ARG FIRMWARE_RELEASE ARG KERNEL_FULL_VERSION WORKDIR /build # Building i915 driver RUN git clone -b ${I915_RELEASE} --single-branch https://github.com/intel-gpu/intel-gpu-i915-backports.git \\ \u0026amp;\u0026amp; cd intel-gpu-i915-backports \\ \u0026amp;\u0026amp; install -D COPYING /licenses/i915/COPYING \\ \u0026amp;\u0026amp; export LEX=flex; export YACC=bison \\ \u0026amp;\u0026amp; export OS_TYPE=rhel_9 \u0026amp;\u0026amp; export OS_VERSION=\"9.2\" \\ \u0026amp;\u0026amp; cp defconfigs/i915 .config \\ \u0026amp;\u0026amp; make olddefconfig \u0026amp;\u0026amp; make modules -j $(nproc) \u0026amp;\u0026amp; make modules_install # Copy out-of-tree drivers to /opt/lib/modules/${KERNEL_FULL_VERSION}/ RUN for file in $(find /lib/modules/${KERNEL_FULL_VERSION}/updates/ -name \"*.ko\"); do \\ cp $file /opt --parents; done # Create the symbolic link for in-tree dependencies RUN ln -s /lib/modules/${KERNEL_FULL_VERSION} /opt/lib/modules/${KERNEL_FULL_VERSION}/host RUN depmod -b /opt ${KERNEL_FULL_VERSION} # Firmware RUN git clone -b ${FIRMWARE_RELEASE} --single-branch https://github.com/intel-gpu/intel-gpu-firmware.git \\ \u0026amp;\u0026amp; install -D /build/intel-gpu-firmware/COPYRIGHT /licenses/firmware/COPYRIGHT \\ \u0026amp;\u0026amp; install -D /build/intel-gpu-firmware/COPYRIGHT /build/firmware/license/COPYRIGHT \\ \u0026amp;\u0026amp; install -D /build/intel-gpu-firmware/firmware/dg2* /build/firmware/ \\ \u0026amp;\u0026amp; install -D /build/intel-gpu-firmware/firmware/pvc* /build/firmware/ # Packaging Intel GPU driver components in the base UBI image for certification FROM registry.redhat.io/ubi9/ubi-minimal:9.2 ARG DRIVER_VERSION ARG KERNEL_FULL_VERSION ARG I915_RELEASE ARG FIRMWARE_RELEASE # Required labels for the image metadata LABEL vendor=\"Intel¬Æ\" LABEL version=\"${DRIVER_VERSION}\" LABEL release=\"${KERNEL_FULL_VERSION}\" LABEL name=\"intel-data-center-gpu-driver-container\" LABEL summary=\"Intel¬Æ Data Center GPU Driver Container Image\" LABEL description=\"Intel¬Æ Data Center GPU Driver container image designed for Red Hat OpenShift Container Platform. \\ The driver container is based on Intel Data Center GPU driver components - i915 driver release:${I915_RELEASE}, \\ and Firmware release:${FIRMWARE_RELEASE}. This driver container image is supported for RHOCP 4.14 RHCOS kernel version: ${KERNEL_FULL_VERSION}.\" RUN microdnf update -y \u0026amp;\u0026amp; rm -rf /var/cache/yum RUN microdnf -y install kmod findutils \u0026amp;\u0026amp; microdnf clean all COPY --from=builder /licenses/ /licenses/ COPY --from=builder /opt/lib/modules/${KERNEL_FULL_VERSION}/ /opt/lib/modules/${KERNEL_FULL_VERSION}/ COPY --from=builder /build/firmware/ /firmware/i915/ Let‚Äôs also define the imagestream for storing the generated driver image:\napiVersion: image.openshift.io/v1 kind: ImageStream metadata: labels: app: intel-dgpu-driver-container-kmmo name: intel-dgpu-driver-container-kmmo namespace: openshift-kmm spec: {} Reference this Containerfile in the following YAML:\n--- apiVersion: kmm.sigs.x-k8s.io/v1beta1 kind: Module metadata: name: intel-dgpu-on-premise namespace: openshift-kmm spec: moduleLoader: container: imagePullPolicy: Always modprobe: moduleName: i915 firmwarePath: /firmware kernelMappings: - regexp: '^.*\\.x86_64$' containerImage: image-registry.openshift-image-registry.svc:5000/openshift-kmm/intel-dgpu-driver-container-kmmo:$KERNEL_FULL_VERSION build: dockerfileConfigMap: name: intel-dgpu-dockerfile-configmap selector: intel.feature.node.kubernetes.io/gpu: \"true\" This file will use the above Containerfile to build the module and store the image in the repository. Note the selector field. It has been modified to use NFD for discovery and only load the kernel module where needed.\nWe can check that the kernel module has been loaded by connecting to the node and checking the status:\nsh-5.1# lsmod|grep i915 i915 3977216 0 intel_vsec 20480 1 i915 compat 24576 2 intel_vsec,i915 video 61440 1 i915 drm_display_helper 172032 2 compat,i915 cec 61440 2 drm_display_helper,i915 i2c_algo_bit 16384 2 ast,i915 drm_kms_helper 192512 5 ast,drm_display_helper,i915 drm 581632 7 drm_kms_helper,compat,ast,drm_shmem_helper,drm_display_helper,i915 Alternatively, we can check the label added by KMM on the nodes:\n$ oc describe node |grep kmm kmm.node.kubernetes.io/openshift-kmm.intel-dgpu-on-premise.ready= Once the kernel module is deployed, use an application to verify HW acceleration is provided by the GPU.\nNote\nHere we‚Äôre directly using the /dev filesystem for accessing the GPU. The recommended way is to use the Intel Device Plugins Operator and then add a CR to expose gpu.intel.com/i915 to the kubelet for workload consumption as described in the repository.\nVerify the deployment Simple approach We‚Äôll be using clinfo to get information from our card. To do so, we‚Äôll create the image and the namespace and then run the utility inside a privileged pod similar to the application that we can use as a more complex approach.\nLet‚Äôs create the BuildConfiguration and ImageStream based on this one by creating a clinfobuild.yaml:\napiVersion: image.openshift.io/v1 kind: ImageStream metadata: name: intel-dgpu-clinfo namespace: openshift-kmm spec: {} --- apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: intel-dgpu-clinfo namespace: openshift-kmm spec: output: to: kind: ImageStreamTag name: intel-dgpu-clinfo:latest runPolicy: Serial source: dockerfile: \"ARG BUILDER=registry.access.redhat.com/ubi9-minimal:latest \\nFROM ${BUILDER} \\n\\nARG OCL_ICD_VERSION=ocl-icd-2.2.13-4.el9.x86_64\\nARG CLINFO_VERSION=clinfo-3.0.21.02.21-4.el9.x86_64\\n\\nRUN microdnf install -y \\\\\\n glibc \\\\\\n yum-utils \\n\\n# install intel-opencl, ocl-icd and clinfo\\nRUN dnf install -y 'dnf-command(config-manager)' \u0026amp;\u0026amp; \\\\\\n \\ dnf config-manager --add-repo https://repositories.intel.com/gpu/rhel/9.0/lts/2350/unified/intel-gpu-9.0.repo \u0026amp;\u0026amp; \\\\\\n dnf install -y intel-opencl \\\\\\n https://mirror.stream.centos.org/9-stream/AppStream/x86_64/os/Packages/$OCL_ICD_VERSION.rpm \\ \\\\\\n https://dl.fedoraproject.org/pub/epel/9/Everything/x86_64/Packages/c/$CLINFO_VERSION.rpm \u0026amp;\u0026amp; \\\\\\n dnf clean all \u0026amp;\u0026amp; dnf autoremove \u0026amp;\u0026amp; rm -rf /var/lib/dnf/lists/* \u0026amp;\u0026amp; \\\\\\n \\ rm -rf /etc/yum.repos.d/intel-graphics.repo \\n\" type: Dockerfile strategy: dockerStrategy: buildArgs: - name: BUILDER value: registry.access.redhat.com/ubi9-minimal:latest - name: OCL_ICD_VERSION value: ocl-icd-2.2.13-4.el9.x86_64 - name: CLINFO_VERSION value: clinfo-3.0.21.02.21-4.el9.x86_64 type: Docker triggers: - type: ConfigChange Let‚Äôs apply it with:\n$ oc create -f clinfobuild.yaml And then define the privileged pod that will run the tool with this pod defined by job.yaml:\napiVersion: v1 kind: Pod metadata: name: intel-dgpu-clinfo spec: containers: - name: clinfo-pod image: image-registry.openshift-image-registry.svc:5000/openshift-kmm/intel-dgpu-clinfo:latest command: [\"clinfo\"] resources: securityContext: privileged: true runAsUser: 0 runAsGroup: 110 volumeMounts: - name: dev mountPath: /dev volumes: - name: dev hostPath: path: /dev Let‚Äôs create the pod with:\n$ oc create -f job.yaml And then examine the output of the pod by running:\n$ oc logs pod intel-dgpu-clinfo Number of platforms 1 Platform Name Intel(R) OpenCL HD Graphics Platform Vendor Intel(R) Corporation Platform Version OpenCL 3.0 Platform Profile FULL_PROFILE ... ... Platform Name Intel(R) OpenCL HD Graphics Number of devices 1 Device Name Intel(R) Data Center GPU Flex Series 140 [0x56c1] Device Vendor Intel(R) Corporation Device Vendor ID 0x8086 Device Version OpenCL 3.0 NEO ... ... Platform Name Intel(R) OpenCL HD Graphics Device Name Intel(R) Data Center GPU Flex Series 140 [0x56c1] ICD loader properties ICD loader Name OpenCL ICD Loader ICD loader Vendor OCL Icd free software ICD loader Version 2.2.12 ICD loader Profile OpenCL 2.2 NOTE: your OpenCL library only supports OpenCL 2.2, but some installed platforms support OpenCL 3.0. Programs using 3.0 features may crash or behave unexpectedly Using OpenVINO application for image text to image with stable diffusion An application using Intel‚Äôs OpenVINO software will be used to showcase the functionality of the GPU acceleration in the processing that will use some keywords introduced to generate images.\nRequirements In the following paragraphs we‚Äôll be covering the requirements that we‚Äôll be preparing for the final step of validating the proper setup of our driver.\nImageStream We need to define an ImageStream to store our container with this YAML:\napiVersion: image.openshift.io/v1 kind: ImageStream metadata: labels: app: jupyter-demo name: jupyter-demo namespace: openshift-kmm spec: {} Containerfile First prepare the container image containing all the required bits and pieces for storing in a registry for later use with this BuildConfig:\nkind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: \"jupyter-demo\" spec: source: dockerfile: | FROM quay.io/jupyter/base-notebook USER root RUN apt update \u0026amp;\u0026amp; \\ apt install -y gpg-agent git wget \u0026amp;\u0026amp; \\ apt clean RUN wget -qO - https://repositories.intel.com/gpu/intel-graphics.key | gpg --dearmor --output /usr/share/keyrings/intel-graphics.gpg RUN echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy/production/2328 unified\" \u0026gt; /etc/apt/sources.list.d/intel-gpu-jammy.list RUN apt update \u0026amp;\u0026amp; \\ apt install -y \\ intel-opencl-icd intel-level-zero-gpu level-zero \\ intel-media-va-driver-non-free libmfx1 libmfxgen1 libvpl2 \\ libegl-mesa0 libegl1-mesa libegl1-mesa-dev libgbm1 libgl1-mesa-dev libgl1-mesa-dri \\ libglapi-mesa libgles2-mesa-dev libglx-mesa0 libigdgmm12 libxatracker2 mesa-va-drivers \\ mesa-vdpau-drivers mesa-vulkan-drivers va-driver-all vainfo hwinfo clinfo \\ libglib2.0-0 \u0026amp;\u0026amp; \\ apt clean USER jovyan RUN pip install --no-cache-dir \"diffusers\u0026gt;=0.14.0\" \"openvino\u0026gt;=2023.3.0\" \"transformers \u0026gt;= 4.31\" accelerate \"urllib3==1.26.15\" ipywidgets opencv-python scipy RUN mkdir -p /home/jovyan/.cache/huggingface strategy: type: Docker output: to: kind: ImageStreamTag name: jupyter-demo:latest Note\nIn the above BuildConfig, there might be newer versions of the software installed by pip. It may be necessary to update and use a newer version. We‚Äôre going to use OpenVINO project notebooks, which already execute some pip commands to install required libraries in any case.\nLet‚Äôs now start the build of the image with:\n$ oc start-build jupyter-demo And once it‚Äôs finished, we can check that the image appears with:\n$ oc get is NAME IMAGE REPOSITORY TAGS UPDATED jupyter-demo image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo latest 2 minutes ago If we want to check the builds with oc logs -f build/\u0026lt;\u0026lt;buildname\u0026gt;, we‚Äôll see an output similar to this one:\n$ oc logs -f build/jupyter-demo-1 time=\"2024-03-13T12:50:47Z\" level=info msg=\"Not using native diff for overlay, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled\" I0313 12:50:47.551349 1 defaults.go:112] Defaulting to storage driver \"overlay\" with options [mountopt=metacopy=on]. Caching blobs under \"/var/cache/blobs\". Pulling image quay.io/jupyter/base-notebook ... Trying to pull quay.io/jupyter/base-notebook:latest... Getting image source signatures ... ... STEP 9/11: RUN mkdir -p /home/jovyan/.cache/huggingface --\u0026gt; 284cd3e642a7 STEP 10/11: ENV \"OPENSHIFT_BUILD_NAME\"=\"jupyter-demo-1\" \"OPENSHIFT_BUILD_NAMESPACE\"=\"openshift-kmm\" --\u0026gt; 9bba674b8144 STEP 11/11: LABEL \"io.openshift.build.name\"=\"jupyter-demo-1\" \"io.openshift.build.namespace\"=\"openshift-kmm\" COMMIT temp.builder.openshift.io/openshift-kmm/jupyter-demo-1:d93bad68 --\u0026gt; c738f8d15e38 Successfully tagged temp.builder.openshift.io/openshift-kmm/jupyter-demo-1:d93bad68 c738f8d15e38ea41f9a17082a435b4e8badf2e7d0569f34c332cf09102b0992d Pushing image image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo:latest ... Getting image source signatures Copying blob sha256:c37f7a4129892837c4258c045d773d933f9307d7dcf6801d80a2903c38e7936c ... ... sha256:59ebd409476f3946cadfccbea9e851574c50b8ef6959f62bdfa2dd708423da30 Copying config sha256:c738f8d15e38ea41f9a17082a435b4e8badf2e7d0569f34c332cf09102b0992d Writing manifest to image destination Successfully pushed image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo@sha256:f9ee5ae8fa9db556e90908b278c7ebb2d2ad271e11da82cfad44620d65834bf8 Push successful We need to define a StorageClass to use an LVM for the underlying storage. Then, a volume creation with persistent storage (PVC) is registered so that the space is allocated and prepared for our application and a secondary one for the cache of the application.\nWe‚Äôll use that space later on to download the Jupyter notebooks that we‚Äôll be using for the GPU demonstration.\nFor the following items, apply each one with:\n$ oc apply -f \u0026lt;file.yaml\u0026gt; StorageClass Let‚Äôs create a file with the following contents defining our Storage Class named sc.yaml\nallowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: lvms-vg1 parameters: csi.storage.k8s.io/fstype: xfs topolvm.io/device-class: vg1 provisioner: topolvm.io reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer Persistent Volume Claim Similarly, we need to create a file for registering the storage that we‚Äôll be naming pvc.yaml with the following contents:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc spec: storageClassName: lvms-vg1 accessModes: - ReadWriteOnce resources: requests: storage: 20Gi PVC for cache Finally, the application uses some cache, so another PVC will be created using a file named cache.yaml with the following contents:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: huggingface-cache spec: storageClassName: lvms-vg1 accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Make sure you‚Äôve applied the above files as instructed before following from this point.\nAt this point, we‚Äôve created the Build, ImageStream, StorageClass, PVC, and PVC for Cache; because we launched the build, we will also have the ImageStream populated, so we‚Äôre ready to run the application.\nRunning the application A Jupyter notebook is an interactive workbook where commands, outputs like visualizations, etc., can be shown alongside the code.\nIt‚Äôs commonly used in data science analysis as it allows you to quickly edit and amend the commands and refresh the output with the new values.\nUsing the image generated in the previous Containerfile and the previous storage PVCs, we can create a pod with this YAML:\napiVersion: v1 kind: Pod metadata: name: kmm-demo-jupyter spec: containers: - name: kmm-demo-jupyter image: image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo args: # Password is paella in case you want to reuse - start-notebook.py - --PasswordIdentityProvider.hashed_password='argon2:$argon2id$v=19$m=10240,t=10,p=8$00Ynt8+Jk4sMtJUM+7Us5Q$ycb5PzmA7IH9yfOPAIfUjMNvDzXHKiMXPvM6+R5nucQ' env: - name: GRANT_SUDO value: \"yes\" - name: NB_GID value: \"110\" ports: - containerPort: 8888 hostPort: 8888 resources: limits: cpu: \"20\" memory: \"64Gi\" securityContext: privileged: true runAsUser: 0 runAsGroup: 110 volumeMounts: - name: dev mountPath: /dev - name: huggingface-cache mountPath: /home/jovyan/.cache/huggingface - name: work mountPath: /home/jovyan/work volumes: - name: dev hostPath: path: /dev - name: huggingface-cache persistentVolumeClaim: claimName: huggingface-cache - name: work persistentVolumeClaim: claimName: pvc Once the app is ready we can forward a local port (for example, to reach the application), but first we‚Äôll prepare the examples we‚Äôll be using with the Jupyter notebooks from the OpenVINO project by getting inside our pod with:\n$ oc rsh kmm-demo-jupyter ‚Ä¶then, once we‚Äôre inside the pod:\n$ pwd /home/jovyan $ ls work $ git clone https://github.com/openvinotoolkit/openvino_notebooks chown -R jovyan:users openvino_notebooks Note\nIf you don‚Äôt specify an authentication method on the pod, a token will be printed on the pod logs, that should be used when reaching the Jupyter interface.\nUsing the following command, we‚Äôll be forwarding a port from your computer to the pod itself so it will be easier to interact with the Jupyter notebook running there:\n$ oc port-forward kmm-demo-jupyter 8888:8888 Once done, on your local computer browser open http://localhost:8888 to access the notebook.\nIn the example above, the hashed password is paella, and it‚Äôs the one we‚Äôll be using to access the Jupyter notebook.\nFrom within the browser, you can access the previous URL, navigate the notebooks, and select 225-stable-diffusion-text-to-image so that the final URL is:\nhttp://localhost:8888/lab/tree/openvino_notebooks/notebooks/225-stable-diffusion-text-to-image/225-stable-diffusion-text-to-image.ipynb\nSkip over the explanation steps and navigate to the area with the GPU selection drop-down (Figure 1).\nFigure 1: GPU selection dropdown\n‚Ä¶and later, the keyword section (Figure 2).\nFigure 2: keyword selection for imput\nIn this section, you can describe what kind of image should be generated. This is where the real magic happens. In this case, we will use the prompt Valencia fallas sunny day and see what kind of image is generated. See Figure 3.\nFigure 3: Output of the pipeline for the initial set of words Of course, you can go back, edit the input keywords, and try new ones (Figure 4)\nFigure 4: Output of the pipeline for the additional set of words\nWrap up We hope that you‚Äôve enjoyed this read and realize how KMM, NFD, and DTK make managing custom drivers across many nodes across a cluster much easier than having to log into each node individually to install drivers.\nWe hope that you‚Äôve enjoyed this read and realize how quick and convenient it is to use KMM, NFD, and DTK to enable support for accelerators in your OpenShift infrastructure.\nBut I do still want to upgrade my cluster! Don‚Äôt worry, KMM automatically will check the modules configured for your hosts and the kernel they are running‚Äîif you‚Äôre using the prebuilt images, KMM will download and enable the prebuilt image and if you‚Äôre using a custom build, a new build process will happen so that the new image is available‚Ä¶ and using the labels added by KMM you can schedule your workloads on the nodes that have the driver ready for consumption.\nIt‚Äôs still recommended to do a staggered upgrade, so only a few nodes are updated before moving into others to avoid, for example, a new kernel having some issues with the build process or no prebuilt driver being available because it is still under the certification and validation process‚Ä¶ rendering workloads requiring a specific device driver to become unschedulable.\nOnce you‚Äôve checked that the driver is available at the preceding link and checked the official documentation on the OpenShift Upgrade process, be ready!\nEnjoy! (and if you do, you can Buy Me a Coffee ) ","wordCount":"4274","inLanguage":"en","image":"https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/cover.png","datePublished":"2024-04-26T00:00:00.007Z","dateModified":"2025-02-12T09:32:52.143Z","author":{"@type":"Person","name":"Pablo Iranzo G√≥mez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://iranzo.io/blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/"},"publisher":{"@type":"Organization","name":"Pablo Iranzo G√≥mez blog","logo":{"@type":"ImageObject","url":"https://iranzo.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://iranzo.io/ accesskey=h title="Home (Alt + H)"><img src=https://iranzo.io/apple-icon-152x152.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://iranzo.io/es/ title=Spanish aria-label=Spanish>Es</a></li></ul></div></div><ul id=menu><li><a href=https://iranzo.io/ title=üè†Home><span>üè†Home</span></a></li><li><a href=https://iranzo.io/about/ title=üóíÔ∏èAbout><span>üóíÔ∏èAbout</span></a></li><li><a href=https://iranzo.io/redken_bot/ title=üêòredken_bot><span>üêòredken_bot</span></a></li><li><a href=https://iranzo.io/projects/ title=üìêProjects><span>üìêProjects</span></a></li><li><a href=https://iranzo.io/archives/ title="üóÑÔ∏è Archives"><span>üóÑÔ∏è Archives</span></a></li><li><a href=https://iranzo.io/categories/ title=üó∫Ô∏èCategories><span>üó∫Ô∏èCategories</span></a></li><li><a href=https://iranzo.io/tags/ title=üè∑Ô∏èTags><span>üè∑Ô∏èTags</span></a></li><li><a href=https://iranzo.io/search title="üîéSearch (Alt + /)" accesskey=/><span>üîéSearch</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://iranzo.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://iranzo.io/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">Enable GPU acceleration with the Kernel Module Management Operator</h1><div class=post-meta><span title='2024-04-26 00:00:00.007 +0000 UTC'>April 26, 2024</span>&nbsp;¬∑&nbsp;21 min&nbsp;¬∑&nbsp;Pablo Iranzo G√≥mez</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#what-is-the-kernel-module-management-operator aria-label="What is the Kernel Module Management Operator?">What is the Kernel Module Management Operator?</a></li><li><a href=#prerequisites aria-label=Prerequisites>Prerequisites</a></li><li><a href=#set-up-node-feature-discovery-operator aria-label="Set up Node Feature Discovery Operator">Set up Node Feature Discovery Operator</a></li><li><a href=#advanced-node-labeling aria-label="Advanced node labeling">Advanced node labeling</a></li><li><a href=#set-up-kernel-module-management-operator aria-label="Set up Kernel Module Management Operator">Set up Kernel Module Management Operator</a></li><li><a href=#create-a-kernel-module aria-label="Create a kernel module">Create a kernel module</a><ul><li><a href=#integration-with-nfd-via-crds aria-label="Integration with NFD via CRDs">Integration with NFD via CRDs</a></li><li><a href=#kmm-dependencies aria-label="KMM dependencies">KMM dependencies</a></li><li><a href=#replace-an-in-tree-module-with-an-out-of-tree-module aria-label="Replace an in-tree module with an out-of-tree module">Replace an in-tree module with an out-of-tree module</a></li><li><a href=#configure-the-driver-toolkit-for-image-building aria-label="Configure the Driver Toolkit for image building">Configure the Driver Toolkit for image building</a></li><li><a href=#manage-heterogeneous-nodes-in-the-cluster aria-label="Manage heterogeneous nodes in the cluster">Manage heterogeneous nodes in the cluster</a></li></ul></li><li><a href=#enable-the-intel-data-center-gpu-flex-140 aria-label="Enable the Intel Data Center GPU Flex 140">Enable the Intel Data Center GPU Flex 140</a><ul><li><a href=#intel-data-center-gpu-flex-140-kernel-module aria-label="Intel Data Center GPU Flex 140 kernel module">Intel Data Center GPU Flex 140 kernel module</a><ul><li><a href=#using-prebuilt-drivers aria-label="Using prebuilt drivers">Using prebuilt drivers</a></li><li><a href=#compiling-your-own-driver aria-label="Compiling your own driver">Compiling your own driver</a></li></ul></li><li><a href=#verify-the-deployment aria-label="Verify the deployment">Verify the deployment</a><ul><li><a href=#simple-approach aria-label="Simple approach">Simple approach</a></li><li><a href=#using-openvino-application-for-image-text-to-image-with-stable-diffusion aria-label="Using OpenVINO application for image text to image with stable diffusion">Using OpenVINO application for image text to image with stable diffusion</a><ul><li><a href=#requirements aria-label=Requirements>Requirements</a><ul><li><a href=#imagestream aria-label=ImageStream>ImageStream</a></li><li><a href=#containerfile aria-label=Containerfile>Containerfile</a></li><li><a href=#storageclass aria-label=StorageClass>StorageClass</a></li><li><a href=#persistent-volume-claim aria-label="Persistent Volume Claim">Persistent Volume Claim</a></li><li><a href=#pvc-for-cache aria-label="PVC for cache">PVC for cache</a></li></ul></li><li><a href=#running-the-application aria-label="Running the application">Running the application</a></li></ul></li></ul></li></ul></li><li><a href=#wrap-up aria-label="Wrap up">Wrap up</a></li><li><a href=#but-i-do-still-want-to-upgrade-my-cluster aria-label="But I do still want to upgrade my cluster!">But I do still want to upgrade my cluster!</a></li></ul></div></details></div><div class=post-content><div class="admonition attention"><p class=admonition-title>Attention</p><p class=admonition>This post was originally published on Red Hat Developer, the community to learn, code, and share faster. To read the original post, <a href=https://developers.redhat.com/articles/2024/04/05/enable-gpu-acceleration-kernel-module-management-operator>click here</a>.</p></div><p><a href=https://tv.redhat.com/detail/6352250333112/enable-gpu-acceleration-with-the-kernel-module-management-operator>Check the video version for Red Hat TV</a></p><p>In this article we cover the required steps to configure Kernel Module Management Operator (KMM) and use it to deploy an out-of-tree (OOT) kernel module, as well as leveraging other related technologies to build a toolset for hardware enablement. To illustrate that process, we&rsquo;ll leverage the <a href=https://www.intel.com/content/www/us/en/products/sku/230020/intel-data-center-gpu-flex-140/specifications.html>Intel Data Center GPU Flex 140</a>.</p><h2 id=what-is-the-kernel-module-management-operator>What is the Kernel Module Management Operator?<a hidden class=anchor aria-hidden=true href=#what-is-the-kernel-module-management-operator>#</a></h2><p>The Kernel Module Management Operator manages, builds, signs, and deploys out-of-tree (OOT) kernel modules and device plug-ins on <a href=https://developers.redhat.com/products/openshift/overview>Red Hat OpenShift Container Platform</a> clusters.</p><p>Before KMM, cluster admins had to manually install drivers to multiple nodes. Upgrades were painful and prone to errors from incompatible drivers. Furthermore, workloads might get scheduled to a node with broken drivers causing scheduling issues or missing hardware. KMM solves all of these problems, as we&rsquo;ll see.</p><p>KMM is designed to accommodate multiple kernel versions at once for any kernel module, allowing for seamless node upgrades and reduced application downtime. For more information, refer to the <a href=https://docs.openshift.com/container-platform/latest/hardware_enablement/kmm-kernel-module-management.html>Kernel Module Management Operator product documentation</a>.</p><p>KMM is also a <a href=https://github.com/kubernetes-sigs/kernel-module-management>community project</a>, which you can test on upstream Kubernetes, and there is a <a href=https://kubernetes.slack.com/archives/C037RE58RED>Slack community channel</a>.</p><h2 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h2><p>For this scenario, we&rsquo;ll require an already working OpenShift environment as we will use it to deploy the different tools on top. Check <a href=https://docs.openshift.com/container-platform/4.14/installing/index.html%22>the documentation</a> for instructions.</p><p>KMM will require a registry to push images to. If you&rsquo;ve installed on bare metal, ensure the internal registry is enabled and configured (refer to <a href=https://docs.openshift.com/container-platform/4.14/installing/installing_bare_metal/installing-bare-metal.html#installation-registry-storage-config_installing-bare-metal%22>Installing a user-provisioned cluster on bare metal</a>).</p><p>Additionally, this tutorial references data available from Intel at the following locations:</p><ul><li><a href=https://github.com/intel/intel-data-center-gpu-driver-for-openshift>Intel Data Center GPU Driver for OpenShift</a></li><li><a href=https://github.com/intel/intel-technology-enabling-for-openshift>Intel Technology Enabling for OpenShift</a></li></ul><h2 id=set-up-node-feature-discovery-operator>Set up Node Feature Discovery Operator<a hidden class=anchor aria-hidden=true href=#set-up-node-feature-discovery-operator>#</a></h2><p>Node Feature Discovery (NFD) detects hardware features available on nodes and advertises those features using nodes labels, so that they can later be used as selector for scheduling decisions.</p><p>The NFD Operator automatically adds labels to the nodes that present some characteristics, including if the node has a GPU and which GPU it has.</p><p>It&rsquo;s an ideal way to identify which nodes require a kernel module to be enabled for the specific node(s) and later use it to instruct KMM to build it only for those.</p><p>We can install it via the following YAML:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Namespace</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>openshift-nfd</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>operators.coreos.com/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>OperatorGroup</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>openshift-nfd</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-nfd</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>targetNamespaces</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>openshift-nfd</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>operators.coreos.com/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Subscription</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfd</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-nfd</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>channel</span>: <span style=color:#e6db74>&#34;stable&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>installPlanApproval</span>: <span style=color:#ae81ff>Automatic</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfd</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>source</span>: <span style=color:#ae81ff>redhat-operators</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>sourceNamespace</span>: <span style=color:#ae81ff>openshift-marketplace</span>
</span></span></code></pre></div><p>Once installed, we can create a CRD as described in the <a href=https://docs.openshift.com/container-platform/latest/hardware_enablement/psap-node-feature-discovery-operator.html#create-cd-cli_node-feature-discovery-operator%22>NFD Operator documentation</a> to start.</p><p>Create a file named <code>nfdcr.yaml</code> with the following contents:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>nfd.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>NodeFeatureDiscovery</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfd-instance</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-nfd</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>operand</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>quay.io/openshift/origin-node-feature-discovery:4.14</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>imagePullPolicy</span>: <span style=color:#ae81ff>Always</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>servicePort</span>: <span style=color:#ae81ff>12000</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>workerConfig</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>configData</span>: <span style=color:#ae81ff>|</span>
</span></span></code></pre></div><p>Then apply it with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>oc apply -f nfdcr.yaml
</span></span></code></pre></div><p>Additionally, create a <a href=https://github.com/intel/intel-technology-enabling-for-openshift/blob/main/nfd/node-feature-rules-openshift.yaml%22>nodefeaturerule.yaml</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>nfd.openshift.io/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>NodeFeatureRule</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dp-devices</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#e6db74>&#34;intel.gpu&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;intel.feature.node.kubernetes.io/gpu&#34;: </span><span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>matchFeatures</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>feature</span>: <span style=color:#ae81ff>pci.device</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>matchExpressions</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>vendor</span>: { <span style=color:#f92672>op: In, value</span>: [<span style=color:#e6db74>&#34;8086&#34;</span>] }
</span></span><span style=display:flex><span>            <span style=color:#f92672>class</span>: { <span style=color:#f92672>op: In, value</span>: [<span style=color:#e6db74>&#34;0300&#34;</span>, <span style=color:#e6db74>&#34;0380&#34;</span>] }
</span></span></code></pre></div><p>And once it has been applied, <code>oc describe node</code> displays the labels applied. Below is a partial output of the labels that NFD applies:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>Labels:             beta.kubernetes.io/arch=amd64
</span></span><span style=display:flex><span>                    beta.kubernetes.io/os=linux
</span></span><span style=display:flex><span>                    feature.node.kubernetes.io/cpu-cpuid.ADX=true
</span></span><span style=display:flex><span>                    feature.node.kubernetes.io/cpu-cpuid.AESNI=true
</span></span><span style=display:flex><span>                    feature.node.kubernetes.io/cpu-cpuid.AMXINT8=true
</span></span><span style=display:flex><span>                    feature.node.kubernetes.io/cpu-cpuid.AMXTILE=true
</span></span><span style=display:flex><span>                    feature.node.kubernetes.io/cpu-cpuid.AVX=true
</span></span><span style=display:flex><span>                    feature.node.kubernetes.io/cpu-cpuid.AVX2=true
</span></span><span style=display:flex><span>‚Ä¶
</span></span></code></pre></div><p>As we can see, it starts showing some of the CPU flags among other values as labels in our system. We can then grep for Intel-specific GPU labels with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc describe node rhvp-intel-01|grep intel.feature
</span></span><span style=display:flex><span>intel.feature.node.kubernetes.io/gpu<span style=color:#f92672>=</span>true
</span></span></code></pre></div><p>&mldr;where we can see that NFD has detected the GPU.</p><p>At this point, NFD will take care of our node&rsquo;s HW detection and labeling, which can later be used as node selectors for KMM to deploy the required modules.</p><h2 id=advanced-node-labeling>Advanced node labeling<a hidden class=anchor aria-hidden=true href=#advanced-node-labeling>#</a></h2><p>For this use case, the Intel Data Center GPU Driver for OpenShift (i915) driver is only available and tested for <a href=https://github.com/intel/intel-data-center-gpu-driver-for-openshift/tree/main/release#intel-data-center-gpu-driver-container-images-for-openshift-release%22>some kernel versions</a>.</p><p>Using NFD labels, we can target specific custom kernel versions for our module deployment and enablement so that only hosts with the required kernel and the required hardware are enabled for driver activation. This ensures that only compatible drivers are installed on nodes with a supported kernel, which is what makes KMM so valuable.</p><p>In this case, we&rsquo;ll be using nodes that contain the <code>intel.feature.node.kubernetes.io/gpu</code> tag.</p><p>Let&rsquo;s move on to the next steps in our journey, installing KMM and building the kernel module with Driver Toolkit (DTK). DTK is a <a href=https://developers.redhat.com/topics/containers%22>container</a> image used as a base image where drivers can be built, as it includes kernel packages, some tools, etc.</p><h2 id=set-up-kernel-module-management-operator>Set up Kernel Module Management Operator<a hidden class=anchor aria-hidden=true href=#set-up-kernel-module-management-operator>#</a></h2><p>Install KMM using OperatorHub in our OpenShift Console or the following <code>kmm.yaml</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Namespace</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>operators.coreos.com/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>OperatorGroup</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kernel-module-management</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>operators.coreos.com/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Subscription</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kernel-module-management</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>channel</span>: <span style=color:#e6db74>&#34;stable&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>installPlanApproval</span>: <span style=color:#ae81ff>Automatic</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kernel-module-management</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>source</span>: <span style=color:#ae81ff>redhat-operators</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>sourceNamespace</span>: <span style=color:#ae81ff>openshift-marketplace</span>
</span></span></code></pre></div><p>With:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>oc apply -f kmm.yaml
</span></span></code></pre></div><p>Once it&rsquo;s done, we can switch to the openshift-kmm project:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>oc project openshift-kmm
</span></span></code></pre></div><h2 id=create-a-kernel-module>Create a kernel module<a hidden class=anchor aria-hidden=true href=#create-a-kernel-module>#</a></h2><p>As mentioned previously, KMM can perform the compilation and installation of kernel module drivers for our hardware.</p><p>A kernel module can:</p><ul><li>Have dependencies</li><li>Replace an existing driver</li></ul><p>Let&rsquo;s explore the use cases in the next sections of this article.</p><h3 id=integration-with-nfd-via-crds>Integration with NFD via CRDs<a hidden class=anchor aria-hidden=true href=#integration-with-nfd-via-crds>#</a></h3><p>KMM uses a kmod (kernel module) image to define which kernel modules to load, which is an OCI image that contains the <code>.ko</code> files.</p><p>In <code>.spec.selector</code>, we can define which nodes should be selected and as we showcased earlier, we can target one specific label added by NFD, so that only those nodes are targeted for loading the module for the hardware installed.</p><h3 id=kmm-dependencies>KMM dependencies<a hidden class=anchor aria-hidden=true href=#kmm-dependencies>#</a></h3><p>Adding a module might have additional module dependencies, that is, extra modules that need to be already loaded in the kernel.</p><p>We can use the Custom Resource Definition (CRD) field <code>.spec.moduleLoader.container.modprobe.modulesLoadingOrder</code> to identify the order for module loading starting with upmost module, then the module it depends on, and so on.</p><h3 id=replace-an-in-tree-module-with-an-out-of-tree-module>Replace an in-tree module with an out-of-tree module<a hidden class=anchor aria-hidden=true href=#replace-an-in-tree-module-with-an-out-of-tree-module>#</a></h3><p>Similar to dependencies, sometimes the module being loaded conflicts with an already loaded kernel module.</p><p>In this case, we need to have KMM first remove the conflicting module via the <code>.spec.moduleLoader.container.inTreeModuleToRemove</code> field of the CRD. KMM will then proceed and load the newer OOT module.</p><p>For the Intel Data Center GPU Flex series, the <code>intel_vsec</code> and <code>i915</code> drivers will have to be removed which will be discussed later in this article.</p><h3 id=configure-the-driver-toolkit-for-image-building>Configure the Driver Toolkit for image building<a hidden class=anchor aria-hidden=true href=#configure-the-driver-toolkit-for-image-building>#</a></h3><p>The <a href=https://github.com/openshift/driver-toolkit>Driver Toolkit</a>) provides a base image with required kernel development packages that are used to build specific drivers for our platform, which match the kernel version used on each node where the accelerators exist.</p><p>By using a specially crafted Containerfile containing a reference to <code>DTK_AUTO</code> as shown below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#66d9ef>ARG</span> DTK_AUTO<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> ${DTK_AUTO}</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> gcc <span style=color:#ae81ff>\.</span>.. |<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><p>The KMM Operator will replace the required variables as well as pull and build the image for the driver.</p><p>And that&rsquo;s all. Easy, right?</p><h3 id=manage-heterogeneous-nodes-in-the-cluster>Manage heterogeneous nodes in the cluster<a hidden class=anchor aria-hidden=true href=#manage-heterogeneous-nodes-in-the-cluster>#</a></h3><p>As we&rsquo;re using labels for selecting specific nodes in our cluster, we can keep a mix of nodes with or without the hardware in our cluster. KMM will take care of loading the required modules on the matching nodes, leaving the other ones without the specific accelerator.</p><p>In our case we&rsquo;re using <code>intel.feature.node.kubernetes.io/gpu=true</code> as the label to match our intended nodes, leaving other nodes without the GPU affected.</p><h2 id=enable-the-intel-data-center-gpu-flex-140>Enable the Intel Data Center GPU Flex 140<a hidden class=anchor aria-hidden=true href=#enable-the-intel-data-center-gpu-flex-140>#</a></h2><p>We&rsquo;re going to explore the step-by-step the process for detecting, configuring, and enabling an Intel GPU in our OpenShift environment.</p><p>One of our workers has the accelerator installed, as reported by <code>lspci | egrep 'Graphic|Display</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>02:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 52)
</span></span><span style=display:flex><span>a0:00.0 Display controller: Intel Corporation Data Center GPU Flex 140 (rev 05)
</span></span></code></pre></div><p>Let&rsquo;s create a <code>MachineConfigPool</code> (MCP) to apply the configuration in our environment:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>machineconfiguration.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>MachineConfigPool</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>machineConfigSelector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchExpressions</span>:
</span></span><span style=display:flex><span>      - {
</span></span><span style=display:flex><span>          <span style=color:#f92672>key</span>: <span style=color:#ae81ff>machineconfiguration.openshift.io/role,</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>operator</span>: <span style=color:#ae81ff>In,</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>values</span>: [<span style=color:#ae81ff>worker, intel-dgpu, master],</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>  <span style=color:#f92672>nodeSelector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>intel.feature.node.kubernetes.io/gpu</span>: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span></code></pre></div><div class="admonition note"><p class=admonition-title>Note</p><p class=admonition>If you&rsquo;re using single node OpenShift for this test, remember that the YAMLs must be adapted so that the configuration via MCO applies to the primary MCP; that is, using the selector <code>machineconfiguration.openshift.io/role: master</code>.</p></div><p>Using a machine configuration, we can define new parameters, like for example disable the built-in drivers with this YAML:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>machineconfiguration.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>MachineConfig</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>machineconfiguration.openshift.io/role</span>: <span style=color:#ae81ff>intel-dgpu</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>100</span>-<span style=color:#ae81ff>intel-dgpu-machine-config-disable-i915</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>config</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>ignition</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>version</span>: <span style=color:#ae81ff>3.2.0</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>storage</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>files</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>contents</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>source</span>: <span style=color:#ae81ff>data:,blacklist%20i915</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mode</span>: <span style=color:#ae81ff>0644</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>overwrite</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/etc/modprobe.d/blacklist-i915.conf</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>contents</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>source</span>: <span style=color:#ae81ff>data:,blacklist%20intel_vsec</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mode</span>: <span style=color:#ae81ff>0644</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>overwrite</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/etc/modprobe.d/blacklist-intel-vsec.conf</span>
</span></span></code></pre></div><p>After this YAML is applied, we can check that there are no modules applied via <code>oc debug node/rhvp-intel-01</code> (NOTE: if you see <code>intel_vsec</code> or <code>i915</code> in the output, verify that the <code>MachineConfig</code> defined was correctly applied):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ lsmod|egrep <span style=color:#e6db74>&#39;i915|vsec&#39;</span>
</span></span></code></pre></div><p>Now, we need to define the path to find the firmware for the module, and for this, there are two approaches; the first one is an MCO that patches the kernel command line (and will cause a reboot of the node) as configured with the following YAML:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>machineconfiguration.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>MachineConfig</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>machineconfiguration.openshift.io/role</span>: <span style=color:#ae81ff>master</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>100</span>-<span style=color:#ae81ff>alternative-fw-path-for-master-nodes</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>config</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>ignition</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>version</span>: <span style=color:#ae81ff>3.2.0</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>kernelArguments</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>firmware_class.path=/var/lib/firmware</span>
</span></span></code></pre></div><p>And for validating, we can check on the hosts via <code>oc debug node/rhvp-intel-01</code> that the new parameter has been enabled:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ cat /proc/cmdline
</span></span><span style=display:flex><span>BOOT_IMAGE<span style=color:#f92672>=(</span>hd5,gpt3<span style=color:#f92672>)</span>/ostree/rhcos-085fdd39288474060c9d5bd7a88fabe8d218fcc960186712834c5e4ab319cb1d/vmlinuz-5.14.0-284.52.1.el9_2.x86_64 ignition.platform.id<span style=color:#f92672>=</span>metal ostree<span style=color:#f92672>=</span>/ostree/boot.0/rhcos/085fdd39288474060c9d5bd7a88fabe8d218fcc960186712834c5e4ab319cb1d/0 ip<span style=color:#f92672>=</span>ens801f0:dhcp,dhcp6 root<span style=color:#f92672>=</span>UUID<span style=color:#f92672>=</span>bf0c9edf-4aab-48a8-9549-5005fff7890e rw rootflags<span style=color:#f92672>=</span>prjquota boot<span style=color:#f92672>=</span>UUID<span style=color:#f92672>=</span>282ee60b-3053-4c2e-8f92-612af621e245 firmware_class.path<span style=color:#f92672>=</span>/var/lib/firmware systemd.unified_cgroup_hierarchy<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> cgroup_no_v1<span style=color:#f92672>=</span>all psi<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>The other approach is to patch the configuration for the KMM operator.</p><p>Alternatively, we can modify the <code>configmap</code> by setting the path for the firmware which doesn&rsquo;t cause a reboot, which is useful in single node OpenShift installations:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc patch configmap kmm-operator-manager-config -n openshift-kmm --type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;json&#39;</span> -p<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/data/controller_config.yaml&#34;, &#34;value&#34;: &#34;healthProbeBindAddress: :8081\nmetricsBindAddress: 127.0.0.1:8080\nleaderElection:\n enabled: true\n resourceID: kmm.sigs.x-k8s.io\nwebhook:\n disableHTTP2: true\n port: 9443\nworker:\n runAsUser: 0\n seLinuxType: spc_t\n setFirmwareClassPath: /var/lib/firmware&#34;}]&#39;</span>
</span></span></code></pre></div><p>If you follow this approach, the KMM pod must be deleted so that the new configuration is taken into effect as soon as the operator recreates it, so that the systems get it applied during the loading of the modules.</p><h3 id=intel-data-center-gpu-flex-140-kernel-module>Intel Data Center GPU Flex 140 kernel module<a hidden class=anchor aria-hidden=true href=#intel-data-center-gpu-flex-140-kernel-module>#</a></h3><p>The Intel GPU kernel module deployment might be a bit tricky because it&rsquo;s a <a href=https://github.com/intel-gpu/intel-gpu-i915-backports.git%22>driver</a> and it has more ties to the specific kernel version being used that we&rsquo;ll be setting in the commands for the build process, as we&rsquo;ll see in the Containerfile used for the build.</p><h4 id=using-prebuilt-drivers>Using prebuilt drivers<a hidden class=anchor aria-hidden=true href=#using-prebuilt-drivers>#</a></h4><p>In this approach, we use KMM to deploy the built and certified drivers already created by Intel which provides the container for each kernel version via a CI/CD pipeline. End users can directly consume that container via this <a href=https://raw.githubusercontent.com/intel/intel-technology-enabling-for-openshift/main/kmmo/intel-dgpu.yaml%22>Module</a> definition:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kmm.sigs.x-k8s.io/v1beta1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Module</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>moduleLoader</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>container</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>modprobe</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>moduleName</span>: <span style=color:#ae81ff>i915</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>firmwarePath</span>: <span style=color:#ae81ff>/firmware</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>kernelMappings</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>regexp</span>: <span style=color:#e6db74>&#39;^.*\.x86_64$&#39;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>containerImage</span>: <span style=color:#ae81ff>registry.connect.redhat.com/intel/intel-data-center-gpu-driver-container:2.2.0-$KERNEL_FULL_VERSION</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>intel.feature.node.kubernetes.io/gpu</span>: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span></code></pre></div><h4 id=compiling-your-own-driver>Compiling your own driver<a hidden class=anchor aria-hidden=true href=#compiling-your-own-driver>#</a></h4><p>For the sake of demonstration, here we&rsquo;ll be building our own driver image using in-cluster builds.</p><p>For building the kernel module, we can use the following Containerfile based on the <a href=https://github.com/intel/intel-data-center-gpu-driver-for-openshift/blob/main/docker/intel-dgpu-driver.Dockerfile%22>upstream instructions</a> that we&rsquo;re already defining as configmap:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu-dockerfile-configmap</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>dockerfile</span>: |-<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Intel Data Center GPU driver components combinations.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG I915_RELEASE=I915_23WW51.5_682.48_23.6.42_230425.56
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG FIRMWARE_RELEASE=23WW49.5_682.48
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Intel Data Center GPU Driver for OpenShift version.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG DRIVER_VERSION=2.2.0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # RHCOS Kernel version supported by the above driver version.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG KERNEL_FULL_VERSION
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Red Hat DTK image is used as builder image to build kernel driver modules.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Appropriate DTK image is provided with the OCP release, to guarantee compatibility
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # between the built kernel modules and the OCP version&#39;s RHCOS kernel.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # DTK_AUTO is populated automatically with the appropriate DTK image by KMM operator.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG DTK_AUTO
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    FROM ${DTK_AUTO} as builder
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG I915_RELEASE
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG FIRMWARE_RELEASE
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG KERNEL_FULL_VERSION
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    WORKDIR /build
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Building i915 driver
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    RUN git clone -b ${I915_RELEASE} --single-branch https://github.com/intel-gpu/intel-gpu-i915-backports.git \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; cd intel-gpu-i915-backports \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; install -D COPYING /licenses/i915/COPYING \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; export LEX=flex; export YACC=bison \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; export OS_TYPE=rhel_9 &amp;amp;&amp;amp; export OS_VERSION=&#34;9.2&#34; \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; cp defconfigs/i915 .config \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; make olddefconfig &amp;amp;&amp;amp; make modules -j $(nproc) &amp;amp;&amp;amp; make modules_install
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Copy out-of-tree drivers to /opt/lib/modules/${KERNEL_FULL_VERSION}/
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    RUN for file in $(find /lib/modules/${KERNEL_FULL_VERSION}/updates/ -name &#34;*.ko&#34;); do \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        cp $file /opt --parents; done
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Create the symbolic link for in-tree dependencies
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    RUN ln -s /lib/modules/${KERNEL_FULL_VERSION} /opt/lib/modules/${KERNEL_FULL_VERSION}/host
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    RUN depmod -b /opt ${KERNEL_FULL_VERSION}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Firmware
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    RUN git clone -b ${FIRMWARE_RELEASE} --single-branch https://github.com/intel-gpu/intel-gpu-firmware.git \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; install -D /build/intel-gpu-firmware/COPYRIGHT /licenses/firmware/COPYRIGHT \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; install -D /build/intel-gpu-firmware/COPYRIGHT /build/firmware/license/COPYRIGHT \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; install -D /build/intel-gpu-firmware/firmware/dg2* /build/firmware/ \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &amp;amp;&amp;amp; install -D /build/intel-gpu-firmware/firmware/pvc* /build/firmware/
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Packaging Intel GPU driver components in the base UBI image for certification
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    FROM registry.redhat.io/ubi9/ubi-minimal:9.2
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG DRIVER_VERSION
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG KERNEL_FULL_VERSION
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG I915_RELEASE
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ARG FIRMWARE_RELEASE
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Required labels for the image metadata
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    LABEL vendor=&#34;Intel¬Æ&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    LABEL version=&#34;${DRIVER_VERSION}&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    LABEL release=&#34;${KERNEL_FULL_VERSION}&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    LABEL name=&#34;intel-data-center-gpu-driver-container&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    LABEL summary=&#34;Intel¬Æ Data Center GPU Driver Container Image&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    LABEL description=&#34;Intel¬Æ Data Center GPU Driver container image designed for Red Hat OpenShift Container Platform. \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    The driver container is based on Intel Data Center GPU driver components - i915 driver release:${I915_RELEASE}, \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    and Firmware release:${FIRMWARE_RELEASE}. This driver container image is supported for RHOCP 4.14 RHCOS kernel version: ${KERNEL_FULL_VERSION}.&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    RUN microdnf update -y &amp;amp;&amp;amp; rm -rf /var/cache/yum
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    RUN microdnf -y install kmod findutils &amp;amp;&amp;amp; microdnf clean all
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    COPY --from=builder /licenses/ /licenses/
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    COPY --from=builder /opt/lib/modules/${KERNEL_FULL_VERSION}/ /opt/lib/modules/${KERNEL_FULL_VERSION}/
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    COPY --from=builder /build/firmware/ /firmware/i915/</span>
</span></span></code></pre></div><p>Let&rsquo;s also define the <code>imagestream</code> for storing the generated driver image:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>image.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ImageStream</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>intel-dgpu-driver-container-kmmo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu-driver-container-kmmo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>: {}
</span></span></code></pre></div><p>Reference this Containerfile in the following YAML:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kmm.sigs.x-k8s.io/v1beta1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Module</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu-on-premise</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>moduleLoader</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>container</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>imagePullPolicy</span>: <span style=color:#ae81ff>Always</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>modprobe</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>moduleName</span>: <span style=color:#ae81ff>i915</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>firmwarePath</span>: <span style=color:#ae81ff>/firmware</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>kernelMappings</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>regexp</span>: <span style=color:#e6db74>&#39;^.*\.x86_64$&#39;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>containerImage</span>: <span style=color:#ae81ff>image-registry.openshift-image-registry.svc:5000/openshift-kmm/intel-dgpu-driver-container-kmmo:$KERNEL_FULL_VERSION</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>build</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>dockerfileConfigMap</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu-dockerfile-configmap</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>intel.feature.node.kubernetes.io/gpu</span>: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span></code></pre></div><p>This file will use the above Containerfile to build the module and store the image in the repository. Note the <code>selector</code> field. It has been modified to use NFD for discovery and only load the kernel module where needed.</p><p>We can check that the kernel module has been loaded by connecting to the node and checking the status:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sh-5.1# lsmod|grep i915
</span></span><span style=display:flex><span>i915                 <span style=color:#ae81ff>3977216</span>  <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>intel_vsec             <span style=color:#ae81ff>20480</span>  <span style=color:#ae81ff>1</span> i915
</span></span><span style=display:flex><span>compat                 <span style=color:#ae81ff>24576</span>  <span style=color:#ae81ff>2</span> intel_vsec,i915
</span></span><span style=display:flex><span>video                  <span style=color:#ae81ff>61440</span>  <span style=color:#ae81ff>1</span> i915
</span></span><span style=display:flex><span>drm_display_helper    <span style=color:#ae81ff>172032</span>  <span style=color:#ae81ff>2</span> compat,i915
</span></span><span style=display:flex><span>cec                    <span style=color:#ae81ff>61440</span>  <span style=color:#ae81ff>2</span> drm_display_helper,i915
</span></span><span style=display:flex><span>i2c_algo_bit           <span style=color:#ae81ff>16384</span>  <span style=color:#ae81ff>2</span> ast,i915
</span></span><span style=display:flex><span>drm_kms_helper        <span style=color:#ae81ff>192512</span>  <span style=color:#ae81ff>5</span> ast,drm_display_helper,i915
</span></span><span style=display:flex><span>drm                   <span style=color:#ae81ff>581632</span>  <span style=color:#ae81ff>7</span> drm_kms_helper,compat,ast,drm_shmem_helper,drm_display_helper,i915
</span></span></code></pre></div><p>Alternatively, we can check the label added by KMM on the nodes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc describe node |grep kmm
</span></span><span style=display:flex><span>kmm.node.kubernetes.io/openshift-kmm.intel-dgpu-on-premise.ready<span style=color:#f92672>=</span>
</span></span></code></pre></div><p>Once the kernel module is deployed, use an application to verify HW acceleration is provided by the GPU.</p><div class="admonition note"><p class=admonition-title>Note</p><p class=admonition>Here we&rsquo;re directly using the <code>/dev</code> filesystem for accessing the GPU. The recommended way is to use the <a href=https://catalog.redhat.com/software/container-stacks/detail/61e9f2d7b9cdd99018fc5736%22>Intel Device Plugins Operator</a> and then add a CR to expose gpu.intel.com/i915 to the kubelet for workload consumption as described in the <a href=https://github.com/intel/intel-technology-enabling-for-openshift/tree/main%22>repository</a>.</p></div><h3 id=verify-the-deployment>Verify the deployment<a hidden class=anchor aria-hidden=true href=#verify-the-deployment>#</a></h3><h4 id=simple-approach>Simple approach<a hidden class=anchor aria-hidden=true href=#simple-approach>#</a></h4><p>We&rsquo;ll be using <code>clinfo</code> to get information from our card. To do so, we&rsquo;ll create the image and the namespace and then run the utility inside a privileged pod similar to the application that we can use as a more complex approach.</p><p>Let&rsquo;s create the <code>BuildConfiguration</code> and <code>ImageStream</code> based <a href=https://github.com/intel/intel-technology-enabling-for-openshift/blob/main/tests/l2/dgpu/clinfo_build.yaml%22>on this one</a> by creating a <code>clinfobuild.yaml</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>image.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ImageStream</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu-clinfo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>: {}
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>build.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BuildConfig</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu-clinfo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>output</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>to</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ImageStreamTag</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu-clinfo:latest</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>runPolicy</span>: <span style=color:#ae81ff>Serial</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>source</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>dockerfile</span>:
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;ARG BUILDER=registry.access.redhat.com/ubi9-minimal:latest \nFROM
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      ${BUILDER}  \n\nARG OCL_ICD_VERSION=ocl-icd-2.2.13-4.el9.x86_64\nARG CLINFO_VERSION=clinfo-3.0.21.02.21-4.el9.x86_64\n\nRUN
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      microdnf install -y \\\n  glibc \\\n  yum-utils \n\n# install intel-opencl,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      ocl-icd and clinfo\nRUN dnf install -y &#39;dnf-command(config-manager)&#39; &amp;amp;&amp;amp; \\\n
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      \ dnf config-manager --add-repo https://repositories.intel.com/gpu/rhel/9.0/lts/2350/unified/intel-gpu-9.0.repo
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &amp;amp;&amp;amp; \\\n  dnf install -y intel-opencl  \\\n  https://mirror.stream.centos.org/9-stream/AppStream/x86_64/os/Packages/$OCL_ICD_VERSION.rpm
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      \ \\\n  https://dl.fedoraproject.org/pub/epel/9/Everything/x86_64/Packages/c/$CLINFO_VERSION.rpm
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &amp;amp;&amp;amp; \\\n  dnf clean all &amp;amp;&amp;amp; dnf autoremove &amp;amp;&amp;amp; rm -rf /var/lib/dnf/lists/* &amp;amp;&amp;amp; \\\n
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      \     rm -rf /etc/yum.repos.d/intel-graphics.repo     \n&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Dockerfile</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>strategy</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>dockerStrategy</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>buildArgs</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>BUILDER</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#ae81ff>registry.access.redhat.com/ubi9-minimal:latest</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>OCL_ICD_VERSION</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#ae81ff>ocl-icd-2.2.13-4.el9.x86_64</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>CLINFO_VERSION</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#ae81ff>clinfo-3.0.21.02.21-4.el9.x86_64</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Docker</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>triggers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>type</span>: <span style=color:#ae81ff>ConfigChange</span>
</span></span></code></pre></div><p>Let&rsquo;s apply it with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc create -f clinfobuild.yaml
</span></span></code></pre></div><p>And then define the privileged pod that will run the tool with this pod defined by <code>job.yaml</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>intel-dgpu-clinfo</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>clinfo-pod</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>image</span>: <span style=color:#ae81ff>image-registry.openshift-image-registry.svc:5000/openshift-kmm/intel-dgpu-clinfo:latest</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>command</span>: [<span style=color:#e6db74>&#34;clinfo&#34;</span>]
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>securityContext</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>privileged</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>runAsUser</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>runAsGroup</span>: <span style=color:#ae81ff>110</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dev</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/dev</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dev</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/dev</span>
</span></span></code></pre></div><p>Let&rsquo;s create the pod with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc create -f job.yaml
</span></span></code></pre></div><p>And then examine the output of the pod by running:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc logs pod intel-dgpu-clinfo
</span></span><span style=display:flex><span>Number of platforms                               <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  Platform Name                                   Intel<span style=color:#f92672>(</span>R<span style=color:#f92672>)</span> OpenCL HD Graphics
</span></span><span style=display:flex><span>  Platform Vendor                                 Intel<span style=color:#f92672>(</span>R<span style=color:#f92672>)</span> Corporation
</span></span><span style=display:flex><span>  Platform Version                                OpenCL 3.0
</span></span><span style=display:flex><span>  Platform Profile                                FULL_PROFILE
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  Platform Name                                   Intel<span style=color:#f92672>(</span>R<span style=color:#f92672>)</span> OpenCL HD Graphics
</span></span><span style=display:flex><span>Number of devices                                 <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  Device Name                                     Intel<span style=color:#f92672>(</span>R<span style=color:#f92672>)</span> Data Center GPU Flex Series <span style=color:#ae81ff>140</span> <span style=color:#f92672>[</span>0x56c1<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>  Device Vendor                                   Intel<span style=color:#f92672>(</span>R<span style=color:#f92672>)</span> Corporation
</span></span><span style=display:flex><span>  Device Vendor ID                                0x8086
</span></span><span style=display:flex><span>  Device Version                                  OpenCL 3.0 NEO
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>    Platform Name                                 Intel<span style=color:#f92672>(</span>R<span style=color:#f92672>)</span> OpenCL HD Graphics
</span></span><span style=display:flex><span>    Device Name                                   Intel<span style=color:#f92672>(</span>R<span style=color:#f92672>)</span> Data Center GPU Flex Series <span style=color:#ae81ff>140</span> <span style=color:#f92672>[</span>0x56c1<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ICD loader properties
</span></span><span style=display:flex><span>  ICD loader Name                                 OpenCL ICD Loader
</span></span><span style=display:flex><span>  ICD loader Vendor                               OCL Icd free software
</span></span><span style=display:flex><span>  ICD loader Version                              2.2.12
</span></span><span style=display:flex><span>  ICD loader Profile                              OpenCL 2.2
</span></span><span style=display:flex><span>    NOTE:   your OpenCL library only supports OpenCL 2.2,
</span></span><span style=display:flex><span>        but some installed platforms support OpenCL 3.0.
</span></span><span style=display:flex><span>        Programs using 3.0 features may crash
</span></span><span style=display:flex><span>        or behave unexpectedly
</span></span></code></pre></div><h4 id=using-openvino-application-for-image-text-to-image-with-stable-diffusion>Using OpenVINO application for image text to image with stable diffusion<a hidden class=anchor aria-hidden=true href=#using-openvino-application-for-image-text-to-image-with-stable-diffusion>#</a></h4><p>An application using Intel&rsquo;s OpenVINO software will be used to showcase the functionality of the GPU acceleration in the processing that will use some keywords introduced to generate images.</p><h5 id=requirements>Requirements<a hidden class=anchor aria-hidden=true href=#requirements>#</a></h5><p>In the following paragraphs we&rsquo;ll be covering the requirements that we&rsquo;ll be preparing for the final step of validating the proper setup of our driver.</p><h6 id=imagestream>ImageStream<a hidden class=anchor aria-hidden=true href=#imagestream>#</a></h6><p>We need to define an ImageStream to store our container with this YAML:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>image.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ImageStream</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>jupyter-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>jupyter-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>openshift-kmm</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>: {}
</span></span></code></pre></div><h6 id=containerfile>Containerfile<a hidden class=anchor aria-hidden=true href=#containerfile>#</a></h6><p>First prepare the container image containing all the required bits and pieces for storing in a registry for later use with this BuildConfig:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>BuildConfig</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>build.openshift.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#e6db74>&#34;jupyter-demo&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>source</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>dockerfile</span>: |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      FROM quay.io/jupyter/base-notebook
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      USER root
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      RUN apt update &amp;amp;&amp;amp; \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        apt install -y gpg-agent git wget &amp;amp;&amp;amp; \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        apt clean
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      RUN wget -qO - https://repositories.intel.com/gpu/intel-graphics.key | gpg --dearmor --output /usr/share/keyrings/intel-graphics.gpg
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      RUN echo &#34;deb [arch=amd64 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy/production/2328 unified&#34; &amp;gt; /etc/apt/sources.list.d/intel-gpu-jammy.list
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      RUN apt update &amp;amp;&amp;amp; \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        apt install -y \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        intel-opencl-icd intel-level-zero-gpu level-zero \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        intel-media-va-driver-non-free libmfx1 libmfxgen1 libvpl2 \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        libegl-mesa0 libegl1-mesa libegl1-mesa-dev libgbm1 libgl1-mesa-dev libgl1-mesa-dri \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        libglapi-mesa libgles2-mesa-dev libglx-mesa0 libigdgmm12 libxatracker2 mesa-va-drivers \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        mesa-vdpau-drivers mesa-vulkan-drivers va-driver-all vainfo hwinfo clinfo \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        libglib2.0-0 &amp;amp;&amp;amp; \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        apt clean
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      USER jovyan
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      RUN pip install --no-cache-dir &#34;diffusers&amp;gt;=0.14.0&#34; &#34;openvino&amp;gt;=2023.3.0&#34; &#34;transformers &amp;gt;= 4.31&#34; accelerate &#34;urllib3==1.26.15&#34; ipywidgets opencv-python scipy
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      RUN mkdir -p /home/jovyan/.cache/huggingface</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>strategy</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Docker</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>output</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>to</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ImageStreamTag</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>jupyter-demo:latest</span>
</span></span></code></pre></div><div class="admonition note"><p class=admonition-title>Note</p><p class=admonition>In the above <code>BuildConfig</code>, there might be newer versions of the software installed by pip. It may be necessary to update and use a newer version. We&rsquo;re going to use <a href=https://github.com/openvinotoolkit/openvino_notebooks>OpenVINO project notebooks</a>, which already execute some pip commands to install required libraries in any case.</p></div><p>Let&rsquo;s now start the build of the image with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc start-build jupyter-demo
</span></span></code></pre></div><p>And once it&rsquo;s finished, we can check that the image appears with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc get is
</span></span><span style=display:flex><span>NAME           IMAGE REPOSITORY                                                              TAGS     UPDATED
</span></span><span style=display:flex><span>jupyter-demo   image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo   latest   <span style=color:#ae81ff>2</span> minutes ago
</span></span></code></pre></div><p>If we want to check the builds with <code>oc logs -f build/&amp;lt;&amp;lt;buildname&amp;gt;</code>, we&rsquo;ll see an output similar to this one:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc logs -f  build/jupyter-demo-1
</span></span><span style=display:flex><span>time<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;2024-03-13T12:50:47Z&#34;</span> level<span style=color:#f92672>=</span>info msg<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Not using native diff for overlay, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled&#34;</span>
</span></span><span style=display:flex><span>I0313 12:50:47.551349       <span style=color:#ae81ff>1</span> defaults.go:112<span style=color:#f92672>]</span> Defaulting to storage driver <span style=color:#e6db74>&#34;overlay&#34;</span> with options <span style=color:#f92672>[</span>mountopt<span style=color:#f92672>=</span>metacopy<span style=color:#f92672>=</span>on<span style=color:#f92672>]</span>.
</span></span><span style=display:flex><span>Caching blobs under <span style=color:#e6db74>&#34;/var/cache/blobs&#34;</span>.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Pulling image quay.io/jupyter/base-notebook ...
</span></span><span style=display:flex><span>Trying to pull quay.io/jupyter/base-notebook:latest...
</span></span><span style=display:flex><span>Getting image source signatures
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>STEP 9/11: RUN mkdir -p /home/jovyan/.cache/huggingface
</span></span><span style=display:flex><span>--&amp;gt; 284cd3e642a7
</span></span><span style=display:flex><span>STEP 10/11: ENV <span style=color:#e6db74>&#34;OPENSHIFT_BUILD_NAME&#34;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;jupyter-demo-1&#34;</span> <span style=color:#e6db74>&#34;OPENSHIFT_BUILD_NAMESPACE&#34;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;openshift-kmm&#34;</span>
</span></span><span style=display:flex><span>--&amp;gt; 9bba674b8144
</span></span><span style=display:flex><span>STEP 11/11: LABEL <span style=color:#e6db74>&#34;io.openshift.build.name&#34;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;jupyter-demo-1&#34;</span> <span style=color:#e6db74>&#34;io.openshift.build.namespace&#34;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;openshift-kmm&#34;</span>
</span></span><span style=display:flex><span>COMMIT temp.builder.openshift.io/openshift-kmm/jupyter-demo-1:d93bad68
</span></span><span style=display:flex><span>--&amp;gt; c738f8d15e38
</span></span><span style=display:flex><span>Successfully tagged temp.builder.openshift.io/openshift-kmm/jupyter-demo-1:d93bad68
</span></span><span style=display:flex><span>c738f8d15e38ea41f9a17082a435b4e8badf2e7d0569f34c332cf09102b0992d
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Pushing image image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo:latest ...
</span></span><span style=display:flex><span>Getting image source signatures
</span></span><span style=display:flex><span>Copying blob sha256:c37f7a4129892837c4258c045d773d933f9307d7dcf6801d80a2903c38e7936c
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>sha256:59ebd409476f3946cadfccbea9e851574c50b8ef6959f62bdfa2dd708423da30
</span></span><span style=display:flex><span>Copying config sha256:c738f8d15e38ea41f9a17082a435b4e8badf2e7d0569f34c332cf09102b0992d
</span></span><span style=display:flex><span>Writing manifest to image destination
</span></span><span style=display:flex><span>Successfully pushed image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo@sha256:f9ee5ae8fa9db556e90908b278c7ebb2d2ad271e11da82cfad44620d65834bf8
</span></span><span style=display:flex><span>Push successful
</span></span></code></pre></div><p>We need to define a StorageClass to use an LVM for the underlying storage. Then, a volume creation with persistent storage (PVC) is registered so that the space is allocated and prepared for our application and a secondary one for the cache of the application.</p><p>We&rsquo;ll use that space later on to download the Jupyter notebooks that we&rsquo;ll be using for the GPU demonstration.</p><p>For the following items, apply each one with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc apply -f &amp;lt;file.yaml&amp;gt;
</span></span></code></pre></div><h6 id=storageclass>StorageClass<a hidden class=anchor aria-hidden=true href=#storageclass>#</a></h6><p>Let&rsquo;s create a file with the following contents defining our Storage Class named <code>sc.yaml</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>allowVolumeExpansion</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>storage.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>StorageClass</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>lvms-vg1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>parameters</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>csi.storage.k8s.io/fstype</span>: <span style=color:#ae81ff>xfs</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>topolvm.io/device-class</span>: <span style=color:#ae81ff>vg1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>provisioner</span>: <span style=color:#ae81ff>topolvm.io</span>
</span></span><span style=display:flex><span><span style=color:#f92672>reclaimPolicy</span>: <span style=color:#ae81ff>Delete</span>
</span></span><span style=display:flex><span><span style=color:#f92672>volumeBindingMode</span>: <span style=color:#ae81ff>WaitForFirstConsumer</span>
</span></span></code></pre></div><h6 id=persistent-volume-claim>Persistent Volume Claim<a hidden class=anchor aria-hidden=true href=#persistent-volume-claim>#</a></h6><p>Similarly, we need to create a file for registering the storage that we&rsquo;ll be naming <code>pvc.yaml</code> with the following contents:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolumeClaim</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>pvc</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>lvms-vg1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteOnce</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>20Gi</span>
</span></span></code></pre></div><h6 id=pvc-for-cache>PVC for cache<a hidden class=anchor aria-hidden=true href=#pvc-for-cache>#</a></h6><p>Finally, the application uses some cache, so another PVC will be created using a file named <code>cache.yaml</code> with the following contents:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolumeClaim</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>huggingface-cache</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>lvms-vg1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteOnce</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>20Gi</span>
</span></span></code></pre></div><p>Make sure you&rsquo;ve applied the above files as instructed before following from this point.</p><p>At this point, we&rsquo;ve created the Build, ImageStream, StorageClass, PVC, and PVC for Cache; because we launched the build, we will also have the ImageStream populated, so we&rsquo;re ready to run the application.</p><h5 id=running-the-application>Running the application<a hidden class=anchor aria-hidden=true href=#running-the-application>#</a></h5><p>A Jupyter notebook is an interactive workbook where commands, outputs like visualizations, etc., can be shown alongside the code.</p><p>It&rsquo;s commonly used in data science analysis as it allows you to quickly edit and amend the commands and refresh the output with the new values.</p><p>Using the image generated in the previous Containerfile and the previous storage PVCs, we can create a pod with this YAML:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kmm-demo-jupyter</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kmm-demo-jupyter</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>image</span>: <span style=color:#ae81ff>image-registry.openshift-image-registry.svc:5000/openshift-kmm/jupyter-demo</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>args</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># Password is paella in case you want to reuse</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>start-notebook.py</span>
</span></span><span style=display:flex><span>        - --<span style=color:#ae81ff>PasswordIdentityProvider.hashed_password=&#39;argon2:$argon2id$v=19$m=10240,t=10,p=8$00Ynt8+Jk4sMtJUM+7Us5Q$ycb5PzmA7IH9yfOPAIfUjMNvDzXHKiMXPvM6+R5nucQ&#39;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>GRANT_SUDO</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;yes&#34;</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>NB_GID</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;110&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>8888</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>hostPort</span>: <span style=color:#ae81ff>8888</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>limits</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;20&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;64Gi&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>securityContext</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>privileged</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>runAsUser</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>runAsGroup</span>: <span style=color:#ae81ff>110</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dev</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/dev</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>huggingface-cache</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/home/jovyan/.cache/huggingface</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>work</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/home/jovyan/work</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>dev</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/dev</span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>huggingface-cache</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>persistentVolumeClaim</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>claimName</span>: <span style=color:#ae81ff>huggingface-cache</span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>work</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>persistentVolumeClaim</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>claimName</span>: <span style=color:#ae81ff>pvc</span>
</span></span></code></pre></div><p>Once the app is ready we can forward a local port (for example, to reach the application), but first we&rsquo;ll prepare the examples we&rsquo;ll be using with the <a href=https://github.com/openvinotoolkit/openvino_notebooks%22>Jupyter notebooks from the OpenVINO project</a> by getting inside our pod with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc rsh kmm-demo-jupyter
</span></span></code></pre></div><p>&mldr;then, once we&rsquo;re inside the pod:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ pwd
</span></span><span style=display:flex><span>/home/jovyan
</span></span><span style=display:flex><span>$ ls
</span></span><span style=display:flex><span>work
</span></span><span style=display:flex><span>$ git clone https://github.com/openvinotoolkit/openvino_notebooks
</span></span><span style=display:flex><span>chown -R jovyan:users openvino_notebooks
</span></span></code></pre></div><div class="admonition note"><p class=admonition-title>Note</p><p class=admonition>If you don&rsquo;t specify an authentication method on the pod, a token will be printed on the pod logs, that should be used when reaching the Jupyter interface.</p></div><p>Using the following command, we&rsquo;ll be forwarding a port from your computer to the pod itself so it will be easier to interact with the Jupyter notebook running there:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ oc port-forward kmm-demo-jupyter 8888:8888
</span></span></code></pre></div><p>Once done, on your local computer browser open <code>http://localhost:8888</code> to access the notebook.</p><p>In the example above, the hashed password is <code>paella</code>, and it&rsquo;s the one we&rsquo;ll be using to access the Jupyter notebook.</p><p>From within the browser, you can access the previous URL, navigate the notebooks, and select <code>225-stable-diffusion-text-to-image</code> so that the final URL is:</p><p><code>http://localhost:8888/lab/tree/openvino_notebooks/notebooks/225-stable-diffusion-text-to-image/225-stable-diffusion-text-to-image.ipynb</code></p><p>Skip over the explanation steps and navigate to the area with the GPU selection drop-down (Figure 1).</p><p><img loading=lazy src=../../../../../blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/image1.png><em>Figure 1: GPU selection dropdown</em></p><p>&mldr;and later, the keyword section (Figure 2).</p><p><img loading=lazy src=../../../../../blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/image2.png><em>Figure 2: keyword selection for imput</em></p><p>In this section, you can describe what kind of image should be generated. This is where the real magic happens. In this case, we will use the prompt <code>Valencia fallas sunny day</code> and see what kind of image is generated. See Figure 3.</p><p><img loading=lazy src=../../../../../blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/image3.png><em><em>Figure 3: Output of the pipeline for the initial set of words</em></em>
Of course, you can go back, edit the input keywords, and try new ones (Figure 4)</p><p><img loading=lazy src=../../../../../blog/2024/04/26/enable-gpu-acceleration-with-the-kernel-module-management-operator/image4.png><em><em>Figure 4: Output of the pipeline for the additional set of words</em></em></p><h2 id=wrap-up>Wrap up<a hidden class=anchor aria-hidden=true href=#wrap-up>#</a></h2><p>We hope that you&rsquo;ve enjoyed this read and realize how KMM, NFD, and DTK make managing custom drivers across many nodes across a cluster much easier than having to log into each node individually to install drivers.</p><p>We hope that you&rsquo;ve enjoyed this read and realize how quick and convenient it is to use KMM, NFD, and DTK to enable support for accelerators in your OpenShift infrastructure.</p><h2 id=but-i-do-still-want-to-upgrade-my-cluster>But I do still want to upgrade my cluster!<a hidden class=anchor aria-hidden=true href=#but-i-do-still-want-to-upgrade-my-cluster>#</a></h2><p>Don&rsquo;t worry, KMM automatically will check the modules configured for your hosts and the kernel they are running‚Äîif you&rsquo;re using the prebuilt images, KMM will download and enable the prebuilt image and if you&rsquo;re using a custom build, a new build process will happen so that the new image is available&mldr; and using the labels added by KMM you can schedule your workloads on the nodes that have the driver ready for consumption.</p><p>It&rsquo;s still recommended to do a staggered upgrade, so only a few nodes are updated before moving into others to avoid, for example, a new kernel having some issues with the build process or <a href=https://github.com/intel/intel-data-center-gpu-driver-for-openshift/tree/main/release#intel-data-center-gpu-driver-container-images-for-openshift-release>no prebuilt driver being available</a> because it is still under the certification and validation process&mldr; rendering workloads requiring a specific device driver to become unschedulable.</p><p>Once you&rsquo;ve checked that the driver is available at the preceding link and checked the official documentation on the <a href=https://access.redhat.com/solutions/4606811>OpenShift Upgrade</a> process, be ready!</p><p>Enjoy! (and if you do, you can
<a href=https://ko-fi.com/I2I4KDA0V target=_blank><img src=https://storage.ko-fi.com/cdn/brandasset/kofi_s_logo_nolabel.png height=36 style=border:0;height:36px;vertical-align:middle;display:inline-block border=0>
</img>
Buy Me a Coffee
</a>)</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://iranzo.io/blog/2024/08/09/rhel-bootable-containers-bootc/><span class=title>¬´ Prev</span><br><span>RHEL bootable containers (BOOTC)</span>
</a><a class=next href=https://iranzo.io/tips/bundle-localuser/><span class=title>Next ¬ª</span><br><span>Install gems on local user folder instead of system wide</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Enable GPU acceleration with the Kernel Module Management Operator on x" href="https://x.com/intent/tweet/?text=Enable%20GPU%20acceleration%20with%20the%20Kernel%20Module%20Management%20Operator&amp;url=https%3a%2f%2firanzo.io%2fblog%2f2024%2f04%2f26%2fenable-gpu-acceleration-with-the-kernel-module-management-operator%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enable GPU acceleration with the Kernel Module Management Operator on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2firanzo.io%2fblog%2f2024%2f04%2f26%2fenable-gpu-acceleration-with-the-kernel-module-management-operator%2f&amp;title=Enable%20GPU%20acceleration%20with%20the%20Kernel%20Module%20Management%20Operator&amp;summary=Enable%20GPU%20acceleration%20with%20the%20Kernel%20Module%20Management%20Operator&amp;source=https%3a%2f%2firanzo.io%2fblog%2f2024%2f04%2f26%2fenable-gpu-acceleration-with-the-kernel-module-management-operator%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enable GPU acceleration with the Kernel Module Management Operator on reddit" href="https://reddit.com/submit?url=https%3a%2f%2firanzo.io%2fblog%2f2024%2f04%2f26%2fenable-gpu-acceleration-with-the-kernel-module-management-operator%2f&title=Enable%20GPU%20acceleration%20with%20the%20Kernel%20Module%20Management%20Operator"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enable GPU acceleration with the Kernel Module Management Operator on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2firanzo.io%2fblog%2f2024%2f04%2f26%2fenable-gpu-acceleration-with-the-kernel-module-management-operator%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enable GPU acceleration with the Kernel Module Management Operator on whatsapp" href="https://api.whatsapp.com/send?text=Enable%20GPU%20acceleration%20with%20the%20Kernel%20Module%20Management%20Operator%20-%20https%3a%2f%2firanzo.io%2fblog%2f2024%2f04%2f26%2fenable-gpu-acceleration-with-the-kernel-module-management-operator%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enable GPU acceleration with the Kernel Module Management Operator on telegram" href="https://telegram.me/share/url?text=Enable%20GPU%20acceleration%20with%20the%20Kernel%20Module%20Management%20Operator&amp;url=https%3a%2f%2firanzo.io%2fblog%2f2024%2f04%2f26%2fenable-gpu-acceleration-with-the-kernel-module-management-operator%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enable GPU acceleration with the Kernel Module Management Operator on ycombinator" href="https://news.ycombinator.com/submitlink?t=Enable%20GPU%20acceleration%20with%20the%20Kernel%20Module%20Management%20Operator&u=https%3a%2f%2firanzo.io%2fblog%2f2024%2f04%2f26%2fenable-gpu-acceleration-with-the-kernel-module-management-operator%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://iranzo.io/>Pablo Iranzo G√≥mez blog</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><center><small>This blog is a participant in the Amazon Associate Program, an affiliate advertising program designed to provide a means for sites to earn advertising fees by advertising and linking to Amazon.</small></center><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>